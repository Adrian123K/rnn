{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>■ 복습</b>\n",
    "    컴퓨터에게 사람의 말(자연어)을 이해시키기 위해서\n",
    "        비교. CNN: 컴퓨터에게 이미지를 이해시키기 위해서\n",
    "    \n",
    "    2장. 개선된 통계기법 -> 단어의 분산표현 생성(밀집벡터) -> 단어와 단어 사이의 유사성\n",
    "    3장. CBOW 신경망을 이용해서 문장을 학습 시킴 -> 단어의 분산 표현 생성(가중치)\n",
    "    4장. CBOW 신경망에 큰 말뭉치가 입력될 수 있도록 개선\n",
    "        CBOW 신경망의 문제점\n",
    "            1. 말뭉치가 크면 메모리 사용량이 많다\n",
    "            2. 성능이 느림\n",
    "            \n",
    "    5장. RNN 신경망 -> 기억하는 신경망\n",
    "        실습\n",
    "            작은 말뭉치                  --------> RNN 신경망에 입력하고 학습\n",
    "            큰 말뭉치(스티브잡스 연설문)\n",
    "                                  문장을 one hot 표현으로 변경\n",
    "        RNN 신경망의 문제점(한계점)\n",
    "            장기기억 취약\n",
    "            기울기 소실 / 폭발\n",
    "            \n",
    "    6장. LSTM 신경망\n",
    "        RNN과 LSTM 신경망의 차이가 무엇\n",
    "            기억셀 존재\n",
    "            게이트를 추가한 RNN LSTM\n",
    "                forget\n",
    "                input\n",
    "                output\n",
    "                새로운 기억\n",
    "        기울기 소실 문제의 원인 : tanh 함수 -> relu함수\n",
    "        기울기 폭발 문제의 원인 : matmul 연산 -> 게이트 이용해서 크기 조정\n",
    "        \n",
    "        LSTM 실습\n",
    "            1. 긍정단어/부정단어를 컴퓨터가 인식할 수 있는지\n",
    "            2. 주가예측을 LSTM 신경망으로 구현\n",
    "                지난 5년치 주가 데이터로 내일의 종가를 예측\n",
    "                    회사의 이름을 입력하면 내일 예측 종가 출력\n",
    "             \n",
    "    7장. seq2seq\n",
    "        RNN을 사용해서 문장 생성 : 기사작성, 소설을 쓰는 AI 생성, 번역하는 신경망\n",
    "        실습\n",
    "            1. 영어 문장 생성\n",
    "            2. 한글 문장 생성\n",
    "        RNN으로 소설을 쓰거나 작곡을 할 때 활용\n",
    "    ------------------------------------------------------------------------------------------------------------\n",
    "        RNN으로 챗봇을 만들려고 할 때 활용\n",
    "            \n",
    "            3. 덧셈을 하는 신경망 구현\n",
    "            4. 번역 신경망 \n",
    "                영어 -> 프랑스어 : keras 구현\n",
    "                영어 -> 한국어 : keras 구현\n",
    "\n",
    "        영어 : you ..... 나머지 문장 알아서 생성\n",
    "        한글 : 가는 말이 ... 나머지 문장 알아서 생성\n",
    "            배운 문장들로만 학습을 해서 나오는 단어들이 기존에 학습한 문장들의 단어.\n",
    "            한글 문장을 넣어서 학습시키면 새로운 문장들이 이어져서 나올 수 있다.\n",
    "                \n",
    "    8장. 어텐션 : seq2seq 문제점을 해결하는 기술\n",
    "        인공지능에게 질문을 하면 대답을 하는 것을 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 8. 세상에는 시계열 데이터가 넘쳐납니다. 언어 데이터, 음성 데이터, 동영상 데이터는 모두 시계열 데이터 입니다. 우리는 시계열 데이터를 다른 시계열 데이터로 변환하는 모델을 생각해 볼 것입니다. 이를 위한 기법으로 책에서 사용하고 있는 방법이 무엇입니까 ?\n",
    "    RNN을 이용하는 seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 9. seq2seq 에서 사용하는 2개의 모듈이 무엇입니까 ?\n",
    "    Encoder, Decoder\n",
    "![fig](http://cfile248.uf.daum.net/image/99338A3D5F5448D32258BE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 10. encoder 의 역할은 무엇이고 decoder 의 역할은 무엇입니까 ?\n",
    "    RNN을 이용해 시계열 데이터를 h라는 은닉 상태 벡터로 변환\n",
    "    \n",
    "<h2><b>Encoder</b></h2>\n",
    "\n",
    "![fig](http://cfile277.uf.daum.net/image/998655475F5457CE299ABC)    \n",
    "![fig2](http://cfile282.uf.daum.net/image/99B4634F5F5458472CB29D)\n",
    "<br>\n",
    "<h2><b>Decoder</b></h2>\n",
    "    \n",
    "    Encoder가 출력하는 벡터 h는 LSTM 계층의 마지막 은닉상태에서 번역에 필요한 정보가 인코딩 되어있다.\n",
    "        Encoding(부호화) : 임의 길이의 문장을 위 그림처럼 고정길이의 벡터로 변환하는 작업, 어떤 정보를 어떤 규칙에 따라 변환\n",
    "        Decoding(복호화) : Encoding된 정보를 원래의 정보로 되돌리는 것\n",
    "    고정길이의 벡터인 h를 decoder를 구성하는 계층의 LSTM 계층이 입력 받음\n",
    "    \n",
    "![fig3](http://cfile290.uf.daum.net/image/99202A4F5F54B18F2F9DD9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 11. 그러면 seq2seq 를 이용해서 번역 신경망을 만들면 전체가 어떻게 구성되어집니까 ?\n",
    "![fig](http://cfile253.uf.daum.net/image/99974C475F54B1E92B79F7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 12. seq2seq 를 이용해서 컴퓨터에게 질문을 해보고 답을 받아 내 봅니다. 아래의 57+5 가 62라는 것을 신경망이 출력할 수 있게 구현하세요. \n",
    "![fig](http://cfile250.uf.daum.net/image/998D34465F54B2912B3F84)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 13. 아래의 가변 길이의 시계열 데이터를  seq2seq로 구현하는데 미니배치로 학습하려면 무엇을 사용해야 합니까?\n",
    "    padding\n",
    "![fig](http://cfile270.uf.daum.net/image/9997124B5F54B2DA31C194)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 14. 지금까지 구현된 seq2seq 신경망의 성능을 좀 더 높이기 위해서 할 수 있는 방법 책에서 소개된 2가지가 무엇입니까 ?\n",
    "    입력 데이터 반전\n",
    "![fig](http://cfile250.uf.daum.net/image/99B7B7495F54B4462A0A4A)\n",
    "\n",
    "<b>개선 전</b>\n",
    "![fig2](http://cfile254.uf.daum.net/image/99375E485F54B4AC291B4D)\n",
    "\n",
    "<b>개선 후</b>\n",
    "![fig3](http://cfile286.uf.daum.net/image/99063E4D5F54B4C92D13CE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
