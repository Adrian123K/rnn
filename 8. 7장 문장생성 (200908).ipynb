{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>■ 복습</b>\n",
    "    컴퓨터에게 사람의 말(자연어)을 이해시키기 위해서\n",
    "        비교. CNN: 컴퓨터에게 이미지를 이해시키기 위해서\n",
    "    \n",
    "    2장. 개선된 통계기법 -> 단어의 분산표현 생성(밀집벡터) -> 단어와 단어 사이의 유사성\n",
    "    3장. CBOW 신경망을 이용해서 문장을 학습 시킴 -> 단어의 분산 표현 생성(가중치)\n",
    "    4장. CBOW 신경망에 큰 말뭉치가 입력될 수 있도록 개선\n",
    "        CBOW 신경망의 문제점\n",
    "            1. 말뭉치가 크면 메모리 사용량이 많다\n",
    "            2. 성능이 느림\n",
    "            \n",
    "    5장. RNN 신경망 -> 기억하는 신경망\n",
    "        실습\n",
    "            작은 말뭉치                  --------> RNN 신경망에 입력하고 학습\n",
    "            큰 말뭉치(스티브잡스 연설문)\n",
    "                                  문장을 one hot 표현으로 변경\n",
    "        RNN 신경망의 문제점(한계점)\n",
    "            장기기억 취약\n",
    "            기울기 소실 / 폭발\n",
    "            \n",
    "    6장. LSTM 신경망\n",
    "        RNN과 LSTM 신경망의 차이가 무엇\n",
    "            기억셀 존재\n",
    "            게이트를 추가한 RNN LSTM\n",
    "                forget\n",
    "                input\n",
    "                output\n",
    "                새로운 기억\n",
    "        기울기 소실 문제의 원인 : tanh 함수 -> relu함수\n",
    "        기울기 폭발 문제의 원인 : matmul 연산 -> 게이트 이용해서 크기 조정\n",
    "        LSTM 실습\n",
    "            1. 긍정단어/부정단어를 컴퓨터가 인식할 수 있는지\n",
    "            2. 주가예측을 LSTM 신경망으로 구현\n",
    "            \n",
    "    지금까지 배운 내용으로 생활에 필요한 것을 구현한다면\n",
    "        주가예측 또는 시계열 데이터 분석\n",
    "        \n",
    "    책을 공부하는 방법\n",
    "        RNN 원리를 책의 이론으로 배우고 실습은 keras로 구현\n",
    "    \n",
    "    7장. RNN을 사용해서 문장 생성 : 기사작성, 소설을 쓰는 AI 생성, 번역하는 신경망\n",
    "        실습\n",
    "            1. 영어 문장 생성\n",
    "            2. 덧셈을 하는 신경망 구현\n",
    "    8장. 어텐션 : 인공지능에게 질문을 하면 대답을 하는 것을 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 1. 7장의 큰 그림\n",
    "    이번 7장은 6장에서 다룬 RNN 을 사용한 언어 모델을 쌀째 손질하여, 문장을 생성하는 기능을 구현합니다.\n",
    "    그리고  seq2seq 에게 간단한 덧셈 문제를 학습시켜 구현합니다. seq2seq 는 encoder 와 decoder 를 연결한 모델로, 결국 2개의 RNN 을 조합한 단순한 구조입니다. \n",
    "    하지만 그 단순함에도 불구하고 seq2seq 는 매우 큰 가능성을 지니고 있어서 다양한 에플리케이션에 적용할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 2. seq2seq 란 무엇인가요 ?\n",
    "    seqeunce to sequence (시계열에서 시계열)를 뜻하는 말로 한 시계열 데이터를 다른 시계열 데이터로 변환하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 3. seq2seq  구현은 어떻게 하나요 ? \n",
    "    RNN 2개를 연결하는 아주 간단한 방법으로 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 4. seq2seq  는 어디에 응용이 되나요 ?\n",
    "    번역, 챗봇, 뉴스기사 자동 생성, 음성 인식 등에 다양하게 응용되어진다\n",
    "                  ↓ \n",
    "             문장 자동 생성(7장)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 5. 5장에서 배웠던 RNN 신경망의 구조는 어떻게 되나요?\n",
    "    You say goodbye and I say hello. 라는 말뭉치가 들어오면 특정 단어가 주어졌을 때 다음으로 나올 확률이 가장 높은 단어가 무엇인지를 예측하는 구조. \n",
    "    확률이 높은 단어는 선택되기 쉽고 확률이 낮은 단어는 선택되게 어려워지게끔 가중치가 생성되는 구조\n",
    "\n",
    "![fig](http://cfile265.uf.daum.net/image/993A34455F543E9F1EDD39)\n",
    "![fig2](http://cfile261.uf.daum.net/image/9943D5495F5440301FF6C8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 6. 그런데 다르게 테스트를 하기 위해서 책에서 결정적이란 용어와 확률적이란 용어를 설명하면서 저자는 결정적인 방법으로 다음 단어를 선택하지 않고 확률적 방법으로 선택하겠다고 하는데 두 용어의 차이가 무엇입니까?\n",
    "    결정적 : 결과가 예측 가능한 것. 기존에 배웠던 것처럼 특정 단어가 주어졌을 때 다음 단어를 예측 가능 확률이 가장 높은 단어를 선택하도록 하는 것\n",
    "    확률적 : 결과가 확률에 따라 정해지는 것. 선택되는 단어는 실행할 때 마다 달라진다.\n",
    "![fig](http://cfile281.uf.daum.net/image/99E3BC355F5442D620D53C)\n",
    "\n",
    "    you say goodbye and i say hello. 라는 문장을 그대도 다 넣는게 아니라 \n",
    "    I 다음에 나올 확률이 높은 say 를 그 다음 입력 단어로 넣고 say 를 입력한 후 다음 단어로 확률이 높은 단어가 hello 이므로 \n",
    "    hello 가 다음 단어로 입력되어집니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 7. 위와 같이 학습하면 어떤 효과가 있습니까 ?\n",
    "    이렇게 생성된 문장은 훈련 데이터에 존재하지 않는 말. 말 그대로 새로 생성된 문장\n",
    "    언어 모델은 훈련 데이터를 암기한 것이 아니라 훈련 데이터에서 사용된 단어의 패턴을 학습한 것이기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 8. 문장 자동 생성을 영어 문장 자동 생성으로 테스트 해봅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T06:23:59.504038Z",
     "start_time": "2020-09-08T06:23:59.298992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "929589\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "from dataset import ptb\n",
    "\"\"\"\n",
    "929_000개의 학습 단어, 73_000개의 validation 단어, 82_000개의 테스트 단어로 구성\n",
    "예: 일기예보 기사를 자동으로 쓰는 AI를 만들고 싶다면 일기예보 기사의 용어(단어)가 들어있는 데이터 셋을 import 해야한다.\n",
    "\"\"\"\n",
    "from ch07.rnnlm_gen import RnnlmGen\n",
    "\"\"\"\n",
    "훈련 데이터의 문장을 암기하는 게 아니라 사용된 단어의 정렬 패턴을 학습하게 한 클래스\n",
    "You say goodbye and I say hello.\n",
    "\"\"\"\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "corpus_size = len(corpus)\n",
    "print(vocab_size) # 10000\n",
    "print(corpus_size) # 929589"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T06:23:59.760136Z",
     "start_time": "2020-09-08T06:23:59.506039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you 'd see the woman 's meeting ownership under the proposed michigan mr. lawson.\n",
      " but it was sold.\n",
      " compared with an idea of both firms who i think i was down around my same time.\n",
      " he is interested as hollywood legislation high-interest capital because anthony technology inc. and services inc.\n",
      " mr. roman would be reached following the facility of the corporation for others.\n",
      " the chairman hopes yesterday as well as possible for period are tentatively priced for about half the institution would n't go share certificate and even if it was succeeded in the name\n"
     ]
    }
   ],
   "source": [
    "model = RnnlmGen()\n",
    "model.load_params('ch06/Rnnlm.pkl')\n",
    "\n",
    "# start 문자와 skip 문자 설정\n",
    "start_word = 'you'\n",
    "start_id = word_to_id[start_word] # you의 단어 id를 추출\n",
    "skip_words = ['N', '<unk>', '$'] # 샘플링 하지 않을 단어를 지정\n",
    "skip_ids = [word_to_id[w] for w in skip_words] # 샘플링하지 않을 단어들의 단어 id를 추출\n",
    "\n",
    "# 문장 생성\n",
    "word_ids = model.generate(start_id, skip_ids)\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>', '.\\n') # 한 문장의 끝을 나타내는 <eos> 표시 대신에 줄바꿈\n",
    "\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제. ch07/generate_better_text.py를 수행해서 이번에는 더 좋은 문장이 나오는지 테스트 하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T06:24:00.169940Z",
     "start_time": "2020-09-08T06:23:59.761137Z"
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file: ch06\\BetterRnnlm.pkl",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-94d633ab706d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBetterRnnlmGen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ch06/BetterRnnlm.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# start 문자와 skip 문자 설정\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Desktop\\Itwill ws\\rnn\\common\\base_model.py\u001b[0m in \u001b[0;36mload_params\u001b[1;34m(self, file_name)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No file: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: No file: ch06\\BetterRnnlm.pkl"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "from dataset import ptb\n",
    "from ch07.rnnlm_gen import BetterRnnlmGen\n",
    "from common.np import *\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "corpus_size = len(corpus)\n",
    "\n",
    "\n",
    "model = BetterRnnlmGen()\n",
    "model.load_params('ch06/BetterRnnlm.pkl')\n",
    "\n",
    "# start 문자와 skip 문자 설정\n",
    "start_word = 'you'\n",
    "start_id = word_to_id[start_word]\n",
    "skip_words = ['N', '<unk>', '$']\n",
    "skip_ids = [word_to_id[w] for w in skip_words]\n",
    "# 문장 생성\n",
    "word_ids = model.generate(start_id, skip_ids)\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>', '.\\n')\n",
    "\n",
    "print(txt)\n",
    "\n",
    "model.reset_state()\n",
    "\n",
    "start_words = 'the meaning of life is'\n",
    "start_ids = [word_to_id[w] for w in start_words.split(' ')]\n",
    "\n",
    "for x in start_ids[:-1]:\n",
    "    x = np.array(x).reshape(1, 1)\n",
    "    model.predict(x)\n",
    "\n",
    "word_ids = model.generate(start_ids[-1], skip_ids)\n",
    "word_ids = start_ids[:-1] + word_ids\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>', '.\\n')\n",
    "print('-' * 50)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>[쉬움주의] 문장 자동 생성_한글_문제</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제1. 다음 text 의 문장을 단어와 단어 id 로 변환하시오 !\n",
    "    text = 경마장에 있는 말이 뛰고 있다 그의 말이 법이다 가는 말이 고와야 오는 말이 곱다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T06:27:57.766011Z",
     "start_time": "2020-09-08T06:27:55.518516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'말이': 1, '경마장에': 2, '있는': 3, '뛰고': 4, '있다': 5, '그의': 6, '법이다': 7, '가는': 8, '고와야': 9, '오는': 10, '곱다': 11}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "\"\"\"\n",
    "문장마다 길이가 다르므로 패딩을 해서 일관된 문장 길이로 만들어주는 모듈\n",
    "신경망에 배치처리를 하려면 일관된 사이즈로 문장을 만들어줘야 하기 때문에 필요\n",
    "\"\"\"\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer # text를 숫자로 변환하는 모듈\n",
    "\n",
    "text = \"\"\"경마장에 있는 말이 뛰고 있다\\n 그의 말이 법이다\\n 가는 말이 고와야 오는 말이 곱다\"\"\"\n",
    "\n",
    "t = Tokenizer() \n",
    "t.fit_on_texts([text]) # text 문자를 가지고 corpus, word_to_id, id_to_word를 생성\n",
    "print(t.word_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    [[2, 3], [2, 3, 1], [2, 3, 1, 4], [2, 3, 1, 4, 5], [6, 1], [6, 1, 7], [8, 1], [8, 1, 9], [8, 1, 9, 10], [8, 1, 9, 10, 1], [8, 1, 9, 10, 1, 11]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제2.  '경마장에 있는 말이 뛰고 있다'와 '그의 말이 법이다'와 '가는 말이 고와야 오는 말이 곱다'라는 세 가지 문장이 있다고 해봅시다. 모델이 문맥을 학습할 수 있도록 전체 문장의 앞의 단어들을 전부 고려하여 학습하도록 데이터를 재구성한다면 아래와 같이 총 11개의 샘플이 구성됩니다.  아래의 11개의 샘플을 아래와 같이 단어id로 출력되게 하시오 ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T06:27:57.862032Z",
     "start_time": "2020-09-08T06:27:57.847028Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 1, 4, 5]]\n",
      "[[6, 1, 7]]\n",
      "[[8, 1, 9, 10, 1, 11]]\n"
     ]
    }
   ],
   "source": [
    "sequences = list()\n",
    "for line in text.split('\\n'):\n",
    "    encoded = t.texts_to_sequences([line])\n",
    "    print(encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ※ 문제3. (점심시간 문제) 아래의 결과를 아래와 같이 출력되게 코드를 수정하시오\n",
    "    [[2, 3, 1, 4, 5]]                             [2, 3, 1, 4, 5]\n",
    "    [[6, 1, 7]]                       ->          [6, 1, 7]\n",
    "    [[8, 1, 9, 10, 1, 11]]                        [8, 1, 9, 10, 1, 11]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T06:27:57.956840Z",
     "start_time": "2020-09-08T06:27:57.942836Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 1, 4, 5]\n",
      "[6, 1, 7]\n",
      "[8, 1, 9, 10, 1, 11]\n"
     ]
    }
   ],
   "source": [
    "sequences = list()\n",
    "for line in text.split('\\n'):\n",
    "    encoded = t.texts_to_sequences([line])\n",
    "    for sent in encoded:\n",
    "        print(sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    문장 ---> 숫자로 변환 -----------> RNN 신경망\n",
    "              자연어 데이터의 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T06:27:58.051952Z",
     "start_time": "2020-09-08T06:27:58.036948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 1, 4, 5]\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "[6, 1, 7]\n",
      "1\n",
      "2\n",
      "[8, 1, 9, 10, 1, 11]\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "sequences = list()\n",
    "for line in text.split('\\n'):\n",
    "    encoded = t.texts_to_sequences([line])[0]\n",
    "    print(encoded)\n",
    "    for i in range(1,len(encoded)):\n",
    "        print(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제. 위의 결과를 아래와 같이 출력되게 하시오\n",
    "    [2, 3, 1, 4, 5]\n",
    "    [2, 3]\n",
    "    [2, 3, 1]\n",
    "    [2, 3, 1, 4]\n",
    "    [2, 3, 1, 4, 5]\n",
    "    [6, 1, 7]\n",
    "    [6, 1]\n",
    "    [6, 1, 7]\n",
    "    [8, 1, 9, 10, 1, 11]\n",
    "    [8, 1]\n",
    "    [8, 1, 9]\n",
    "    [8, 1, 9, 10]\n",
    "    [8, 1, 9, 10, 1]\n",
    "    [8, 1, 9, 10, 1, 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T06:27:58.492648Z",
     "start_time": "2020-09-08T06:27:58.487647Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 1, 4, 5]\n",
      "[2, 3]\n",
      "[2, 3, 1]\n",
      "[2, 3, 1, 4]\n",
      "[2, 3, 1, 4, 5]\n",
      "[6, 1, 7]\n",
      "[6, 1]\n",
      "[6, 1, 7]\n",
      "[8, 1, 9, 10, 1, 11]\n",
      "[8, 1]\n",
      "[8, 1, 9]\n",
      "[8, 1, 9, 10]\n",
      "[8, 1, 9, 10, 1]\n",
      "[8, 1, 9, 10, 1, 11]\n"
     ]
    }
   ],
   "source": [
    "sequences = list()\n",
    "for line in text.split('\\n'):\n",
    "    encoded = t.texts_to_sequences([line])[0]\n",
    "    print(encoded)\n",
    "    for i in range(1,len(encoded)):\n",
    "        sequence = encoded[:i+1]\n",
    "        print(sequence)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T06:27:58.857063Z",
     "start_time": "2020-09-08T06:27:58.848061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3], [2, 3, 1], [2, 3, 1, 4], [2, 3, 1, 4, 5], [6, 1], [6, 1, 7], [8, 1], [8, 1, 9], [8, 1, 9, 10], [8, 1, 9, 10, 1], [8, 1, 9, 10, 1, 11]]\n"
     ]
    }
   ],
   "source": [
    "sequences = list()\n",
    "for line in text.split('\\n'):\n",
    "    encoded = t.texts_to_sequences([line])[0]\n",
    "    for i in range(1,len(encoded)):\n",
    "        sequence = encoded[:i+1]\n",
    "        sequences.append(sequence)\n",
    "print(sequences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    자연어(사람의 말) ----------> RNN 신경망\n",
    "                    데이터 전처리\n",
    "                        1. Tokenizer() : 토큰화와 정수 인코딩(단어에 대한 인덱싱)을 위해 사용\n",
    "                        2. pad_sequence() : 샘플의 길이를 동일하게 맞춰주기 위해 사용\n",
    "                        3. to_categorical() : 라벨들을 원핫 표현으로 변경해주기 위해 사용\n",
    "    \n",
    "    위의 전처리의 상세 과정\n",
    "http://cafe.daum.net/oracleoracle/Sedp/743"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제3. 아래의 리스트안의 샘플 리스트 중에 가장 길이가 긴 샘플의 길이를 출력하시오\n",
    "    [[2, 3], [2, 3, 1], [2, 3, 1, 4], [2, 3, 1, 4, 5], [6, 1], [6, 1, 7], [8, 1], [8, 1, 9], [8, 1, 9, 10], [8, 1, 9, 10, 1], [8, 1, 9, 10, 1, 11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T06:27:59.926134Z",
     "start_time": "2020-09-08T06:27:59.921134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(l) for l in sequences)\n",
    "print(max_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제4. 아래의 리스트를 가장 긴 리스트를 기준으로 나머지 리스트를 0 으로 패딩하시오\n",
    "    변경 전\n",
    "    [[2, 3], [2, 3, 1], [2, 3, 1, 4], [2, 3, 1, 4, 5], [6, 1], [6, 1, 7], [8, 1], [8, 1, 9], [8, 1, 9, 10], [8, 1, 9, 10, 1], [8, 1, 9, 10, 1, 11]]\n",
    "    \n",
    "    변경 후\n",
    "    [[ 0  0  0  0  2  3]\n",
    "     [ 0  0  0  2  3  1]\n",
    "     [ 0  0  2  3  1  4]\n",
    "     [ 0  2  3  1  4  5]\n",
    "     [ 0  0  0  0  6  1]\n",
    "     [ 0  0  0  6  1  7]\n",
    "     [ 0  0  0  0  8  1]\n",
    "     [ 0  0  0  8  1  9]\n",
    "     [ 0  0  8  1  9 10]\n",
    "     [ 0  8  1  9 10  1]\n",
    "     [ 8  1  9 10  1 11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T06:28:00.732351Z",
     "start_time": "2020-09-08T06:28:00.716340Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  2  3]\n",
      " [ 0  0  0  2  3  1]\n",
      " [ 0  0  2  3  1  4]\n",
      " [ 0  2  3  1  4  5]\n",
      " [ 0  0  0  0  6  1]\n",
      " [ 0  0  0  6  1  7]\n",
      " [ 0  0  0  0  8  1]\n",
      " [ 0  0  0  8  1  9]\n",
      " [ 0  0  8  1  9 10]\n",
      " [ 0  8  1  9 10  1]\n",
      " [ 8  1  9 10  1 11]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "sequences = pad_sequences(sequences, maxlen=6, padding='pre')\n",
    "print(sequences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제5. 아래의 리스트를 훈련 데이터와 훈련 데이터의 라벨로 구성하시오 !\n",
    "    변경 전\n",
    "    [[ 0  0  0  0  2  3]\n",
    "     [ 0  0  0  2  3  1]\n",
    "     [ 0  0  2  3  1  4]\n",
    "     [ 0  2  3  1  4  5]\n",
    "     [ 0  0  0  0  6  1]\n",
    "     [ 0  0  0  6  1  7]\n",
    "     [ 0  0  0  0  8  1]\n",
    "     [ 0  0  0  8  1  9]\n",
    "     [ 0  0  8  1  9 10]\n",
    "     [ 0  8  1  9 10  1]\n",
    "     [ 8  1  9 10  1 11]]\n",
    "\n",
    "    변경 후\n",
    "    [[ 0  0  0  0  2]\n",
    "     [ 0  0  0  2  3]\n",
    "     [ 0  0  2  3  1]\n",
    "     [ 0  2  3  1  4]\n",
    "     [ 0  0  0  0  6]\n",
    "     [ 0  0  0  6  1]\n",
    "     [ 0  0  0  0  8]\n",
    "     [ 0  0  0  8  1]\n",
    "     [ 0  0  8  1  9]\n",
    "     [ 0  8  1  9 10]\n",
    "     [ 8  1  9 10  1]]\n",
    "     \n",
    "     [ 3  1  4  5  1  7  1  9 10  1 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T06:28:01.471183Z",
     "start_time": "2020-09-08T06:28:01.453179Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0,  0,  0,  0,  2],\n",
       "        [ 0,  0,  0,  2,  3],\n",
       "        [ 0,  0,  2,  3,  1],\n",
       "        [ 0,  2,  3,  1,  4],\n",
       "        [ 0,  0,  0,  0,  6],\n",
       "        [ 0,  0,  0,  6,  1],\n",
       "        [ 0,  0,  0,  0,  8],\n",
       "        [ 0,  0,  0,  8,  1],\n",
       "        [ 0,  0,  8,  1,  9],\n",
       "        [ 0,  8,  1,  9, 10],\n",
       "        [ 8,  1,  9, 10,  1]]),\n",
       " array([ 3,  1,  4,  5,  1,  7,  1,  9, 10,  1, 11]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "sequences = np.array(pad_sequences(sequences, maxlen=6, padding='pre'))\n",
    "X = sequences[:,:-1]\n",
    "y = sequences[:,-1]\n",
    "X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제6. 라벨을 아래와 같이 원핫 표현으로 변경하시오 !\n",
    "    변경 전:\n",
    "    [ 3  1  4  5  1  7  1  9 10  1 11]\n",
    "\n",
    "    변경 후:\n",
    "    [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] # 3에 대한 원-핫 벡터\n",
    "     [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] # 1에 대한 원-핫 벡터\n",
    "     [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] # 4에 대한 원-핫 벡터\n",
    "     [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] # 5에 대한 원-핫 벡터\n",
    "     [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] # 1에 대한 원-핫 벡터\n",
    "     [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] # 7에 대한 원-핫 벡터\n",
    "     [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] # 1에 대한 원-핫 벡터\n",
    "     [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.] # 9에 대한 원-핫 벡터\n",
    "     [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] # 10에 대한 원-핫 벡터\n",
    "     [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] # 1에 대한 원-핫 벡터\n",
    "     [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] # 11에 대한 원-핫 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T06:28:02.212366Z",
     "start_time": "2020-09-08T06:28:02.206365Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(t.word_index) + 1\n",
    "y = to_categorical(y, num_classes = vocab_size)\n",
    "y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 자연어 데이터를 위한 모델 구성\n",
    "    Embedding() 관련 용어 설명\n",
    "        단어를 원-핫 벡터로 만드는 과정을 원-핫 인코딩이라고 하였습니다. \n",
    "        이와 대비적으로 단어를 밀집 벡터로 만드는 작업을 워드 임베딩(word embedding)이라고 합니다. \n",
    "        밀집 벡터는 워드 임베딩 과정을 통해 나온 결과므로 임베딩 벡터(embedding vector)라고도 합니다.\n",
    "\n",
    "    Embedding() : \n",
    "        Embedding()은 단어를 밀집 벡터로 만드는 역할을 합니다. \n",
    "        인공 신경망 용어로는 임베딩 층(embedding layer)을 만드는 역할을 합니다. \n",
    "        Embedding()은 정수 인코딩이 된 단어들을 입력을 받아서 임베딩을 수행합니다.\n",
    "        Embedding()은 (number of samples, input_length)인 2D 정수 텐서를 입력받습니다. \n",
    "        이 때 각 sample은 정수 인코딩이 된 결과로, 정수의 시퀀스입니다. \n",
    "        Embedding()은 워드 임베딩 작업을 수행하고  3D 텐서를 리턴합니다.\n",
    "\n",
    "    아래의 코드는 실제 동작되는 코드가 아니라 의사 코드(pseudo-code)로 임베딩의 개념 이해를 돕기 위해서 작성되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제7.  위에서 만든 훈련 데이터 X 와 라벨 y 를 SimpleRNN 신경망에 입력하여 학습 시키시오 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T06:28:05.685684Z",
     "start_time": "2020-09-08T06:28:03.715567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples\n",
      "Epoch 1/200\n",
      "11/11 - 1s\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "2 root error(s) found.\n  (0) Internal:  Blas GEMM launch failed : a.shape=(11, 32), b.shape=(32, 32), m=11, n=32, k=32\n\t [[{{node sequential/simple_rnn/while/body/_1/MatMul_1}}]]\n\t [[Reshape_7/_32]]\n  (1) Internal:  Blas GEMM launch failed : a.shape=(11, 32), b.shape=(32, 32), m=11, n=32, k=32\n\t [[{{node sequential/simple_rnn/while/body/_1/MatMul_1}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_distributed_function_1581]\n\nFunction call stack:\ndistributed_function -> distributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-a375cdc837f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m model.compile(loss='categorical_crossentropy',\n\u001b[0;32m     11\u001b[0m               optimizer='adam', metrics=['accuracy'])\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: 2 root error(s) found.\n  (0) Internal:  Blas GEMM launch failed : a.shape=(11, 32), b.shape=(32, 32), m=11, n=32, k=32\n\t [[{{node sequential/simple_rnn/while/body/_1/MatMul_1}}]]\n\t [[Reshape_7/_32]]\n  (1) Internal:  Blas GEMM launch failed : a.shape=(11, 32), b.shape=(32, 32), m=11, n=32, k=32\n\t [[{{node sequential/simple_rnn/while/body/_1/MatMul_1}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_distributed_function_1581]\n\nFunction call stack:\ndistributed_function -> distributed_function\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN\n",
    "\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=max_len-1)) # 레이블을 분리하였으므로 이제 X의 길이는 5\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=200, verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제8. 다음의 함수를 이용해서 이제 입력된 단어로부터 다음 단어를 예측해서 문장을 생성하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T06:24:00.179942Z",
     "start_time": "2020-09-08T06:23:59.317Z"
    }
   },
   "outputs": [],
   "source": [
    "def sentence_generation(model, t, current_word, n):  # 모델, 토크나이저, 현재 단어, 반복할 횟수\n",
    "    init_word = current_word  # 처음 들어온 단어도 마지막에 같이 출력하기위해 저장\n",
    "    sentence = ''\n",
    "    for _ in range(n):  # n번 반복\n",
    "        encoded = t.texts_to_sequences([current_word])[0]  # 현재 단어에 대한 정수 인코딩\n",
    "        encoded = pad_sequences([encoded], maxlen=5,\n",
    "                                padding='pre')  # 데이터에 대한 패딩\n",
    "        result = model.predict_classes(encoded, verbose=0)\n",
    "\n",
    "    # 입력한 X(현재 단어)에 대해서 Y를 예측하고 Y(예측한 단어)를 result에 저장.\n",
    "        for word, index in t.word_index.items():\n",
    "            if index == result:  # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면\n",
    "                break  # 해당 단어가 예측 단어이므로 break\n",
    "\n",
    "        current_word = current_word + ' ' + word  # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
    "        sentence = sentence + ' ' + word  # 예측 단어를 문장에 저장\n",
    "\n",
    "    # for문이므로 이 행동을 다시 반복\n",
    "\n",
    "    sentence = init_word + sentence\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T06:25:09.198685Z",
     "start_time": "2020-09-08T06:25:09.180688Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-dd3a136972bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# '경마장에'를 시작 단어로 하고 그 뒤에 이어서 나오는 단어로 예측되는 단어를 4개 보여달라\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_generation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'경마장에'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_generation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'그의'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_generation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'가는'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "# '경마장에'를 시작 단어로 하고 그 뒤에 이어서 나오는 단어로 예측되는 단어를 4개 보여달라\n",
    "print(sentence_generation(model, t, '경마장에', 4))\n",
    "print(sentence_generation(model, t, '그의', 2))\n",
    "print(sentence_generation(model, t, '가는', 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T06:24:00.180942Z",
     "start_time": "2020-09-08T06:23:59.321Z"
    }
   },
   "outputs": [],
   "source": [
    "print(sentence_generation(model, t, '경마장에',4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제9. 시작 단어를 다른 단어로 입력해보고 이어나 오는 예상 단어의 개수의 숫자를 더 크게 넣으시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T06:31:14.237677Z",
     "start_time": "2020-09-08T06:31:14.194668Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentence_generation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-eb353cc9c912>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_generation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'말이'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sentence_generation' is not defined"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, t, '말이', 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
