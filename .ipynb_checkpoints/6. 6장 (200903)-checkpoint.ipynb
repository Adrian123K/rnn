{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>■ 복습</b>\n",
    "    컴퓨터에게 인간의 언어를 이해시키기 위한 작업\n",
    "    \n",
    "    2장. 단어의 분산표현 (밀집벡터) : 개선된 통계기법\n",
    "    3장. CBOW 신경망을 이용해서 문장을 학습 : 밀집벡터를 입력층에서 추출\n",
    "    4장. CBOW 신경망을 큰 말뭉치가 들어갈 수 있도록 개선 : 밀집벡터 추출\n",
    "        CBOW 신경망의 문제점\n",
    "            1. 말뭉치가 크면 메모리 사용량이 많다\n",
    "            2. 성능이 느림            \n",
    "    5장. RNN(순환 신경망) : 기억하는 신경망\n",
    "        실습: 작은 말뭉치      -> RNN 신경망에 입력하고 학습을 시킴\n",
    "        실습: 스티브잡스 연설문 -> RNN 신경망에 입력하고 학습을 시킴\n",
    "                         ↑\n",
    "                    문장을 one hot 표현으로 변경\n",
    "        실습: 시계열 데이터    -> RNN 신경망에 입력\n",
    "            가상으로 생성, 1~11월까지의 데이터를 RNN 신경망에 넣고 12월 데이터를 예상하게끔 상관계수로 12월 데이터를 신경망이 잘 맞췄는지 확인\n",
    "    6장. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 1. LSTM 의 큰그림 \n",
    "    이번 6장에서는 가 추가된 RNN 을 살펴봅니다.  RNN 의 큰 문제점 2가지는 기울기 소실 또는 기울기 폭발 이었는데 그것을 대신하는 계층으로써 게이트 가 추가된 RNN 이 큰 효과가 있습니다 \n",
    "    이 계층들에는 게이트 라는 구조가 사용되며, 게이트는 데이터와 기울기 흐름을 적절히 제어하는 메커니즘입니다.\n",
    "    또한, LSTM 계층을 사용한 언어모델을 생성합니다.  그리고 PTB 데이터셋을 학습하여 퍼플렉서티를 구합니다.\n",
    "    나아가 LSTM 다층화, 드롭아웃, 가중치 공유 등의 기법을 적용해 정확도를 큰 폭으로 향상 시킵니다.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 2.  RNN 계층에서 일어나는 일이 무엇인지 아래의 그림을 보고 설명하세요\n",
    "![fig6-2](http://cfile279.uf.daum.net/image/992EEB345F4F5CC42885F8)\n",
    "\n",
    "    RNN 계층은 시계열 데이터인 X1을 입력하면 h1을 출력\n",
    "    h1은 RNN 계층의 은닉상태라고 하며 과거 정보를 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 3. RNN 의 가장 큰 문제점은 무엇인가요 ? \n",
    "    장기기억에 취약하다는 문제점이 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 4. 언어모델의 역할이 무엇인가요 ?\n",
    "    주어진 단어들을 기초로 다음에 출현할 단얼르 예측하는 역할\n",
    "    과거의 주가 데이터를 기초로 다음에 출현할 주가를 예측하는 역할"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 5. 단순한 RNN 계층에서 일어나는 큰 문제점 2가지는 무엇입니까?\n",
    "    1. 기울기 소실\n",
    "    2. 기울기 폭발"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 6.  기울기 소실이 발생하는 이유가 무엇입니까?\n",
    "![fig6-6](http://cfile265.uf.daum.net/image/99A45F425F4F6AA3358D57)\n",
    "\n",
    "    RNN 계층의 활성화 함수가 tanh 함수이기 때문\n",
    "    그래서 이를 개선하기 위해서 ReLU 함수를 사용한다\n",
    "\n",
    "    x가 0으로부터  멀어질 수록 기울기가 작아집니다.\n",
    "        역전파에서는 기울기가 tanh 노드를 지날 때 마다 값은 계속 작아진다는 뜻입니다.  그래서 tanh 함수를 T번 통과하면 기울기도 T번 반복해서 작아집니다.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 7.  기울기 폭발은 왜 발생하나요?\n",
    "![fig6-7](http://cfile260.uf.daum.net/image/999B4E475F4F6C113733DC)\n",
    "\n",
    "    Matmul 때문\n",
    "        Matmul 노드에의 역전파는 dhWt라는 행렬곱으로 기울기를 계산\n",
    "        같은 계산을 시계열 데이터의 시간 크기만큼 반복\n",
    "        기울기는 Matmul 노드를 지날 때 마다 그림 6-7처럼 커지게 된다\n",
    "        \n",
    "    기울기 소실 -> tanh\n",
    "    기울기 폭발 -> Matmul 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 8.  기울기 폭발의 대책은 무엇입니까 ?\n",
    "    기울기 클리핑 기법 사용\n",
    "    기울기가 문턱 값을 초과하면 아래의 식처럼 기울기를 수정\n",
    "![fig](http://cfile277.uf.daum.net/image/9979B94B5F4F6CB829CB3A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 9.  그래서 위의 문제를 LSTM 은 어떻게 해결했습니까 ?\n",
    "![fig6-13](http://cfile252.uf.daum.net/image/995B23485F4F6D2E0676F2)\n",
    "\n",
    "    게이트가 추가된 RNN으로 해결\n",
    "    게이트는 데이터의 흐름을 제어\n",
    "    게이트를 얼마나 열고 닫을지도 데이터를 통해 자동으로 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 10.  게이트가 추가된 RNN 의 종류 2가지가 무엇입니까?\n",
    "    1. LSTM (Long short-term memory models)\n",
    "    2. GRU (Grated Recurrent Unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 11.  RNN 과 LSTM 의 차이가 무엇입니까 ?\n",
    "![fig](http://cfile244.uf.daum.net/image/99E6194E5F4F8F1D14692D)\n",
    "    c라는 경로의 유무\n",
    "        c : 기억셀\n",
    "        h : 은닉 상태"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 12.  은닉상태 ht 는 어떻게 계산되어져 출력되는 것입니까 ?\n",
    "![fig](http://cfile300.uf.daum.net/image/99C197355F4F9144067189)\n",
    "\n",
    "    과거의 기억셀 Ct-1는 어떤 계산에 의해 갱신된 Ct로 출력되고 이 출력된 Ct는 tanh로 계산되어져서 ht로 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 13.  아래의 그림에 output 게이트의 역할이 무엇입니까 ?\n",
    "![fig](http://cfile268.uf.daum.net/image/99DC37415F4F92ED0A287E)\n",
    "\n",
    "    tanh(ct)의 각 원소에 대해서 그것이 다음 시각의 은닉 상태에 얼마나 중요한가를 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 14.  forget 게이트의 역할은 무엇입니까 ?\n",
    "    이전 기억셀 Ct-1의 기억중에서 불필요한 기억을 잊게 해주는 역할\n",
    "![fig](http://cfile294.uf.daum.net/image/991D84445F4F959D0D7DCB)\n",
    "![e](http://cfile271.uf.daum.net/image/99C0644C5F4F956A0F0B3B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 15.  새로운 기억셀의 역할은 무엇입니까 ?\n",
    "    잊어야 할 기억은 잊어버리되 새로 기억해야할 정보는 기억해야하므로 새로 기억해야할 정보를 기억하게 하는 역할\n",
    "![fig](http://cfile268.uf.daum.net/image/99FE20455F4F978B15A4A2)\n",
    "![e](http://cfile256.uf.daum.net/image/99DF44485F4F97EF1F1DCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 16.  input 게이트의 i 역할은 무엇입니까 ?\n",
    "    새로운 정보를 무비판적으로 수용하는게 아니라 적절히 선택하게 하는 역할\n",
    "![fig](http://cfile288.uf.daum.net/image/99572D435F4F98DC0B31BE)\n",
    "![e](http://cfile241.uf.daum.net/image/994BD4425F4F98BF10F2DE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history['val_'+metric], '')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([metric, 'val_'+metric])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True,\n",
    "                          as_supervised=True)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "\n",
    "encoder = info.features['text'].encoder\n",
    "\n",
    "print('Vocabulary size: {}'.format(encoder.vocab_size))\n",
    "\n",
    "sample_string = 'Hello TensorFlow.'\n",
    "\n",
    "encoded_string = encoder.encode(sample_string)\n",
    "print('Encoded string is {}'.format(encoded_string))\n",
    "\n",
    "original_string = encoder.decode(encoded_string)\n",
    "print('The original string: \"{}\"'.format(original_string))\n",
    "\n",
    "assert original_string == sample_string\n",
    "\n",
    "for index in encoded_string:\n",
    "    print('{} ----> {}'.format(index, encoder.decode([index])))\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.padded_batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = test_dataset.padded_batch(BATCH_SIZE)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(encoder.vocab_size, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_dataset, epochs=10,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=30)\n",
    "\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))\n",
    "\n",
    "\n",
    "def pad_to_size(vec, size):\n",
    "    zeros = [0] * (size - len(vec))\n",
    "    vec.extend(zeros)\n",
    "    return vec\n",
    "\n",
    "\n",
    "def sample_predict(sample_pred_text, pad):\n",
    "    encoded_sample_pred_text = encoder.encode(sample_pred_text)\n",
    "\n",
    "    if pad:\n",
    "        encoded_sample_pred_text = pad_to_size(encoded_sample_pred_text, 64)\n",
    "    encoded_sample_pred_text = tf.cast(encoded_sample_pred_text, tf.float32)\n",
    "    predictions = model.predict(tf.expand_dims(encoded_sample_pred_text, 0))\n",
    "\n",
    "    return (predictions)\n",
    "\n",
    "# predict on a sample text without padding.\n",
    "\n",
    "sample_pred_text = ('The movie was cool. The animation and the graphics '\n",
    "                    'were out of this world. I would recommend this movie.')\n",
    "predictions = sample_predict(sample_pred_text, pad=False)\n",
    "print(predictions)\n",
    "\n",
    "# predict on a sample text with padding\n",
    "\n",
    "sample_pred_text = ('The movie was cool. The animation and the graphics '\n",
    "                    'were out of this world. I would recommend this movie.')\n",
    "predictions = sample_predict(sample_pred_text, pad=True)\n",
    "print(predictions)\n",
    "\n",
    "\n",
    "plot_graphs(history, 'accuracy')\n",
    "\n",
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ imb (internet Movie Database) dataset ?\n",
    "    인터넷 영화 데이터베이스에서 수집한 50_000개의 영화 리뷰 텍스트를 담은 데이터 셋\n",
    "        25_000개 리뷰는 훈련용, 25_000개는 테스트로 나눠져있다\n",
    "        긍정적인 리뷰와 부정적인 리뷰의 개수가 동일하게 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제1. 텐써 플로우 버젼을 확인하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제2. imdb 데이터셋에서 훈련 데이터와 테스트 데이터를 가져오는데 매개변수 num_words=10000 을 사용하여 훈련 데이터에서 가장 많이 등장하는 상위 10,000개의 단어를 출력하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = keras.datasets.imdb\n",
    "(train_data, train_labels), (test_data, test_label) = imdb.load_data(num_words=10_000)\n",
    "\n",
    "print(len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제3.  훈련 데이터는 어떻게 생겼으며 훈련 데이터의 라벨은 어떻게 생겼는지 확인하시오 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = keras.datasets.imdb\n",
    "(train_data, train_labels), (test_data, test_label) = imdb.load_data(num_words=10_000)\n",
    "\n",
    "print(train_data[0], train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
    "\n",
    "    1 (label)\n",
    "    \n",
    "    단어id (corpus)로 구성\n",
    "        0은 부정적인 리뷰이고 1은 긍정적인 리뷰 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제4.  첫번째 영화리뷰와 두번째 영화 리뷰의 길이가 서로 다름을 확인하시오 ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data[0]), len(train_data[1])) # 218 189"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제5.  위의 정수로 된 리뷰를 텍스트로 출력하는 함수를 생성하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# 처음 몇 개 인덱스는 사전에 정의되어 있습니다\n",
    "\n",
    "word_index = {k: (v+3) for k, v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "\n",
    "# {34704: 'fawn', 52009: 'tsuk...  }   정수를 단어로 매핑함\n",
    "reverse_word_index = dict([(value, key)\n",
    "                           for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "print(decode_review(train_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    <START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제6. 위에서 word_index 는 무엇인가요 ?\n",
    "    단어와 정수 인덱스(corpus)를 매핑한 딕셔너리 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제7.  print( len(train_data[0]), len(train_data[1]) ) 는 218 189 로 첫번째 리뷰의 길이와 두번째 리뷰의 길이가 서로 틀립니다. 이 리뷰의 길이를  둘다 256으로 맞춰주세요. 남은 부분은 0으로 채워넣으세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
    "                                                        value=word_index[\"<PAD>\"],\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=256)\n",
    "\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
    "                                                       value=word_index[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=256)\n",
    "\n",
    "print(len(train_data[0]), len(train_data[1]))\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제8. 위에서  padding='post' 는 무엇인가요 ?\n",
    "    https://stackoverflow.com/questions/42943291/what-does-keras-io-preprocessing-sequence-pad-sequences-do\n",
    "    >>> pad_sequences([[1, 2, 3], [3, 4, 5, 6], [7, 8]])\n",
    "    array([[0, 1, 2, 3],\n",
    "           [3, 4, 5, 6],\n",
    "           [0, 0, 7, 8]], dtype=int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제9.  모델을 생성하세요 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 16, input_shape=(None,)))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제10. 위의 코드중 Embedding 이 무엇입니까?\n",
    "    단어를 밀집벡터로 만드는 것\n",
    "http://cafe.daum.net/oracleoracle/Sedp/572\n",
    "![emb](http://cfile289.uf.daum.net/image/995FDA355F4E2F0E0D8D0F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제11. 위의 코드중 GlobalAveragePooling1D 는 무엇입니까 ? \n",
    "    입력으로 들어오는 데이터의 shape는 (25000,256,16)\n",
    "    두번째 차원인(리뷰당 단어 개수) 방향으로 평균을 구하여 shape가 (25000,16)인 배열 사용\n",
    "![fig](http://cfile265.uf.daum.net/image/99F7C24B5F5008690AAE67)\n",
    "![fig2](http://cfile284.uf.daum.net/image/99D6704F5F5008770A00CC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제12.  만든 모델로 훈련을 시키세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = train_data[:10000]\n",
    "partial_x_train = train_data[10000:]\n",
    "\n",
    "y_val = train_labels[:10000]\n",
    "partial_y_train = train_labels[10000:]\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=40,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)\n",
    "\n",
    "results = model.eval‎uate(test_data,  test_labels, verbose=2)\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    782/782 - 2s - loss: 0.3258 - accuracy: 0.8725\n",
    "    [0.32584846019744873, 0.8724799752235413]\n",
    "    87% 의 정확도를 보입니다.  고급 방법을 사용한 모델은 정확도를 95% 까지 올립니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제13. RNN 신경망을 사용하여 정확도를 더 올리세요 ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history['val_'+metric], '')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([metric, 'val_'+metric])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "dataset, info = tfds.load('imdb_reviews/subwords8k',\n",
    "                          with_info=True, as_supervised=True)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "\n",
    "encoder = info.features['text'].encoder\n",
    "\n",
    "print('Vocabulary size: {}'.format(encoder.vocab_size))\n",
    "\n",
    "sample_string = 'Hello TensorFlow.'\n",
    "\n",
    "encoded_string = encoder.encode(sample_string)\n",
    "print('Encoded string is {}'.format(encoded_string))\n",
    "\n",
    "original_string = encoder.decode(encoded_string)\n",
    "print('The original string: \"{}\"'.format(original_string))\n",
    "\n",
    "assert original_string == sample_string\n",
    "\n",
    "for index in encoded_string:\n",
    "    print('{} ----> {}'.format(index, encoder.decode([index])))\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.padded_batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = test_dataset.padded_batch(BATCH_SIZE)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(encoder.vocab_size, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_dataset, epochs=10,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=30)\n",
    "\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))\n",
    "\n",
    "\n",
    "def pad_to_size(vec, size):\n",
    "    zeros = [0] * (size - len(vec))\n",
    "    vec.extend(zeros)\n",
    "    return vec\n",
    "\n",
    "\n",
    "def sample_predict(sample_pred_text, pad):\n",
    "    encoded_sample_pred_text = encoder.encode(sample_pred_text)\n",
    "\n",
    "    if pad:\n",
    "        encoded_sample_pred_text = pad_to_size(encoded_sample_pred_text, 64)\n",
    "    encoded_sample_pred_text = tf.cast(encoded_sample_pred_text, tf.float32)\n",
    "    predictions = model.predict(tf.expand_dims(encoded_sample_pred_text, 0))\n",
    "\n",
    "    return (predictions)\n",
    "\n",
    "# predict on a sample text without padding.\n",
    "\n",
    "\n",
    "sample_pred_text = ('The movie was cool. The animation and the graphics '\n",
    "                    'were out of this world. I would recommend this movie.')\n",
    "predictions = sample_predict(sample_pred_text, pad=False)\n",
    "print(predictions)\n",
    "\n",
    "# predict on a sample text with padding\n",
    "\n",
    "sample_pred_text = ('The movie was cool. The animation and the graphics '\n",
    "                    'were out of this world. I would recommend this movie.')\n",
    "predictions = sample_predict(sample_pred_text, pad=True)\n",
    "print(predictions)\n",
    "\n",
    "\n",
    "plot_graphs(history, 'accuracy')\n",
    "\n",
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 문제14.  출력된 결과를 시각화 하시오 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. RNN 신경망으로 영화 평가 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제1.  train_dataset 의  shape 를 확인하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset, info = tfds.load('imdb_reviews/subwords8k',\n",
    "                          with_info=True, as_supervised=True)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "\n",
    "print(train_dataset)\n",
    "print(tf.data.experimental.cardinality(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제2.  train_dataset 에 무엇이 있는지 확인하시오 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_numpy = tfds.as_numpy(train_dataset)\n",
    "for ex in ds_numpy:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제3. info 에 있는 텍스트를 확인하시오 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = info.features['text'].encoder\n",
    "print(encoder.subwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ['the_', ', ', '. ', 'a_', 'and_', 'of_', 'to_', 's_', 'is_', 'br', 'in_', 'I_', 'that_', 'this_', 'it_', ' /><', ' />', 'was_', 'The_', 'as_', 't_', 'with_', 'for_', '.<', 'on_', 'but_', 'movie_', ' (', 'are_', 'his_', 'have_', 'film_', 'not_', 'ing_', 'be_', 'ed_', 'you_', ' \"', 'it', 'd_', 'an_', 'he_', 'by_', 'at_', 'one_', 'who_', 'y_', 'from_', 'e_', 'or_', 'all_', 'like_', 'they_', '\" ', 'so_', 'just_', 'has_', ') ', 'her_', 'about_', 'out_', 'This_', 'some_', 'ly_', 'movie', 'film', 'very_', 'more_', 'It_', 'would_', 'what_', 'when_', 'which_', 'good_', 'if_', 'up_', 'only_', 'even_', 'their_', 'had_', 'really_', 'my_', 'can_', 'no_', 'were_', 'see_', 'she_', '? ', 'than_', '! ', 'there_', 'get_', 'been_', 'into_', ' - ', 'will_', 'much_', 'story_', 'because_', 'ing', 'time_', 'n_', 'we_', 'ed', 'me_', ': ', 'most_', 'other_', 'don', 'do_', 'm_', 'es_', 'how_', 'also_', 'make_', 'its_', 'could_', 'first_', 'any_', \"' \", 'people_', 'great_', 've_', 'ly', 'er_', 'made_', 'r_', 'But_', 'think_', \" '\", 'i_', 'bad_', 'A_', 'And_', 'It', 'on', '; ', 'him_', 'being_', 'never_', 'way_', 'that', 'many_', 'then_', 'where_', 'two_', 'In_', 'after_', 'too_', 'little_', 'you', '), ', 'well_', 'ng_', 'your_', 'If_', 'l_', '). ', 'does_', 'ever_', 'them_', 'did_', 'watch_', 'know_', 'seen_', 'time', 'er', 'character_', 'over_', 'characters_', 'movies_', 'man_', 'There_', 'love_', 'best_', 'still_', 'off_', 'such_', 'in', 'should_', 'the', 're_', 'He_', 'plot_', 'films_', 'go_', 'these_', 'acting_', 'doesn', 'es', 'show_', 'through_', 'better_', 'al_', 'something_', 'didn', 'back_', 'those_', 'us_', 'less_', '...', 'say_', 'is', 'one', 'makes_', 'and', 'can', 'all', 'ion_', 'find_', 'scene_', 'old_', 'real_', 'few_', 'going_', 'well', 'actually_', 'watching_', 'life_', 'me', '. <', 'o_', 'man', 'there', 'scenes_', 'same_', 'he', 'end_', 'this', '... ', 'k_', 'while_', 'thing_', 'of', 'look_', 'quite_', 'out', 'lot_', 'want_', 'why_', 'seems_', 'every_', 'll_', 'pretty_', 'got_', 'able_', 'nothing_', 'good', 'As_', 'story', ' & ', 'another_', 'take_', 'to', 'years_', 'between_', 'give_', 'am_', 'work_', 'isn', 'part_', 'before_', 'actors_', 'may_', 'gets_', 'young_', 'down_', 'around_', 'ng', 'thought_', 'though_', 'end', 'without_', 'What_', 'They_', 'things_', 'life', 'always_', 'must_', 'cast_', 'almost_', 'h_', '10', 'saw_', 'own_', 'here', 'bit_', 'come_', 'both_', 'might_', 'g_', 'whole_', 'new_', 'director_', 'them', 'horror_', 'ce', 'You_', 'least_', 'bad', 'big_', 'enough_', 'him', 'feel_', 'probably_', 'up', 'here_', 'making_', 'long_', 'her', 'st_', 'kind_', '--', 'original_', 'fact_', 'rather_', 'or', 'far_', 'nt_', 'played_', 'found_', 'last_', 'movies', 'When_', 'so', '\", ', 'comes_', 'action_', 'She_', 've', 'our_', 'anything_', 'funny_', 'ion', 'right_', 'way', 'trying_', 'now_', 'ous_', 'each_', 'done_', 'since_', 'ic_', 'point_', '\". ', 'wasn', 'interesting_', 'c_', 'worst_', 'te_', 'le_', 'ble_', 'ty_', 'looks_', 'show', 'put_', 'looking_', 'especially_', 'believe_', 'en_', 'goes_', 'over', 'ce_', 'p_', 'films', 'hard_', 'main_', 'be', 'having_', 'ry', 'TV_', 'worth_', 'One_', 'do', 'al', 're', 'again', 'series_', 'takes_', 'guy_', 'family_', 'seem_', 'plays_', 'role_', 'away_', 'world_', 'My_', 'character', ', \"', 'performance_', '2_', 'So_', 'watched_', 'John_', 'th_', 'plot', 'script_', 'For_', 'sure_', 'characters', 'set_', 'different_', 'minutes_', 'All_', 'American_', 'anyone_', 'Not_', 'music_', 'ry_', 'shows_', 'too', 'son_', 'en', 'day_', 'use_', 'someone_', 'for', 'woman_', 'yet_', '.\" ', 'during_', 'she', 'ro', '- ', 'times_', 'left_', 'used_', 'le', 'three_', 'play_', 'work', 'ness_', 'We_', 'girl_', 'comedy_', 'ment_', 'an', 'simply_', 'off', 'ies_', 'funny', 'ne', 'acting', 'That_', 'fun_', 'completely_', 'st', 'seeing_', 'us', 'te', 'special_', 'ation_', 'as', 'ive_', 'ful_', 'read_', 'reason_', 'co', 'need_', 'sa', 'true_', 'ted_', 'like', 'ck', 'place_', 'they', '10_', 'However', 'until_', 'rest_', 'sense_', 'ity_', 'everything_', 'people', 'nt', 'ending_', 'again_', 'ers_', 'given_', 'idea_', 'let_', 'nice_', 'help_', 'no', 'truly_', 'beautiful_', 'ter', 'ck_', 'version_', 'try_', 'came_', 'Even_', 'DVD_', 'se', 'mis', 'scene', 'job_', 'ting_', 'Me', 'At_', 'who', 'money_', 'ment', 'ch', 'recommend_', 'was', 'once_', 'getting_', 'tell_', 'de_', 'gives_', 'not', 'Lo', 'we', 'son', 'shot_', 'second_', 'After_', 'To_', 'high_', 'screen_', ' -- ', 'keep_', 'felt_', 'with', 'great', 'everyone_', 'although_', 'poor_', 'el', 'half_', 'playing_', 'couple_', 'now', 'ble', 'excellent_', 'enjoy_', 'couldn', 'x_', 'ne_', ',\" ', 'ie_', 'go', 'become_', 'less', 'himself_', 'supposed_', 'won', 'understand_', 'seen', 'ally_', 'THE_', 'se_', 'actor_', 'ts_', 'small_', 'line_', 'na', 'audience_', 'fan_', 'et', 'world', 'entire_', 'said_', 'at', '3_', 'scenes', 'rs_', 'full_', 'year_', 'men_', 'ke', 'doing_', 'went_', 'director', 'back', 'early_', 'Hollywood_', 'start_', 'liked_', 'against_', 'remember_', 'love', 'He', 'along_', 'ic', 'His_', 'wife_', 'effects_', 'together_', 'ch_', 'Ra', 'ty', 'maybe_', 'age', 'S_', 'While_', 'often_', 'sort_', 'definitely_', 'No', 'script', 'times', 'absolutely_', 'book_', 'day', 'human_', 'There', 'top_', 'ta', 'becomes_', 'piece_', 'waste_', 'seemed_', 'down', '5_', 'later_', 'rs', 'ja', 'certainly_', 'budget_', 'th', 'nce_', '200', '. (', 'age_', 'next_', 'ar', 'several_', 'ling_', 'short_', 'sh', 'fe', 'Of_', 'instead_', 'Man', 'T_', 'right', 'father_', 'actors', 'wanted_', 'cast', 'black_', 'Don', 'more', '1_', 'comedy', 'better', 'camera_', 'wonderful_', 'production_', 'inter', 'course', 'low_', 'else_', 'w_', 'ness', 'course_', 'based_', 'ti', 'Some_', 'know', 'house_', 'say', 'de', 'watch', 'ous', 'pro', 'tries_', 'ra', 'kids_', 'etc', ' \\x96 ', 'loved_', 'est_', 'fun', 'made', 'video_', 'un', 'totally_', 'Michael_', 'ho', 'mind_', 'No_', 'Be', 'ive', 'La', 'Fi', 'du', 'ers', 'Well', 'wants_', 'How_', 'series', 'performances_', 'written_', 'live_', 'New_', 'So', 'Ne', 'Na', 'night_', 'ge', 'gave_', 'home_', 'heart', 'women_', 'nu', 'ss_', 'hope_', 'ci', 'friends_', 'Se', 'years', 'sub', 'head_', 'Y_', 'Du', '. \"', 'turn_', 'red_', 'perfect_', 'already_', 'classic_', 'tri', 'ss', 'person_', 'star_', 'screen', 'style_', 'ur', 'starts_', 'under_', 'Then_', 'ke_', 'ine', 'ies', 'um', 'ie', 'face_', 'ir', 'enjoyed_', 'point', 'lines_', 'Mr', 'turns_', 'what', 'side_', 'sex_', 'Ha', 'final_', ').<', 'With_', 'care_', 'tion_', 'She', 'ation', 'Ar', 'ma', 'problem_', 'lost_', 'are', 'li', '4_', 'fully_', 'oo', 'sha', 'Just_', 'name_', 'ina', 'boy_', 'finally_', 'ol', '!<', 'Bo', 'about', 'though', 'hand', 'ton', 'lead_', 'school_', 'ns', 'ha', 'favorite_', 'stupid_', 'gi', 'original', 'mean_', 'To', 'took_', 'either_', 'ni', 'book', 'episode_', 'om', 'Su', 'D_', 'Mc', 'house', 'cannot_', 'stars_', 'behind_', 'see', 'other', 'Che', 'role', 'art', 'ever', 'Why_', 'father', 'case_', 'tic_', 'moments_', 'Co', 'works_', 'sound_', 'Ta', 'guess_', 'perhaps_', 'Vi', 'thing', 'fine_', 'fact', 'music', 'non', 'ful', 'action', 'ity', 'ct', 'ate_', 'type_', 'lack_', 'death_', 'art_', 'able', 'Ja', 'ge_', 'wouldn', 'am', 'tor', 'extremely_', 'pre', 'self', 'Mor', 'particularly_', 'bo', 'est', 'Ba', 'ya', 'play', 'Pa', 'ther', 'heard_', 'however', 'ver', 'dy_', 'Sa', 'ding_', 'led_', 'late_', 'feeling_', 'per', 'low', 'ably_', 'Un', 'On_', 'known_', 'kill_', 'fight_', 'beginning_', 'cat', 'bit', 'title_', 'vo', 'short', 'old', 'including_', 'Da', 'coming_', 'That', 'place', 'looked_', 'best', 'Lu', 'ent_', 'bla', 'quality_', 'except_', '...<', 'ff', 'decent_', 'much', 'De', 'Bu', 'ter_', 'attempt_', 'Bi', 'taking_', 'ig', 'Ti', 'whose_', 'dialogue_', 'zz', 'war_', 'ill', 'Te', 'war', 'Hu', 'James_', '..', 'under', 'ring_', 'pa', 'ot', 'expect_', 'Ga', 'itself_', 'line', 'lives_', 'let', 'Dr', 'mp', 'che', 'mean', 'called_', 'complete_', 'terrible_', 'boring_', 'others_', '\" (', 'aren', 'star', 'long', 'Li', 'mother_', 'si', 'highly_', 'ab', 'ex', 'os', 'nd', 'ten_', 'ten', 'run_', 'directed_', 'town_', 'friend_', 'David_', 'taken_', 'finds_', 'fans_', 'Mar', 'writing_', 'white_', 'u_', 'obviously_', 'mar', 'Ho', 'year', 'stop_', 'f_', 'leave_', 'king_', 'act_', 'mind', 'entertaining_', 'ish_', 'Ka', 'throughout_', 'viewer_', 'despite_', 'Robert_', 'somewhat_', 'hour_', 'car_', 'evil_', 'Although_', 'wrong_', 'Ro', 'dead_', 'body_', 'awful_', 'home', 'exactly_', 'bi', 'family', 'ts', 'usually_', 'told_', 'z_', 'oc', 'minutes', 'tra', 'some', 'actor', 'den', 'but', 'Sha', 'tu', 'strong_', 'Jo', 'real', 'la', 'gin', 'ul', 'amazing_', 'save_', 'wrong', 'dis', 'obvious_', 'close_', 'sometimes_', 'shown_', 'head', 'land', 'Go', 'mer', 'ending', 'else', 'audience', 'su', 'parts_', 'ga', 'before', 'cinema', 'opening_', 'laugh_', 'Ca', 'sh_', 'guys_', 'ds_', 'number_', 'Ma', 'soon_', 'ob', 'po', 'wonder_', 'group_', 'men', 'Mac', 'thinking_', 'fan', 'across_', 'turned_', 'ant', 'tells_', 'em', 'night', 'ton_', 'picture_', 'past_', 'Hi', 'girl', 'ght', 'woman', 'started_', 'ba', 'Ru', 'da', 'wi', 'running_', 'part', 'wish_', 'ner', 'ap', 'rn', 'ant_', 'mon', 'ast', 'awful', 'Yes', 'The', 'ard', 'nce', 'era', 'today', 'ad', 'Now_', '.) ', 'local_', 'killer_', 'huge_', 'flick', 'ends_', 'light', 'ons_', 'Al', 'knew_', 'due_', 'direction_', 'close', 'Gra', 'od', 'giving_', 'Le', 'op', 'Pe', 'ey_', 'wa', 'sta', 'worse_', 'single_', 'cut_', 'light_', 'ia', 'happens_', 'supporting_', 'room_', 'girls_', 'female_', 'E_', 'falls_', 'nd_', 'ish', 'mostly_', 'tan', 'major_', 'bring_', 'killed_', 'ele', 'el_', 'dark_', 'myself_', 'Pro', 'ent', 'ated_', 'British_', 'va', '....', 'talking_', 'con', 'tion', 'children_', 'by', 'voice_', 'sense', 'Car', '.. ', 'ain', 'For', 'Con', 'performance', 'au', 'stories_', 'ine_', 'Or', 'order_', 'first', 'ac', '8_', 'involved_', 'interesting', 'drama_', 'Dan', 'away', 'From_', 'ping_', 'boy', 'air', 'sing_', 'lle', 'You', 'lo', 'ian', 'ingly_', 'ia_', 'haven', 'using_', 'fo', 'dy', 'modern_', 'ST', 'wife', 'unt', 'game_', 'together', 'pp', 'clearly_', 'First_', 'sad', 'ris', 'ven', 'col', 'Maybe_', 'val', 'sexual_', 'serious_', 'relationship_', 'musical_', 'boring', 'But', 'hit_', 'brilliant_', 'easily_', 'living_', 'ca', 'police_', 'ip', ' , ', 'feels_', 'effects', 'sex', 'ist_', 'die', 'para', 'ort', 'humor_', 'Cor', 'ist', 'et_', 'Richard_', 'call_', 'example', 'appears_', 'actress_', 'rit', 'matter_', 'ar_', 'ns_', 'needs_', 'important_', 'fli', 'ec', 'stupid', 'ee', 'change_', 'bur', ' . ', 'comic_', 'DVD', 'We', '?<', 'Paul_', 'child_', 'ag', 'enjoy', 'cha', 'actual_', 'says_', 'nearly_', 'heart_', 'did', 'similar_', 'side', 'ru', 'ped_', 'und', 'super', 'name', 'clear_', \"', \", 'cu', 'child', 'moment_', 'ions_', 'fall_', 'done', 'chance_', 'then', 'ian_', 'George_', 'exc', 'enough', 'Jack_', 'win', 'Di', 'ying_', 'said', '80', 'ze', 'example_', 'themselves_', 'named_', 'ger', 'near_', 'guy', 'car', 'horrible_', 'bri', '!! ', 'ori', 'his', 'ded_', 'An_', 'released_', 'laugh', 'kept_', 'beyond_', 'b_', 'Sch', 'An', 'Lan', 'In', 'gar', 'genre', 'cho', 'Har', 'title', 'romantic_', 'mother', 'English_', 'mention_', 'interest_', 'Its_', 'money', 'face', 'brought_', 'ut', 'after', 'Win', 'working_', 'ny', 'knows_', 'happened_', 'certain_', '6_', 'within_', 'usual_', 'upon_', 'il', 'Her_', 'from', 'drama', 'Si', 'Mo', 'God', 'five_', 'whether_', 'tried_', 'ial_', 'history_', 'far', 'Re', 'novel', 'chi', 'inc', 'ure_', 'ied_', 'anti', 'Mad', 'lly_', 'Is_', '7_', 'ess', 'bunch_', 'vin', 'slow_', 'style', 'hi', 'eyes_', 'cinema_', 'showing_', 'gen', 'ra_', 'among_', 'unc', 'Po', 'Peter_', 'kid_', 'ght_', 'ny_', 'gh', 'tro', 'four_', 'ue', 'ley_', 'stuff_', 'strange_', 'sit_', 'sch', 'anyway', '199', 'hours_', 'These_', 'Most_', 'own', 'ned_', 'ban', 'Fa', 'decided_', 'xi', 'top', 'll', 'get', 'events_', 'Also_', 'typical_', 'shots_', 'look', 'happy_', 'um_', 'simple_', 'either', 'comment', 'ssi', 'ps', 'Bar', 'Per', 'saying_', 'none_', 'surprised_', 'sse', 'ka', 'ily_', 'horror', 'dig', 'tt', 'ric', 'post', 'TV', '198', '* ', 'half', 'gn', 'ste', 'ls', 'hero_', 'Pi', 'Like_', 'sad_', 'hear_', 'begins_', 'rent_', 'ure', 'rie', 'greatest_', 'Je', 'van', 'sci', 'kid', 'himself', 'Also', 'view_', 'score_', 'dge', 'became_', 'Cra', '197', 'ones_', 'cal', '9_', 'hor', 'hand_', 'days_', 'yourself_', 'tle', 'gan', 'ea', 'ago', 'WA', 'pen', 'ls_', 'learn_', 'Sta', 'By_', 'middle_', 'job', 'uc', 'ko', 'bar', 'lots_', 'cheap_', 'fi', 'stay_', 'stand_', 'pri', 'za', 'im', 'ight', 'happen_', 'Ab', 'Gar', 'ore', 'lan', 'classic', 'writer_', 'ster', 'picture', 'hate_', 'der', 'grand', 'disc', 'Mi', 'ud', 'é', 'murder_', 'basically_', 'jokes_', 'famous_', 'eg', 'easy_', 'rm', 'der_', 'R_', 'Mat', 'two', 'daughter', 'Spi', 'camera', 'AN', 'glo', 'talk_', 'daughter_', 'Fre', 'ri', 'perfect', 'experience_', 'buy_', 'zo', 'bu', 'Pu', 'Col', 'uni', 'later', 'children', 'sets_', 'annoying_', 'Tom_', 'uses_', 'jo', 'dead', 'psycho', 'mid', 'room', 'ki', 'hope', 'dialogue', 'attention_', 'cc', 'above_', 'possibly_', 'mo', 'difficult_', 'Mon', 'Japanese_', '!\" ', 'death', 'class_', ': \"', 'tic', 'ler', 'bus', 'genre_', 'stre', 'keeps_', 'cre', 'una', 'tly_', 'leaves_', 'RE', 'yes', 'realize_', 'nor_', 'figure_', 'Chan', 'rec', 'minute_', 'leading_', 'high', 'gui', 'ug', 'sequence_', 'na_', 'help', 'ani', 'Who_', 'exist', 'documentary_', 'sal', 'pe', 'key_', 'Bra', 'murder', 'leg', 'songs_', 'production', 'dle', 'cla', 'arm', 'US', \"'. \", 'reason', 'moving_', 'alone_', 'Ko', 'Bel', 'fu', 'elements_', 'Ste', 'prof', 'ning_', 'ey', 'dark', 'tur', 'les_', 'Ni', 'NOT_', 'ps_', 'bor', 'ary_', ' />\"', 'tter', 'level_', 'ys', 'apparently_', 'poorly_', 'meets_', 'killing_', 'id', 'ging_', 'ep', 'emotional_', 'brings_', 'means_', 'fla', 'episodes_', 'doubt_', 'camp', 'ME', 'Ad', 'sen', 'opinion', 'nch', 'ell', 'Ri', 'writer', 'something', 'Fe', 'flick_', 'flaw', 'ath', 'net', 'lines', 'cinematography_', 'straight_', 'slow', 'lu', 'ber', 'shi', 'husband_', 'forward_', 'form_', 'cra', 'ay', 'Fo', 'Another_', 'wo', 'whom_', 'reality_', 'hold_', 'Chi', 'Bro', 'roles_', 'move_', 'fire', 'brother_', 'Gi', 'Ben', 'review', 'que', 'cri', 'television_', 'overall_', 'French_', 'violence_', 'lla', 'enti', 'ass', 'previous_', 'forced_', 'cop', 'Oscar_', 'DE', 'possible_', 'hat', 'ear', 'budget', 'Tu', 'Ber', 'start', 'nti', 'hard', 'yn', 'school', 'deal_', 'rest', 'problems_', 'lie', 'ite', 'cool_', 'add_', 'towards_', 'reading_', 'LO', 'Gold', 'regard', 'itself', 'OK', 'leads_', 'id_', 'ved_', 'moments', 'dia', 'aw', '!) ', ' $', 'write_', 'theme_', 'Wo', 'filmed_', 'use', 'talent_', 'silly_', 'personal_', 'performances', 'needed_', 'mit', 'meant_', 'cli', 'Sho', 'tain', 'Pri', 'whi', 'comments_', 'city_', 'various_', 'sing', 'rate_', 'create_', 'respect', 'port', 'act', '194', 'message_', 'ted', 'dance_', 'case', 'ves_', 'song_', 'somehow_', 'incredibly_', 'points_', 'manages_', 'career_', 'begin_', 'Tra', 'RI', '20_', 'lai', 'interested_', 'terrible', 'hell_', 'har', 'Ku', 'Ger', 'video', 'ren', 'ky_', 'Ap', 'review_', 'ds', 'blood', 'worse', 'new', 'des', 'ways_', 'read', 'herself_', 'fre', ' *', 'set', 'rated_', 'friends', 'feature_', 'eventually_', 'blood_', 'Sea', 'ving_', 'enjoyable_', 'appear_', 'Stan', 'SE', 'thought', 'suit', 'qui', 'political_', 'person', 'les', 'gla', 'around', 'think', 'len', 'hit', 'direction', 'tale_', 'mess', 'dramatic_', 'ual_', 'gore_', 'Can', 'Am', 'ver_', 'others', 'ju', 'fairly_', 'dan', 'power_', 'dro', 'count', 'Her', 'une', 'third_', 'rop', 'crap', 'ai', 'ade', 'Joe_', 'town', 'ridiculous_', 'gone_', 'William_', 'particular_', 'older_', 'male_', 'humor', 'ard_', 'where', 'run', 'ld', 'bb', 'C_', 'ther_', 'sp', 'plenty_', 'ling', 'future_', 'stars', 'sin', 'pi', 'meet_', 'lt', 'da_', 'check_', 'En', '?\" ', 'ball', 'animation_', 'ta_', 'King_', 'hardly_', 'cul', '60', 'rt', 'Is', 'rai', 'land_', 'clu', 'wise', 'fast_', 'class', 'bra', 'worked_', 'question', 'per_', 'ok', 'expecting_', 'front_', 'come', 'Cu', 'scary_', 'past', 'hero', 'Mel', 'gri', 'average_', 'writers_', 'nk', 'fashion', 'dream', 'bear', 'attempts_', 'stand', 'total_', 'through', 'sm', 'ms', 'ice', 'gs_', 'eye', 'effort_', 'ale', 'warm', 'note', 'ger_', 'follow_', 'cro', 'vis', 'subject_', 'reviews_', 'mm', 'ect', 'Wa', 'Rob', 'imagine_', 'however_', 'decides_', 'brother', 'achieve', 'things', 'stage_', 'sound', 'rating_', 'ously_', 'ier', 'features_', 'ase', 'Vo', 'really', 'pay', 'pal', 'filled_', 'Disney_', 'telling_', 'join', 'coa', 'Lee_', 'team_', 'ov', 'emp', 'days', 'bin', 'ann', 'ally', 'women', 'social_', 'friend', 'vic', 'novel_', 'gle', 'ance_', 'weak_', 'viewers_', 'sy', 'fort', 'idea', 'Mu', 'MA', 'thriller', 'medi', 'forget_', 'York_', 'Au', 'stuff', 'ons', 'hilarious_', 'career', 'Ke', 'Christ', 'ors_', 'mentioned_', 'mark', 'def', 'watching', 'version', 'lor', 'flo', 'country_', 'G_', 'Bat', 'plain_', 'Sam', 'Anyway', 'lic', 'expected_', 'Tru', 'Great_', 'Ser', 'N_', 'And', '?) ', 'san', 'hr', 'Ham', 'pay_', 'lea', 'hol', 'Unfortunately', 'Luc', 'uti', 'row', 'history', 'bea', 'What', 'Or_', 'unless_', 'ica', 'episode', 'stra', 'sounds_', 'ability_', 'Cha', 'sco', 'represent', 'portrayed_', 'outs', 'dri', 'crap_', 'Oh', 'word_', 'open_', 'fantastic_', 'II', 'power', 'ical_', 'badly_', 'Well_', 'IN', 'Angel', 'waiting_', 'sees_', 'mor', 'ari', 'tom', 'sli', 'nation', 'mi', 'inf', 'Mil', 'viewing_', 'rt_', 'premise_', 'ma_', 'fit_', 'wl', 'unique_', 'talent', 'stay', 'fails_', 'breath', 'thi', 'ert', 'Sco', 'talk', 'slightly_', 'je', 'ah', 'NE', 'Fin', 'ridiculous', 'la_', 'Ki', 'vir', 'hea', 'ely_', 'beautiful', 'admit_', 'pu', 'crime_', 'comment_', '0_', 'shot', 'free_', 'entertaining', 'deserves_', 'mas', 'dialog_', 'hip', 'ff_', 'talented_', 'runs_', 'ini', 'ew', 'ded', 'Gri', 'roles', 'realistic_', 'clo', 'ana', 'Rat', 'Oh_', 'Man_', 'Den', 'spent_', 'rse', 'die_', 'Spe', 'Dra', 'ord', 'mal', 'ism', 'del', 'War', 'Cro', 'nn', 'min', 'fighting_', 'excellent', 'ct_', 'ask_', 'abo', 'parents_', 'ou', 'flash', 'Ver', 'Star', 'ym', 'score', 'nature_', 'den_', 'cou', 'body', 'aff', 'Ze', 'Pat', 'Mal', 'lab', 'wing_', 'theater_', 'sho', 'ow', 'mini', 'biggest_', 'Best_', 'wrote_', 'perfectly_', 'pack', 'ile', 'bly_', 'agree_', 'Perhaps_', '-- ', 'sign', 'di', 'cer', 'caught_', 'Good_', 'visual_', 'roll', 'my', 'memorable_', 'kids', 'ise_', 'hin', 'bre', 'beat', 'ring', 'reveal', 'res', 'pit', 'fa', '70', 'words_', 'wn', 'wait_', 'storyline_', 'make', 'ended_', 'ship_', 'ose', 'hot_', 'add', 'DO', 'ib', 'eri', 'directors_', 'amount_', 'Sure', 'ua', 'tin', 'mu', 'hilarious', 'eti', 'deep_', 'battle_', 'bas', 'Pre', 'Ali', 'tre', 'tie', 'thriller_', 'spirit', 'sister', 'ship', 'ser', 'rl', 'rich_', 'outside_', 'ato', 'ad_', 'Do', 'weren', 'sla', 'ro_', 'large_', 'craft', 'Shi', 'ye', 'true', 'spend_', 'rd', 'entirely_', 'Do_', 'wit', 'quickly_', 'powerful_', 'ary', 'Jane_', '193', 'sti', 'ph', 'mel', 'list', 'interest', 'footage_', 'comm', 'Tri', 'vers', 'spe', 'sna', 'sequences_', 'present', 'casting_', 'Star_', 'M_', ').', 'shoot', 'result_', 'gre', 'fore', 'ete', 'break', 'soundtrack_', 'sion_', 'poor', 'lay', 'eas', 'black', 'temp', 'nda', 'king', 'compared_', 'chu', 'break_', 'Ben_', 'ute', 'recent_', 'pure_', 'oi', 'lie_', 'burn', 'uns', 'rip', 'ner_', 'late', 'husband', 'former_', 'dull_', 'argu', 'Hollywood', 'nc', 'ming_', 'lin', 'atmosphere_', 'wood', 'why', 'amazing', 'ron', 'rat', 'gra', 'sed_', 'period_', 'game', 'Sto', 'win_', 'ult', 'scar', 'pun', 'hei', ' `', 'release_', 'present_', 'pin', 'ks_', 'appreciate_', '00', 'jump', 'bomb', 'HA', 'showed_', 'nan', 'kills_', 'decade', 'NO', 'Boy', 'ting', 'rating', 'editing_', 'actress', 'Wal', 'Ea', '\", \"', 'weird_', 'inside_', 'hair', 'eli', 'disappointed_', 'Wor', 'ski', 'ings_', 'fast', 'drag', 'adapt', 'TO', 'NG_', 'sequel_', 'fle', 'Sand', 'RO', 'whatever_', 'sleep', 'sca', 'ret', 'ney_', 'creepy_', 'cal_', '\") ', 'sor', 'popular_', 'nne', 'kick', 'ht', 'display', 'another', 'ves', 'please_', 'moves_', 'care', 'bet', 'bat', 'War_', 'CO', 'program', 'predictable_', 'positive_', 'hing_', 'copy_', 'bia', 'anything', 'affect', 'thrill', 'rk', 'mark_', 'ism_', 'edit', 'Bri', 'rate', 'missing_', 'ila', 'ial', 'guess', 'ft', 'entr', 'decide_', '30', 'sun', 'filmmakers_', 'box_', 'ating_', 'Cla', 'CA', '18', 'nie', 'material_', 'married_', 'hu', 'fin', 'blo', 'Wood', 'Tom', 'vi', 'oni', 'ena', 'BA', 'path', 'os_', 'human', 'mag', 'ins', 'earlier_', 'TI', 'LA', 'Far', 'portrayal_', 'orc', 'lame_', 'ks', 'form', 'call', 'acted_', 'Christmas_', 'violence', 'superb_', 'idiot', 'follow', 'blow', 'SO', 'Les', 'Bill_', '30_', 'sorry_', 'created_', 'common_', 'cheesy_', 'Lea', 'Carl', '!!! ', 'question_', 'pt', 'pick', 'med_', 'leaving_', 'box', 'Ci', 'Bla', 'AR', '\".<', 'ze_', 'makers_', 'draw', 'ala', 'Day', 'B_', 'succeed', 'pat', 'ones', 'gay_', 'cy', 'barely_', 'ara', 'air_', 'San', 'Director_', 'xt', 'screenplay_', 'pan', 'miss_', 'does', 'consider_', 'com', 'ER', 'ub', 'ple', 'mystery_', 'mine', 'involving_', 'familiar_', 'Mari', 'German_', 'nat', 'eye_', 'dly_', 'disa', 'country', 'att', 'app', 'tho', 'press', 'mat', 'llo', 'fi_', 'connect', 'called', 'ane', 'May', 'LE', 'K_', 'Italian_', 'Every_', 'sure', 'ster_', 'starring_', 'horse', 'further_', 'entertainment_', 'ense', 'dog', 'disappointed', 'cher', 'af', 'won_', 'secret', 'likes_', 'indi', 'follows_', 'ball_', 'God_', 'Cur', '196', 'wasted_', 'ideas_', 'cur', 'Bal', 'lly', 'ire', 'gu', 'general_', 'believable_', 'aus', 'Stu', 'Despite_', 'understand', 'lit', 'last', 'cy_', 'bought_', 'ago_', 'Very_', 'Only_', 'Han', 'wear', 'thu', 'themselves', 'recently_', 'ms_', 'intention', 'focus_', 'ations_', 'ali', 'yp', 'yet', 'ici', 'gy', 'exten', 'Min', 'Lin', 'Ed', 'Dar', 'tis', 'credits_', 'Now', '50', 'sister_', 'setting_', 'odd_', 'missed_', 'mea', 'lot', 'ight_', 'gg', 'fantasy_', 'ash', 'US_', 'Overall', 'young', 'suddenly_', 'nge', 'members_', 'dra', 'cover_', 'artist', 'Watch_', 'moment', 'background_', '.....', 'seriously_', 'mic', 'considered_', 'Ric', 'Pres', '! <', ' (\"', 'opinion_', 'ise', 'gun', 'different', 'Sou', 'utterly_', 'asse', 'alt', 'Though_', 'LY_', 'Big_', 'situation_', 'rio', 'il_', 'ef', 'ding', 'Still', 'Cre', 'younger_', 'special', 'raise', 'El', '90', 'walk_', 'tone_', 'tes_', 'sitting_', 'glad_', 'base', 'Let', 'Boo', 'vent', 'lead', 'considering_', 'animated_', 'witness', 'torture', 'throw', 'sea', 'load', 'lim', 'hot', 'following_', 'ess_', 'center', 'Scott_', 'NG', 'BO', '15_', 'word', 'rid', 'pop', 'ions', 'ges', 'enter', 'Sal', 'Gre', 'ties_', 'spl', 'hy', 'ery_', 'disappointment', 'avoid_', 'Jud', 'Ce', 'need', 'hel', 'hands_', 'develop', 'cause_', 'Steve_', 'zombie_', 'voice', 'successful_', 'eo', 'Mary_', 'EN', 'Because_', 'stage', 'rv', 'master', 'crazy_', 'Mer', 'rent', 'hes', 'OF_', 'yl', 'tive_', 'remake_', 'passion', 'managed_', 'fra', 'fans', 'drive', 'CH', 'Blo', 'Art', 'surprise_', 'suggest', 'list_', 'imme', 'crew_', 'continu', 'Sci', 'solid_', 'ora', 'eu', 'Men', 'Cal', 'sus', 'shar', 'omi', 'ita', 'istic_', 'Pl', 'Jack', 'Davi', 'wonder', 'slasher_', 'produced_', 'frame', 'cle', 'Em', 'subs', 'state', 'seek', 'ona', 'mention', 'laughing_', 'iti', 'hide', 'date', 'Some', 'touch', 'soft', 'shop', 'interview', 'dumb_', 'clean', 'bored_', 'bill', 'bed_', 'beauty_', 'basic_', 'Cou', 'zi', 'ultimately_', 'thinks_', 'sto', 'odd', 'masterpiece', 'kind', 'cool', 'Ac', 'tto', 'sit', 'nci', 'ized_', 'gore', 'dee', 'boo', 'Va', 'Come', 'ning', 'escape', 'eng', 'RA', 'America', 'worthy_', 'unre', 'tche', 'shame_', 'nothing', 'explo', 'Sl', 'Bus', 'BE', '13', 'pra', 'least', 'effect_', 'deliver', 'boys_', 'Wi', 'Stra', 'Fr', 'Cap', '**', '\".', 'space_', 'potential_', 'oli', 'lon', 'ind', 'gor', 'gon', 'generally_', 'ext', 'chees', 'beginning', 'Tony_', 'wait', 'meaning', 'ley', 'fire_', 'des_', 'cop_', 'ati', 'Ram', 'Ex', '195', 'were', 'survive', 'ral_', 'push', 'mut', 'killer', 'dist', 'charm', 'ang', 'Frank', 'writing', 'worth', 'wor', 'stop', 'stick_', 'ler_', 'chemistry_', 'cap', 'ae', 'Ya', 'second', 'ost', 'machine', 'lessly_', 'individual', 'experience', 'ead', 'dancing_', 'Sy', 'Del', 'Bor', '!!', 'would', 'suspense_', 'project', 'intelligent_', 'cover', 'asi', 'Brit', 'speak_', 'season_', 'oth', 'ida', 'factor', 'amo', 'World_', 'Once_', 'Hard', ' ... ', 'tol', 'live', 'changed_', 'brain', 'uri', 'seriously', 'release', 'likely_', 'gne', 'explain_', 'ance', 'added_', 'Here_', 'AL', '% ', 'wre', 'spar', 'gree', 'eyes', 'detail', 'Night', 'Mag', 'term', 'tape', 'public_', 'pleas', 'lives', 'ker', 'ile_', 'had', 'dre', 'directing_', 'dialog', 'convincing_', 'chance', 'big', 'beat_', 'appl', 'truth_', 'spa', 'rica', 'monster_', 'market', 'imm', 'have', 'fine', 'clue', 'card', 'blu', 'adult_', 'Who', 'Jim_', 'Bea', '.)', 'value', 'twist_', 'thrown_', 'phe', 'model', 'entertainment', 'Where_', 'LI', 'Ju', 'Black_', 'ura', 'nic', 'han', 'failed_', 'cinematic_', 'bizarre_', 'ben', 'Gu', 'rare_', 'mbo', 'historical_', 'everyone', 'epi', 'ate', 'ada', 'Cli', 'wind', 'sou', 'nder', 'mb', 'held_', 'formula', 'flu', 'effect', 'clever_', 'catch_', 'W_', 'pick_', 'business_', 'attempt', 'Show', 'Paul', 'segment', 'romance_', 'ram', 'nom', 'how', 'ged_', 'flow', 'equally_', 'computer_', 'commercial', 'Val', 'IMDb_', 'trans', 'sent_', 'pet', 'lk', 'ider', 'corn', 'channel', 'Ge', 'Christopher_', 'ways', 'tat', 'subject', 'shooting_', 'return_', 'neither_', 'neighbor', 'lady_', 'impossible_', 'Spa', 'BI', '***', ' -', 'yr', 'violent_', 'syn', 'suffer', 'fur', 'cru', 'Charl', 'secret_', 'rp', 'ros', 'pie', 'ious_', 'hoping_', 'ence_', 'Ye', 'Son', 'trick', 'nia', 'effective_', 'desp', 'costume', 'check', 'board_', 'ami', 'aire', 'ado', 'Whi', 'Two_', 'Rose', 'Green', 'surround', 'promise', 'mad', 'lesson', 'imagination', 'hum', 'excuse_', 'escape_', 'aspect_', 'ak', 'Thu', 'Pal', 'Kr', 'Bur', 'vil', 'travel', 'reso', 'protagonist', 'object', 'nes', 'longer_', 'lia', 'key', 'incredible_', 'hoo', 'fool', 'expression', 'bot', 'bel', 'Ree', 'Oscar', 'Fu', 'safe', 'remains_', 'note_', 'natural_', 'just', 'hm', 'grace', 'credit_', 'constantly_', 'Sam_', 'Ren', 'OK_', 'view', 'unlike_', 'surprise', 'success_', 'ssion', 'song', 'player', 'match_', 'ela', 'din', 'critic', 'accident', '20', 'otherwise_', 'material', 'knowing_', 'ings', 'ffe', 'depth_', 'cula', 'Whe', 'Ph', 'Ai', 'respect_', 'puts_', 'pher', 'kin', 'concept_', 'zed_', 'unfortunate', 'que_', 'predictable', 'order', 'onto_', 'meta', 'ev', 'dress', 'dog_', 'cell', 'Thi', 'Frank_', 'spin', 'rot', 'military_', 'hall', 'cut', 'choice_', 'chick', 'bs', 'Za', 'Many_', 'witch', 'weak', 'swa', 'rti', 'producers_', 'inn', 'gold', 'fault', 'ez', 'cute_', 'cult_', 'WO', 'SH', 'drink', ', (', 'wall', 'theme', 'taste', 'sion', 'iz', 'gun_', 'ek', 'drawn_', 'anyone', 'antic', 'tension_', 'team', 'sweet_', 'ree', 'perform', 'partner', 'horrible', 'contains_', 'Es', 'De_', 'Chris_', 'AT', 'vote', 'tch_', 'singing_', 'shine', 'hasn', 'happen', 'gal', 'demon', 'dar', 'Jer', 'GE', 'ske', 'indeed_', 'guys', 'emotion', 'apart_', 'See', 'Roger', 'Pol', 'trouble_', 'seat', 'planet', 'exciting_', 'err', 'dream_', 'cus', 'arrive', 'HO', '!!!!', 'trip_', 'today_', 'sle', 'setting', 'rr', 'plus_', 'og', 'faci', 'disp', 'crack', 'cen', 'Gun', 'words', 'will', 'prefer', 'pect', 'noi', 'leader', 'dit', 'deal', 'creep', 'Zo', 'Sid', 'East', 'record', 'poo', 'normal_', 'message', 'ffi', 'fer', 'correct', 'colle', 'ator', 'Ros', 'Other_', 'zen', 'usi', 'pil', 'mental_', 'ji', 'immediately_', 'ible_', 'capt', 'bab', 'Chu', 'tar', 'stands_', 'progress', 'making', 'lc', 'fic', 'exp', 'encounter', 'circ', 'change', 'annoying', 'Mur', 'Lor', 'Little_', 'tl', 'rain', 'fail', 'died_', 'Time', 'Blood', 'tell', 'reflect', 'ked_', 'judge', 'ide', 'development_', 'control_', 'clima', 'bed', 'alr', 'Tre', 'trouble', 'thr', 'spot', 'ress', 'red', 'pol', 'hill', 'eb', 'TH', 'Ken', '\\x85 ', 'surprisingly_', 'rep', 'freak', 'dep', 'college_', 'brilliant', 'blin', 'bath', 'People_', 'Nat', 'Charles_', 'walking_', 'ref', 'reco', 'pace_', 'nde', 'mil', 'mainly_', 'literally_', 'fia', 'dull', 'Sn', 'Ever', 'Dam', 'Bre', 'Brad', 'Both_', 'ward', 'trash', 'tough_', 'serve', 'reasons_', 'ngs', 'llen', 'ines', 'honest', 'focus', 'carrie', 'aim', 'Us', 'Prince', 'Nothing_', 'truth', 'supp', 'sma', 'musical', 'inco', 'fight', 'enc', 'bother', 'arch', 'Jon', 'Japan', 'Er', 'Des', '!!!', 'unw', 'unfortunately_', 'til', 'rese', 'marri', 'ior', 'ene', 'ain_', 'Aust', 'ular', 'tru', 'tch', 'tale', 'prop', 'phan', 'orat', 'nit', 'matter', 'host', 'hood', '\\\\&undsc', 'Not', 'Film_', 'Ama', 'yle', 'var', 'standards', 'pers', 'nice', 'meaning_', 'laughs_', 'joke_', 'iss', 'happi', 'era_', 'WH', 'Lil', 'Girl', 'ES', ' />-', 'watche', 'tant', 'qua', 'presented_', 'minor_', 'gro', 'fie', 'door', 'corp', 'catch', 'cally_', 'bert', 'Indian_', 'Gen', 'questions_', 'lacks_', 'forever', 'establish', 'esc', 'cheap', 'Sol', 'while', 'twist', 'society_', 'pass_', 'overa', 'merely_', 'highlight', 'flat_', 'fill', 'color', 'cartoon_', 'Will_', 'NT', 'IT', 'Harry_', 'Fan', 'youth', 'possible', 'orm', 'free', 'eight', 'destroy', 'creati', 'cing_', 'ces_', 'Carr', 'unl', 'suggest_', 'slo', 'owner', 'kh', 'instead', 'influence', 'experiment', 'convey', 'appeal_', 'Ol', 'Night_', '---', 'vy', 'terms_', 'sick_', 'par', 'once', 'law', 'ize_', 'infe', 'Spo', 'House_', '\\x85', 'studio_', 'simple', 'rre', 'guard', 'girlfriend_', 'fear', 'dam', 'concern', 'amusing_', 'adaptation_', 'Ms', 'King', 'water', 'ory_', 'officer', 'litera', 'knock', 'grat', 'falling_', 'ered_', 'cow', 'cond', 'alo', 'Kar', 'Der', 'Cri', 'text', 'skin', 'sequel', 'level', 'impression_', 'ice_', 'force_', 'fake_', 'deri', 'contain', 'band_', 'appa', 'South_', 'HE', 'Conn', 'wise_', 'ur_', 'ual', 'sy_', 'luck', 'lack', 'impressi', 'disaster', 'business', 'being', 'beg', 'Burt', ' <', 'villain_', 'type', 'shoot_', 'shame', 'sb', 'pt_', 'proves_', 'manner', 'lame', 'impressive_', 'ern', 'disappear', 'alone', 'LL', 'Having_', 'Brook', 'Arm', '!\"', 'works', 'state_', 'shock', 'rev', 'mus', 'int', 'ino', 'images_', 'brid', 'berg', 'alis', 'Clo', 'singer', 'shr', 'rock_', 'provides_', 'page', 'instance', 'drug_', 'crime', 'beautifully_', 'acts_', 'UN', 'Tal', 'Bruce_', 'self_', 'reality', 'mans', 'lived_', 'innocent_', 'ically_', 'fall', 'dict', 'Henry_', 'Fox', 'Bac', 'sold', 'says', 'period', 'ome', 'melodrama', 'include_', 'evil', 'Ins', 'stati', 'silent_', 'ria', 'mom', 'met_', 'guns', 'ground', 'gate', 'fell_', 'cle_', 'cari', 'birth', 'Look', 'Hill', '1950', 'water_', 'reminded_', 'express', 'delight', 'als_', 'Wes', 'Mis', 'Louis', 'Grant', 'xe', 'written', 'touch_', 'ters_', 'squa', 'moral', 'ffer', 'aut', 'appearance_', 'Sim', 'Nor', 'Mont', 'IS_', 'Cath', 'take', 'shel', 'protect', 'gut', 'ans', 'Too_', 'Scar', 'Death', 'American', 'AND_', 'throw_', 'suck', 'standard_', 'sil', 'should', 'share_', 'scary', 'loves_', 'indu', 'foot', 'ew_', 'answer', 'Wit', 'Van_', 'Terr', 'Str', 'subtle_', 'stories', 'store_', 'must', 'ments_', 'mbi', 'gs', 'ft_', 'fellow_', 'erat', 'eni', 'crash', 'ches', 'becoming_', 'appeared_', 'TE', 'Fal', '., ', 'visit', 'viewer', 'tag', 'surely_', 'sur', 'stri', 'putting_', 'pull_', 'process', 'pointless_', 'nta', 'mass', 'hur', 'hell', 'gue', 'girls', 'Rev', 'Pan', 'Billy_', 'villain', 'suppose_', 'sick', 'prom', 'narrat', 'mer_', 'followed_', 'decision', 'auto', 'adult', 'Movie_', 'Ban', 'tone', 'thoroughly_', 'sympath', 'sts_', 'sk', 'pot', 'piece', 'offers_', 'nte', 'most', 'helps_', 'det', 'cti', 'brief_', 'block', 'adds_', 'Street', 'Red_', 'Qui', 'Love', 'BL', 'support_', 'ses_', 'rta', 'recognize', 'mission', 'ignore', 'hon', 'broad', 'bid', 'ano', 'Swe', 'Shakespeare', 'Ron', 'Mart', 'Charlie_', 'thanks_', 'tage_', 'serial_', 'revenge_', 'ors', 'office_', 'nst', 'feature', 'drugs', 'disturb', 'anymore', 'Bl', \", '\", 'univers', 'touching_', 'strange', 'improve', 'iff', 'heavy_', 'fare', 'central_', 'buff', 'Inter', 'EA', 'worr', 'turning_', 'tired_', 'than', 'seemingly_', 'motion_', 'ku', 'has', 'goe', 'evi', 'duc', 'dem', 'cinematography', 'aspects_', 'any', 'High', 'Cho', 'tick', 'surviv', 'suicide', 'return', 'remember', 'ppy_', 'noti', 'mess_', 'mes', 'inve', 'grow', 'enge', 'dom', 'Tar', 'Since_', 'Roy', '19', ' ( ', 'track_', 'racis', 'narrative_', 'nal', 'mysterious_', 'moral_', 'imp', 'desert', 'compl', 'along', 'Sw', 'Super', 'HI', 'Dor', 'America_', 'vert', 'superb', 'stu', 'shouldn', 'science_', 'rough', 'ray', 'ova', 'dumb', 'deb', 'court', 'control', 'complex_', 'butt', 'Joe', 'Ir', 'Direct', 'throughout', 'tende', 'stic_', 'somewhere_', 'sel', 'pti', 'picked_', 'parts', 'mob', 'fear_', 'developed_', 'couple', 'cas', 'attitude', 'apo', 'Sun', 'MO', 'L_', 'Ei', 'teen_', 'pull', 'ough', 'hunt', 'favor', 'dos', 'delivers_', 'chill', 'ately', 'Van', 'vat', 'tz', 'trip', 'stuck_', 'rela', 'mood_', 'finish', 'essen', 'ering_', 'disappoint', 'could', 'commit', 'TA', 'Lam', 'Harris', 'whole', 'value_', 'ural', 'sim', 'season', 'redeeming_', 'poli', 'please', 'happened', 'geo', 'force', 'ero', 'core_', 'cand', 'blue', 'bell', 'assi', 'asp', 'adventure_', 'Sin', 'McC', 'whatsoever', 'sky', 'shows', 'pse', 'language_', 'insight', 'ier_', 'finding_', 'everything', 'cker', 'challenge', 'books_', 'Out', 'Ji', 'Glo', 'tune', 'terri', 'prem', 'oe', 'nish', 'movement', 'ities_', 'effort', 'absolute_', 'Brian_', 'Alan_', 'unin', 'unde', 'ude', 'tear', 'oh_', 'ize', 'ilia', 'hint', 'credib', 'craz', 'choice', 'charming_', 'audiences_', 'apart', 'York', 'Marc', 'wonderful', 'willing_', 'wild', 'repeated', 'refer', 'ready_', 'radi', 'punch', 'prison', 'painful_', 'pain', 'paid_', 'pace', 'nni', 'mate_', 'hole', 'future', 'disturbing_', 'cia', 'buck', 'ache', 'Taylor', 'Lind', 'Hol', 'vel', 'tor_', 'terrific_', 'suspense', 'sf', 'research', 'remark', 'problem', 'plu', 'pathetic_', 'negative_', 'lovely_', 'lift', 'hype', 'gl', 'earn', 'ave', 'Their_', 'SS', 'Cass', 'slowly_', 'rented_', 'opportunity_', 'fat', 'every', 'este', 'dub', 'cons', 'bull', 'Sav', 'P_', 'My', 'wondering_', 'unbe', 'twe', 'statu', 'shin', 'rock', 'party_', 'inform', 'heroine', 'hate', 'girlfriend', 'fate', 'ette', 'dies_', 'comparison', 'alb', 'ak_', 'Lis', 'Christian_', 'Act', 'yon', 'storyline', 'soul', 'rece', 'rea', 'product', 'nut', 'lets_', 'funniest_', 'field_', 'city', 'Stephen_', 'GH', 'Ann', 'wee', 'weapon', 'viewing', 'tte', 'sty', 'spi', 'quality', 'price', 'possess', 'ntly', 'dd', 'compa', 'buy', 'agree', 'Hal', 'Comp', 'twists_', 'shak', 'nudity_', 'mati', 'giant_', 'company_', 'baby_', 'admit', 'Finally', 'wn_', 'whe', 'romance', 'presence_', 'myself', 'jokes', 'ident', 'friendship', 'fift', 'explore', 'episodes', 'element_', 'edi', 'eat', 'conve', 'Ira', 'However_', 'DI', 'winning_', 'sexy_', 'rescue', 'physical_', 'pe_', 'oid', 'nobody_', 'nis', 'mad_', 'lin_', 'ket', 'hom', 'generation', 'dance', 'attack', 'appropriate', 'allowed_', 'Ve', 'RS', 'Mr_', 'Kid', 'Instead_', 'Hell', 'Everything_', 'Before_', 'Arthur_', 'waste', 'themes_', 'stunt', 'rap', 'million_', 'hi_', 'games', 'fair_', 'distract', 'cross', 'boat', 'available_', 'abilit', 'Hitler', 'Fl', 'Cas', 'wearing_', 'spirit_', 'rede', 'rb', 'perspective', 'ocr', 'mac', 'kle', 'gang_', 'floor', 'fab', 'Pen', 'ON', 'Kur', 'Jerry_', 'Here', 'Andrew', '??', 'window', 'uss', 'mp_', 'intens', 'expert', 'ei', 'changes_', 'carry_', 'born_', 'bee', 'award', 'Sor', 'Jos', 'Home', 'Cat', '1980', 'zing_', 'victim', 'tight', 'space', 'slu', 'pli', 'neat', 'mistake', 'ky', 'joke', 'includes_', 'hear', 'emb', 'dev', 'damn_', 'confusi', 'church', 'NI', 'Clark', 'theatre', 'sso', 'lock', 'laughed_', 'fran', 'drive_', 'danger', 'alle', 'Which_', 'Western', 'Roman', 'Rit', 'Pie', 'Law', 'France', 'Did_', '14', 'vor', 'usual', 'turn', 'supposedly_', 'sm_', 'satisf', 'realistic', 'pieces_', 'nse', 'near', 'image_', 'flat', 'development', 'design', 'contrast', 'colla', 'board', 'arti', 'anywhere', 'Unfortunately_', 'Rock', 'Ford', 'Doc', 'white', 'small', 'replace', 'prison_', 'owe', 'minat', 'may', 'inspired_', 'helped_', 'expect', 'doll', 'dish', 'chase', 'awa', 'Those_', 'Second', 'OR', 'Nazi', 'Ell', 'watchable', 'via', 'test', 'stick', 'step_', 'speech', 'relationship', 'pass', 'ote', 'nel', 'mild', 'gue_', 'embarrass', 'describe_', 'bound', 'bother_', 'aging', 'Julie', '70s', 'via_', 'street_', 'squ', 'scream', 'pos', 'overs', 'mix_', 'martial_', 'magic_', 'jud', 'gener', 'eh', 'concept', 'alien', 'FO', 'which', 'values_', 'success', 'soldiers_', 'pla', 'lous', 'lose_', 'io', 'ike', 'fish', 'eth', 'ddy', 'crowd', 'creative_', 'conc', 'beh', 'bbi', 'Matth', 'Europe', '1970', 'ulat', 'track', 'target', 'swea', 'stal', 'refuse', 'phon', 'pho', 'hang', 'gea', 'doubt', 'compr', 'cloth', 'cliché', 'bland', 'behavior', 'aci', 'Simp', 'Leon', 'England', 'Edi', 'Cons', ')<', ' .', 'wy', 'worker', 'volu', 'vehicle', 'tour', 'random_', 'phone_', 'ong', 'moved_', 'grave', 'folk', 'filming_', 'feelings_', 'build_', 'basi', 'Tor', 'TR', 'Sk', 'New', 'Miss_', 'Kl', 'Kat', 'Boll', 'zil', 'ust', 'robot', 'result', 'reac', 'ped', 'pea', 'ow_', 'mmi', 'laughs', 'issues_', 'intended_', 'impressed_', 'favorite', 'dw', 'documentary', 'doctor_', 'debut', 'account', 'North', 'Im', 'GO', 'weird', 'transform', 'train', 'swi', 'sum', 'soci', 'same', 'reh', 'ld_', 'ffic', 'conversation', 'comedic_', 'artistic_', 'adi', 'accept', 'Stone', 'Jew', 'CR', 'threaten', 'stea', 'scra', 'sake', 'potential', 'listen', 'het', 'cted_', 'cod', 'chase_', 'berg_', 'appear', 'Ton', 'Queen', 'Mark_', 'Hall', 'FI', 'wer', 'thes', 'sons', 'provide_', 'nger', 'ney', 'mot', 'mask', 'flesh', 'exe', 'dozen', 'disgu', 'conclusion', 'accent', 'Victoria', 'SP', 'Jr', 'Char', 'Albert', 'try', 'tal_', 'round_', 'mix', 'ison', 'hundred', 'holds_', 'gger', 'approach_', 'Space', 'Okay', 'MI', 'Love_', 'Elvi', 'Doo', 'tragic_', 'sweet', 'stud', 'sible', 'remain', 'pur', 'nts_', 'ken', 'got', 'fam', 'edge_', 'Hea', 'Film', 'Cast', 'teenage_', 'technical_', 'skip', 'rend', 'our', 'illus', 'ham', 'favourite_', 'ensi', 'consist', 'cold_', 'cent', 'cate', 'MAN', 'F_', 'Die', 'Cub', 'Chinese_', 'yourself', 'ugh', 'stretch', 'society', 'rth', 'root', 'reminds_', 'reg', 'rd_', 'put', 'purpose', 'ition_', 'humanity', 'gotten_', 'fest', 'feel', 'fascinat', 'failure', 'culture_', 'cont', 'allow_', 'pursu', 'preci', 'if', 'belong', 'VE', 'Sar', 'O_', 'Nic', 'Dead', 'AC', ' ****', 'western_', 'uct', 'thro', 'tes', 'struggle_', 'straight', 'stic', 'similar', 'repe', 'pid', 'nes_', 'mou', 'irre', 'hic', 'explained', 'deeply_', 'cs_', 'confront', 'clichés', 'attack_', 'asks_', 'Yet_', 'Was_', 'Tro', 'Stre', 'Rei', 'Kelly_', 'Julia', 'Bas', '? <', 'ties', 'technique', 'stunning_', 'slight', 'skill', 'sat_', 'outstanding_', 'lies_', 'journey_', 'hap', 'expla', 'definit', 'critics_', 'continue_', 'compelling_', 'charge', 'Thing', 'PE', 'Marie', 'Lynch', 'Jason_', 'Hen', 'Av', '.... ', '\\x97', 'wanting_', 'wanna', 'transp', 'thats_', 'smok', 'respons', 'professional_', 'print', 'physic', 'names_', 'inge', 'infa', 'grip', 'green', 'ggi', 'buster', 'bum', 'belief', 'accept_', 'abuse', 'Rain', 'Pos', 'Lee', 'Hoo', 'All', 'threa', 'soundtrack', 'realized_', 'ration', 'purpose_', 'notice_', 'member_', 'lovers', 'log', 'kni', 'inse', 'inde', 'impl', 'government_', 'door_', 'community', 'also', 'Zombie', 'WI', 'Sur', 'Stewart_', 'Roo', 'NA', 'Comm', 'Anna', 'wonderfully_', 'vac', 'tit', 'thus_', 'shadow', 'rg', 'resol', 'religious_', 'problems', 'nonsense', 'naked_', 'marvel', 'fantastic', 'em_', 'earth_', 'demand', 'cost', 'bes', 'band', 'background', 'Mas', 'Bon', 'African', ':<', 'thousand', 'realism', 'race_', 'ption', 'pred', 'neg', 'met', 'little', 'kn', 'flying_', 'ement', 'editing', 'abandon', 'Take', 'On', 'Mich', 'Gin', 'Fer', 'wide', 'victim_', 'spell', 'search_', 'rush', 'road_', 'rank', 'pping_', 'mpl', 'kil', 'incomp', 'humour_', 'group', 'ghost', 'ens', 'electr', 'edg', 'dru', 'culture', 'cars', 'Wil', 'UR', 'Haw', 'Give', 'Fat', 'Dou', 'Ant', 'AD', 'vs', 'tia', 'rei', 'regret', 'necessar', 'master_', 'mani', 'honestly_', 'hey', 'hadn', 'gant', 'fresh_', 'exce', 'document', 'direct_', 'dated_', 'afraid_', 'OU', 'Mid', 'Len', 'Good', 'Beat', 'yer', 'walk', 'ture_', 'train_', 'theor', 'stink', 'spit', 'rarely_', 'proper', 'intelligen', 'hed_', 'hair_', 'forgot', 'fascinating_', 'ere', 'deliver_', 'believable', 'awesome_', 'attend', 'actresses_', 'Up', 'Par', 'Bad_', 'zombie', 'ys_', 'wards', 'trash_', 'strip', 'spectacular', 'six_', 'silly', 'shed_', 'praise', 'loud_', 'inspir', 'insi', 'god', 'four', 'devi', 'Sir', 'Plan', 'PL', 'Everyone_', 'Dol', 'thinking', 'store', 'spo', 'rou', 'pou', 'opposite', 'dud', 'difference_', 'deli', 'compare_', 'cable', 'VER', 'Tim_', 'Ob', 'Jane', 'Jam', 'Don_', 'CI', 'yo', 'want', 'villains', 'toward_', 'taste_', 'support', 'stone', 'sted_', 'spect', 'satire', 'row_', 'rag', 'observ', 'nel_', 'motiv', 'moro', 'lust', 'lect', 'ively_', 'gli', 'gie', 'fet', 'eld', 'div', 'creating_', 'brain_', 'bird', 'attention', 'ates_', 'ald', 'Sher', 'Russ', 'Rea', 'Joan_', 'Gab', 'Coo', 'Bond', '40', 'trade', 'sive_', 'routine', 'plane_', 'photograph', 'ound', 'om_', 'nk_', 'mountain', 'mate', 'listen_', 'isa', 'imagina', 'gia', 'embarrassing', 'convince', 'building_', 'avoid', 'Wow', 'SA', 'Al_', 'vy_', 'unsu', 'tty_', 'situations_', 'sensi', 'results', 'recogni', 'quick', 'plan_', 'mod', 'masterpiece_', 'limit', 'lar', 'gorgeous_', 'fil', 'ensu', 'edly_', 'cor', 'context', 'bul', 'bottom_', 'began_', 'animation', 'anc', 'acc', 'Ty', 'Sc', 'London_', 'Lewis', '.\"<', 'weight', 'rubbish', 'rab', 'project_', 'powers', 'personalit', 'offer_', 'noir_', 'killed', 'justif', 'jun', 'information_', 'gem', 'ative_', 'PO', 'Jeff_', 'Gui', 'voca', 'tab', 'spot_', 'remind', 'proceed', 'kick_', 'ious', 'grab', 'enem', 'educat', 'claim', 'cks', 'charisma', 'bal', 'Scott', 'Over', 'Mus', 'Laure', 'Kan', 'Hunt', 'Dead_', 'Acti', '90_', '50_', ' ! ! ! ! ! ! ! ! ! !', 'ws_', 'vul', 'village', 'speed', 'skills', 'public', 'outl', 'naive', 'mos', 'latter_', 'ki_', 'iat', 'honest_', 'ga_', 'emotions_', 'detective_', 'citi', 'bits_', 'answer_', 'accomplish', 'Washington', 'Sm', 'Dal', 'CE', 'Bett', 'Af', '40_', 'sell', 'pret', 'pper', 'opera', 'notabl', 'involved', 'important', 'humorous', 'finale', 'dise', 'date_', 'contribut', 'complain', 'comedies_', 'battle', 'balance', 'Go_', 'Fla', 'Alon', '); ', 'wis', 'ups', 'spoke', 'pulled_', 'points', 'mediocre_', 'ker_', 'introduced_', 'independent_', 'hil', 'fits_', 'eating_', 'confused_', 'concerned', 'cing', 'ca_', 'bran', 'borat', 'bing_', 'ay_', 'abr', 'Russian_', 'Kevin_', 'H_', 'Fred_', 'Exce', 'English', 'Danny_', 'Dani', 'Coll', 'Alt', '100_', 'used', 'translat', 'shape', 'odi', 'manage_', 'loy', 'lik', 'ibi', 'eat_', 'behav', 'apparent_', 'admi', 'acr', 'ach', 'Young_', 'Run', 'Martin_', 'Mak', 'Hart', 'Asi', '25', '& ', 'trag', 'terror', 'tea', 'shallow', 'rob', 'rape', 'pond', 'ole', 'neck', 'nature', 'loving_', 'jerk', 'hours', 'hidden_', 'gar_', 'field', 'fel', 'existence', 'erotic', 'constant_', 'cau', 'bar_', 'VI', 'Univers', 'Sen', 'CK', '100', 'wealth', 'wave', 'understanding_', 'sole', 'ral', 'none', 'nasty_', 'mari', 'likable_', 'ith', 'intense_', 'hou', 'gh_', 'ely', 'dic', 'dea', 'clip', 'bow', 'UL', 'Nu', 'Moon', 'Ital', 'Ed_', 'Cle', '.......', 'yeah', 'tree', 'successful', 'ril', 'ract', 'philosoph', 'parents', 'marriage_', 'lte', 'ject', 'ite_', 'hun', 'fantas', 'fame', 'extra_', 'dreadful', 'details_', 'dad_', 'capture_', 'annoy', 'Other', '?!', 'tions', 'stalk', 'speak', 'revolution', 'redu', 'pretend', 'politic', 'places_', 'parody', 'park', 'onic', 'nowhere_', 'mono', 'mile', 'manipulat', 'loses_', 'lli', 'into', 'hid', 'ghost_', 'gha', 'engage', 'assum', 'ador', 'admire', 'X_', 'See_', 'Full', 'Eye', 'zy', 'ware', 'ven_', 'uncle', 'treated_', 'television', 'surreal', 'student_', 'rival', 'ride_', 'recall', 'nudity', 'locations', 'ility', 'hamm', 'gags', 'fill_', 'dealing_', 'co_', 'climax_', 'bon', 'atmosphere', 'aged_', 'Rock_', 'Kim', 'Had', 'Brid', 'Anton', 'zombies_', 'unfunny', 'techn', 'source', 'section', 'pris', 'priest', 'police', 'olo', 'nine', 'maker', 'limited_', 'ik', 'genius_', 'enjoyable', 'distan', 'desperate_', 'believe', 'asked_', 'appearance', 'Ring', 'Pete', 'Master', 'Kin', 'Harr', 'Earth', 'Dog', 'Brown', 'Bren', 'Add', 'web', 'tee', 'sucks', 'structure', 'regi', 'porn_', 'osi', 'llian', 'lett', 'length_', 'ior_', 'hal', 'faith', 'enta', 'deserve_', 'cartoon', 'bs_', 'ahead_', 'Got', 'Eu', 'Americans_', 'Alex', 'speaking_', 'smil', 'photographe', 'ope', 'mpe', 'minim', 'million', 'mental', 'magnificent', 'lur', 'lov', 'keeping_', 'iting', 'homo', 'haunt', 'fiction_', 'fee', 'exploit', 'entertain', 'dding', 'attracti', 'advice', 'Park', 'Fur', 'Cage', 'suc', 'songs', 'smart_', 'shock_', 'rif', 'repl', 'ranc', 'ran', 'photography_', 'patient', 'ladies', 'hated_', 'growing_', 'cheer', 'attractive_', 'ass_', 'approach', 'ants_', 'Mrs', 'Hay', 'Hank', 'Eli', 'EVER', 'Batman_', 'week', 'sword', 'rac', 'promot', 'portray', 'pictures_', 'lt_', 'ito', 'interna', 'forgive', 'device', 'corrupt', 'choreograph', 'chop', 'blame_', 'atch', 'VE_', 'KE', 'Johnny_', 'vity', 'ville', 'vas', 'uit', 'tional_', 'quote', 'quick_', 'producer_', 'personally_', 'parti', 'oa', 'nity', 'loo', 'ives', 'increas', 'ical', 'heads_', 'graphic', 'going', 'featuring_', 'defin', 'cute', 'criminal', 'cheat', 'cash', 'cann', 'bol', 'bec', 'Welles', 'SPOILERS', 'Power', 'Kell', 'Georg', 'Gene_', 'Blai', 'Again', '11', 'yell', 'vious', 'unusual_', 'tradition', 'summar', 'stunn', 'revealed', 'remo', 'psychi', 'provi', 'prepare', 'offer', 'insane', 'happens', 'efforts', 'delic', 'current_', 'construct', 'bil', 'aries', 'animals_', 'advance', 'Kong', 'Jan', 'Howard', 'Daw', 'Cru', ' !', 'terribly_', 'teache', 'tas', 'sudden', 'sleaz', 'sharp', 'ress_', 'rape_', 'ppi', 'numbers_', 'mouth', 'lower', 'ime', 'ifie', 'ideal', 'exception_', 'ema', 'charm_', 'breaking_', 'addition_', 'Walke', 'Lat', 'Jean_', 'Eddie_', 'City_', '.\"', 'warning', 'versions', 'tack', 'reli', 'ration_', 'prove_', 'plo', 'pile', 'performer', 'monk', 'intellectual', 'handle', 'ets', 'essor', 'ature', 'atri', 'ans_', 'Int', 'Fel', 'European_', 'Cus', 'As', 'wr', 'worst', 'witty', 'wild_', 'wedding', 'students_', 'sadly_', 'princip', 'paint', 'mmy', 'mixed_', 'kinda_', 'frequent', 'discover_', 'dal', 'command', 'colour', 'bou', 'bored', 'Wild', 'Ul', 'Really', 'Mitch', 'Cinema', 'Andy_', '16', 'visuals', 'varie', 'ut_', 'unfold', 'suspect', 'semi', 'responsible_', 'religion', 'rapi', 'py_', 'otic', 'numerous_', 'news', 'nces', 'kl', 'junk', 'joy', 'insult', 'festival', 'drop_', 'costumes_', 'been', 'bag', 'aware_', 'aver', 'Mir', 'Last_', 'Hon', 'Frie', 'Cent', 'wishe', 'vie', 'toy', 'repeat', 'pter', 'oppo', 'open', 'noticed_', 'murders_', 'ka_', 'harm', 'finish_', 'extreme_', 'eno', 'dying_', 'doo', 'ddle', 'clear', 'cat_', 'bru', 'addict', 'Smith', 'Rod', 'Rem', 'zzle', 'tory', 'starting_', 'specific', 'screaming', 'scenery_', 'psychological_', 'occur', 'obli', 'mn', 'lica', 'laughter', 'inso', 'grad', 'goof', 'gas', 'element', 'dom_', 'dism', 'deals_', 'ctor', 'camp_', 'audi', 'ator_', 'ack', 'Smith_', 'Sh', 'Kenne', 'Holl', 'Dean', 'xious', 'uncom', 'situation', 'shots', 'seem', 'rin', 'pain_', 'originally_', 'number', 'nightmare', 'mystery', 'ml', 'kiss', 'imag', 'iful', 'grew_', 'grade_', 'gge', 'event', 'eate', 'dramati', 'dad', 'condition', 'conce', 'comfort', 'chair', 'aur', 'YOU', 'Red', 'REAL', 'Norma', 'Kir', 'wash', 'upt', 'titi', 'returns_', 'retr', 'restr', 'require', 'relief', 'realise', 'rch', 'rang', 'ple_', 'lus', 'lip', 'intrigue', 'incident', 'iler', 'ha_', 'ground_', 'fores', 'exh', 'dancer', 'anger', 'Wr', 'They', 'Sinatra', 'SI', 'Op', 'Long', 'GI', 'Dem', 'yd', 'week_', 'treatment', 'treat', 'stan', 'slic', 'separate', 'screenplay', 'remarkable_', 'pped_', 'persona', 'mble', 'invi', 'innocen', 'hack', 'gru', 'gma', 'glass', 'forgotten_', 'fem', 'confi', 'clever', 'bone', 'amateur', 'Richard', 'Ray_', 'Please_', 'Kris', 'IM', 'Gordon', 'ED', 'Black', 'wen', 'very', 'ured', 'theater', 'stab', 'redi', 'perce', 'peace', 'passe', 'ops', 'oon', 'morning', 'llow', 'legend', 'irritating', 'hopes_', 'gross', 'genuinely_', 'ech', 'crus', 'bitter', 'acti', 'accura', 'Yu', 'Rome', 'Parker', 'Dia', 'studio', 'still', 'stereotypes', 'serv', 'sequences', 'sequence', 'pres', 'portray_', 'poet', 'opti', 'only', 'ins_', 'impact_', 'emotion_', 'ek_', 'earth', 'dou', 'dislike', 'Sti', 'Reg', 'Philip', 'Bil', 'Att', 'Ash', 'Adam_', 'viol', 'v_', 'uma', 'ultimate_', 'ught', 'trailer_', 'superior_', 'sucked', 'sno', 'service', 'ride', 'por', 'plan', 'mum', 'mme', 'merc', 'lonel', 'guide', 'fici', 'facts', 'evidence', 'doctor', 'discover', 'depend', 'degree', 'cruel', 'counter', 'color_', 'cess', 'cause', 'bro', 'ambitio', 'amaze', 'alternat', 'Wom', 'White_', 'John', 'Bud', 'wound', 'wander', 'typi', 'technology', 'swe', 'standing_', 'reuni', 'organi', 'ngly_', 'minu', 'leas', 'gift', 'executed', 'environment', 'diss', 'demonstrat', 'compani', 'allows_', 'Wayne', 'Kno', 'Instead', 'DA', 'Cart', 'Anthony_', 'unable_', 'uf', 'twin', 'tely', 'sympathetic', 'spoof', 'sis', 'saying', 'rh', 'repr', 'rave', 'promising', 'nch_', 'moo', 'ming', 'liz', 'lighting_', 'lesbian', 'large', 'izing_', 'impos', 'dor', 'disco', 'corny', 'arts_', 'Wars', 'Trac', 'Seve', 'Poli', 'PA', 'Moore', 'LL_', 'Jimmy_', 'Gary_', '?\"', 'zero', 'underw', 'tou', 'spen', 'sheer_', 'scared_', 'rever', 'relationships_', 'proved_', 'predict', 'pia', 'obsc', 'lum', 'learn', 'herself', 'gras', 'finished_', 'continues_', 'brave', 'aris', 'api', 'THIS_', 'Mille', 'Leg', 'First', 'Dis', 'Allen_', 'traditional_', 'statement', 'spir', 'soon', 'rence', 'ran_', 'pros', 'opi', 'mistake_', 'lawyer', 'discovers_', 'deepe', 'ction_', 'cares', 'brutal_', 'brutal', 'breaks_', 'antly', 'accent_', 'Killer', 'Can_', 'Broadway', 'unintentional', 'unbelievable_', 'tte_', 'suspect_', 'strike', 'sens', 'screw', 'rtu', 'pant', 'opens_', 'obsessi', 'mates', 'los', 'logic', 'kit', 'joy_', 'inte', 'iness_', 'han_', 'exact', 'entertained', 'ego', 'dreams_', 'convention', 'collecti', 'chest', 'bling_', 'authentic', 'Then', 'Much_', 'Mot', 'Bette', 'viewers', 'vampire_', 'teach', 'stylis', 'someone', 'sne', 'saved_', 'rule', 'regular_', 'practic', 'ppe', 'pion', 'notice', 'native', 'monsters', 'lo_', 'learned_', 'incon', 'hour', 'hood_', 'feeling', 'embe', 'driving_', 'convincing', 'cav', 'ber_', 'angle', 'absurd', 'Trek', 'Sat', 'Paris_', 'Mol', 'Max', 'Kh', 'Emma', 'Edward', 'Anyone_', '?? ', '17', ' \" ', 'wrap', 'unrealistic', 'tam', 'subtitle', 'spoilers', 'since', 'sexual', 'render', 'remake', 'rely', 'pop_', 'oge', 'oft', 'nett', 'monst', 'law_', 'ional', 'inclu', 'ich', 'ians_', 'hotel_', 'graphic_', 'gonna_', 'gent', 'flashbacks', 'families', 'erin', 'dropp', 'dir', 'bond', 'affair_', 'Scre', 'Dun', 'wide_', 'ttl', 'topic', 'symboli', 'switch', 'solve', 'send', 'rud', 'rem', 'reasons', 'reasonabl', 'pee', 'nar', 'location_', 'ining_', 'gam', 'disappointing_', 'desire_', 'criminal_', 'considera', 'century_', 'celebrat', 'brow', 'area', 'Thin', 'Rec', \"' (\", 'ward_', 'vision_', 'treme', 'surprising_', 'super_', 'risk', 'receive', 'qual', 'pic', 'mee', 'levels', 'kins', 'jack', 'ire_', 'introduc', 'hits_', 'happening_', 'handsome', 'gradua', 'giv', 'garbage', 'forces_', 'finest_', 'easi', 'depressing', 'credits', 'asto', 'Sadly', 'Ple', 'Inc', 'Dick_', 'Alexand', 'wooden_', 'wood_', 'stro', 'steal_', 'soul_', 'reference', 'race', 'quis', 'pir', 'perv', 'obvious', 'majority_', 'lean', 'kes_', 'insti', 'identity', 'everybody_', 'double_', 'dies', 'credit', 'const', 'confe', 'compar', 'centur', 'bloody_', 'Under', 'Twi', 'Sean_', 'Lio', 'Halloween', 'Gal', 'Clu', 'Came', 'Barbara_', '?)', '11_', 'ws', 'ulous', 'subtle', 'substance', 'string', 'shocking_', 'scientist_', 'rian', 'nou', 'multi', 'lf', 'inal', 'harsh', 'handed', 'fir', 'expectations_', 'excited', 'exceptional', 'eva', 'complete', 'comic', 'childhood_', 'ched_', 'adults_', 'Timo', 'Soo', 'Mos', 'Kath', 'Karl', 'Cinderella', 'Christian', 'Age', 'Adam', '!). ', 'zar', 'zan', 'trap', 'trai', 'thin_', 'site_', 'site', 'rich', 'resi', 'reach_', 'quirk', 'patr', 'ony', 'nerv', 'matche', 'inept', 'imagine', 'horri', 'front', 'ford_', 'epic_', 'dat', 'cynic', 'ckin', 'cie', 'caused_', 'brothers_', 'belo', 'appealing', 'West_', 'UK', 'TC', 'Suc', 'Rand', 'Grad', 'Domin', 'Disney', '12_', 'warr', 'vision', 'spoo', 'seeing', 'scenario', 'scale', 'rad', 'ola', 'next', 'necessary_', 'indicat', 'exploitation', 'ened_', 'directing', 'depict', 'curio', 'ciati', 'bullet', 'appre', 'amateurish', 'Yo', 'Watching_', 'Sky', 'Shar', 'Part_', 'Nichol', 'Mars', 'Are_', 'wel', 'visit_', 'unne', 'underrated', 'tedious', 'seconds_', 'rig', 'report', 'reme', 'rar', 'mond_', 'media_', 'lying_', 'las', 'language', 'ised_', 'instant', 'inspiration', 'creates_', 'conflict', 'compose', 'chan', 'cab', 'ava', 'always', 'Water', 'Steven_', 'Pas', 'Nick_', 'Let_', 'Down', 'yth', 'victims_', 'theaters', 'seasons', 'sai', 'rising', 'recr', 'plann', 'pent', 'painfully_', 'ot_', 'occu', 'nob', 'moti', 'lem', 'lati', 'gua', 'fights_', 'event_', 'elev', 'discovered_', 'cs', 'cliché_', 'cance', 'bik', 'bigger_', 'backs', 'atic', 'Shan', 'Sab', 'Poi', 'Hitchcock', 'GR', 'Francis', 'Det', 'Care', 'Anderson', 'veteran', 'ution_', 'theless', 'sports', 'slave', 'ses', 'revi', 'refreshing', 'quar', 'provok', 'premise', 'paper', 'nty', 'norm', 'mood', 'menac', 'loud', 'loose', 'letter', 'investigati', 'introduce', 'holes_', 'gan_', 'fund', 'ents_', 'drunk', 'disgusting', 'dio', 'confusing_', 'cky', 'baby', 'THE', 'Nancy', 'Kate_', 'Gia', 'Carol', 'Cand', \"'.\", 'western', 'unf', 'struc', 'strong', 'search', 'sav', 'ries_', 'resemble', 'rental', 'raci', 'producer', 'nic_', 'news_', 'memor', 'many', 'magical', 'format', 'equal', 'decl', 'curs', 'ction', 'convict', 'contrived', 'capable_', 'bringing_', 'boyfriend_', 'bli', 'anybody_', 'animal_', 'advertis', 'Music', 'Jun', 'Jones', 'Greg', 'Fra', 'Donald_', 'Dark', '1930', 'é_', 'yc', 'urne', 'tire', 'step', 'scr', 'reporter', 'position', 'okay', 'nted_', 'misse', 'logical', 'ient', 'identif', 'feet', 'fail_', 'creat', 'content_', 'contemp', 'concei', 'border', 'ask', 'actual', 'Way', 'Plus', 'Mill', 'Foo', 'Dy', 'Bec', ' ,', 'utter_', 'urban', 'struggle', 'sign_', 'sher', 'seduc', 'scientist', 'saw', 'released', 'received_', 'lity_', 'jump_', 'island_', 'ignor', 'ick', 'horrifi', 'hange', 'handled', 'endea', 'dil', 'ative', 'angry_', 'ages_', 'accus', 'Writ', 'Without_', 'Wall', 'Thank', 'Sla', 'Qua', 'Page', 'ND', 'Lost', 'Fish', 'Eric_', 'Does', 'Clau', 'Cel', 'Camp', 'Australian', 'Arn', 'Ann_', 'Ala', 'Actually', \".' \", \",' \", 'wall_', 'thoughts', 'somebody_', 'round', 'proud', 'oy', 'overly_', 'opera_', 'offensive', 'myth', 'murderer', 'mpt', 'ivi', 'ir_', 'iga', 'iar', 'holi', 'hearted_', 'gath', 'fictional', 'expectation', 'etta', 'enco', 'ence', 'deserved_', 'depiction', 'dece', 'comedian', 'bles', 'aside_', 'ambi', 'ake', 'Wonder', 'Why', 'Through', 'Overall_', 'Off', 'OI', 'More_', 'Jennifer_', 'Gill', 'Germany', 'Douglas_', 'Cy', 'CGI_', '\").', 'walks_', 'ury', 'three', 'thank_', 'surp', 'soph', 'sed', 'satisfying', 'rebel', 'pure', 'practically_', 'minds', 'manage', 'lp', 'learns_', 'isl', 'involves_', 'impro', 'impa', 'icon', 'hyp', 'fortune', 'erm', 'cuts_', 'copi', 'conclusion_', 'ced_', 'captured_', 'bble', 'arro', 'Wei', 'Sis', 'Pin', 'Marg', 'Life', 'Laur', 'Later', 'Hop', 'Eva', 'Blue', 'Barry', 'Baby', 'whilst_', 'unfa', 'twi', 'test_', 'ters', 'stric', 'streets', 'stom', 'spoil', 'relative', 'relate_', 'recommend', 'ology', 'middle', 'laughable', 'jea', 'genuine_', 'gat', 'frustrati', 'forth', 'excitement', 'costs', 'cord', 'compo', 'bright_', 'bank', 'aka', 'WE', 'Ten', 'THAT', 'Pur', 'Pitt', 'Mike_', 'Hum', 'Being_', 'veri', 'turi', 'tun', 'tel', 'task', 'sting', 'six', 'sentimental', 'quit', 'pleasure_', 'pity', 'personality_', 'motivation', 'moder', 'miserabl', 'mirror', 'manner_', 'logi', 'ein', 'eful', 'dubbed', 'discussi', 'ders', 'defeat', 'dangerous_', 'cry_', 'clos', 'cial_', 'chor', 'Wat', 'Wan', 'Spanish_', 'Have', 'Guy', 'Game', '. . ', 'winner', 'welcome', 'unexp', 'ture', 'tall', 'tal', 'stoo', 'smo', 'serious', 'rc', 'phi', 'outrage', 'oh', 'national_', 'mber_', 'mba', 'loser', 'lee', 'largely_', 'involve', 'ico', 'garbage_', 'found', 'even', 'distinct', 'design_', 'cure', 'consu', 'circumstances', 'calls_', 'blown_', 'attract', 'anime', 'Zi', 'Vietnam', 'Ryan', 'ON_', 'NY', 'Lady_', 'La_', 'Flor', 'Bern', 'AI', ' )', 'unk', 'unh', 'ugly_', 'tine', 'spre', 'simpli', 'significant', 'sequels', 'remembered_', 'reache', 'plat', 'obsessed_', 'ncy_', 'mysteri', 'mous', 'mbs', 'lover_', 'lights', 'lad', 'industr', 'ible', 'grown_', 'general', 'fru', 'explosion', 'exception', 'ese', 'endur', 'domina', 'dera', 'cies', 'built_', 'barr', 'Tod', 'Ran', 'Maria', 'Grand', 'Dee', 'Aw', ' />**', 'xo', 'voices', 'visually', 'ui', 'twice_', 'tend_', 'spor', 'solut', 'slap', 'scien', 'robbe', 'redibl', 'prot', 'prevent', 'ood', 'kee', 'issue_', 'ironic', 'iron', 'investigat', 'intr', 'hl', 'gus', 'food_', 'enl', 'dl', 'described_', 'complaint', 'careful', 'apartment_', 'alcohol', 'aid', 'acy', 'Year', 'Vis', 'Vir', 'Tow', 'Fly', 'Dream', 'Award', '*****', 'vague', 'strat', 'reviewers_', 'offend', 'locat', 'iu', 'ital', 'iev', 'hospital_', 'fou', 'financ', 'filmmaker_', 'farm', 'evening', 'essentially_', 'energy_', 'ef_', 'complex', 'competi', 'ching', 'bal_', 'ax', 'ances', 'acted', 'ace_', 'Story', 'LD', 'Inde', 'Hope', 'Duk', 'Dian', 'Bob', 'Back', 'Any_', 'About_', ' ...', 'yard', 'whenever_', 'wake', 'ures_', 'unse', 'trust_', 'treat_', 'teenager', 'stock_', 'rri', 'rise_', 'rant', 'pupp', 'pte', 'pes', 'overd', 'operati', 'occasional', 'nicely_', 'nical', 'liners', 'impo', 'holding_', 'engaging_', 'diver', 'distribut', 'dim', 'delightful_', 'crappy_', 'cook', 'connection_', 'cohe', 'bore', 'Vincen', 'Susan', 'Rep', 'Powell', 'Oliver', 'Neil', 'Murphy', 'Mic', 'Indi', 'Ele', 'Bru', 'Beaut', '. *', ' />*', 'zation', 'urge', 'urag', 'teenagers', 'seven_', 'river', 'prep', 'nail', 'mble_', 'matters', 'loose_', 'iva', 'issue', 'intriguing_', 'ili', 'god_', 'glimpse', 'ently', 'els_', 'een_', 'develop_', 'desire', 'cops_', 'contra', 'buil', 'broke', 'ater', 'asleep', 'adventur', 'Williams_', 'Wend', 'None_', 'Mod', 'House', 'Horror_', 'Anim', '192', 'ughter', 'trial', 'soap_', 'severe', 'road', 'poster', 'portraying_', 'phr', 'pathetic', 'overlook', 'moving', 'month', 'lau', 'lacking_', 'knowledge_', 'kidnapp', 'interpretation', 'industry_', 'hurt', 'heavi', 'genius', 'false', 'existent', 'execution', 'drop', 'difference', 'determine', 'detail_', 'dent', 'cutting', 'combin', 'comb', 'cket', 'chron', 'capital', 'bodies', 'bic', 'believes_', 'area_', 'angles', 'Ted', 'Sop', 'End', 'Dre', 'Dick', 'Ak', 'Africa', ' ? ', 'vol', 'system', 'steps', 'situations', 'sexuality', 'sets', 'ripp', 'revel', 'rel', 'realiz', 'private', 'paper_', 'notch', 'nge_', 'mistr', 'merit', 'mbl', 'match', 'losing_', 'lme', 'interacti', 'indeed', 'ifica', 'henc', 'heaven', 'fro', 'fon', 'femin', 'faces_', 'enh', 'driven_', 'dressed_', 'dne', 'decen', 'ctic', 'coming', 'club_', 'castle', 'captures_', 'building', 'atic_', 'athe', 'assassin', 'army_', 'alien_', 'abso', 'Tho', 'Scr', 'Prob', 'Para', 'Gor', 'Eg', 'Com', 'City', 'At', 'Apparently', ' / ', 'ule', 'ue_', 'tograph', 'thirt', 'thank', 'suit_', 'suffering_', 'sight_', 'sey', 'screenwriter', 'rell', 'ppet', 'passed_', 'pacing_', 'normally_', 'mill', 'lyn', 'ition', 'gers', 'football', 'faithful', 'expose', 'expos', 'emerge', 'ell_', 'depicted', 'crude', 'criticism', 'combination_', 'claim_', 'carr', 'bt', 'brilliantly_', 'boss', 'analy', 'ame', 'Ray', 'Pic', 'Lord_', 'Kill', 'Fea', 'Evil', 'Bos', 'BS', 'AB', '\" - ', ' :', 'tta', 'trailer', 'soli', 'rum', 'revolve', 'ressi', 'quiet_', 'portrays_', 'populat', 'plant', 'oin', 'occasionally_', 'nost', 'nau', 'mun', 'lb', 'ipat', 'hysteri', 'grow_', 'gag', 'fus', 'foot_', 'finger', 'figur', 'esp', 'equi', 'ener', 'dec', 'chain', 'broken_', 'agent', 'actions_', 'aa', 'Russell', 'Indian', 'Heav', 'Daniel_', 'Ast', ' /> ', 'zard', 'unlikely', 'ump', 'tele', 'teacher_', 'subplot', 'rub', 'rte', 'rly_', 'radio_', 'quir', 'pair_', 'ordinary_', 'oppos', 'nsi', 'mouth_', 'maintain', 'lve', 'loc', 'inventi', 'inexp', 'imitat', 'generate', 'gal_', 'frightening', 'frig', 'foreign_', 'filmmaker', 'excess', 'elle', 'creator', 'count_', 'controvers', 'cliche', 'casti', 'bet_', 'aking_', 'acqu', 'Three', 'Texas', 'Tarzan_', 'Earth_', 'Dan_', 'Besides', 'yw', 'woods_', 'wan', 'vest', 'uous', 'unit', 'therefore_', 'tears_', 'surface', 'steals_', 'sni', 'shut', 'roman', 'roll_', 'rele', 'reaction', 'qualities', 'proper_', 'profession', 'photo', 'months_', 'mem', 'makeup', 'longe', 'lam', 'ix', 'insist', 'inher', 'fying_', 'forgettable', 'faced', 'expens', 'enthusias', 'describ', 'cry', 'commentary_', 'collection_', 'civili', 'category', 'cam', 'believed', 'ancient_', 'Walter_', 'Sum', 'Sometimes', 'Sel', 'Lou', 'Kn', 'Joseph_', 'Gro', 'Fon', 'Columbo', 'system_', 'student', 'shocked', 'sell_', 'ridi', 'prior', 'primar', 'mon_', 'mmer', 'lish', 'higher_', 'fatal', 'employe', 'dirty', 'cris', 'conf', 'ckle', 'blend', 'bility_', 'baseball', 'awake', 'arr', 'ape', 'alive_', 'Wid', 'Santa_', 'Kei', 'Dep', 'Burn', 'Bob_', '´', 'warn', 'unknown_', 'twenty_', 'touches', 'supernatural', 'sitcom', 'saving_', 'rupt', 'relatively_', 'possibilit', 'nose', 'mes_', 'massive', 'male', 'ied', 'honor', 'heroes_', 'gig', 'gangs', 'divi', 'diat', 'consequen', 'classics', 'cases', 'bug', 'brief', 'bott', 'assume_', 'associate', 'assistan', 'arra', 'aria', 'absen', 'VHS_', 'Steve', 'Port', 'Paris', 'Old_', 'Morgan_', 'Horr', 'High_', 'General', 'Din', 'Dark_', 'Colo', 'Avoid_', 'zel', 'unnecessary_', 'unexpected_', 'tragedy_', 'tim', 'stle', 'stereo', 'stai', 'send_', 'recommended_', 'produce', 'pregnan', 'noon', 'move', 'ludicrous', 'lude', 'length', 'ident_', 'ide_', 'grue', 'focused', 'extraordinar', 'desperate', 'depress', 'dai', 'creature_', 'covered_', 'chief', 'boss_', 'asking_', 'Yeah', 'WW', 'Rid', 'Island', 'FA', 'Denn', 'Ch', 'Basically', 'Ang', 'Ami', '?! ', '): ', 'virtually_', 'underg', 'truck', 'training', 'tif', 'surf', 'rmin', 'reject', 'rante', 'plots_', 'placed_', 'ni_', 'mature', 'lousy_', 'justice_', 'io_', 'glori', 'gentle', 'fly_', 'explanation_', 'execut', 'exaggerat', 'events', 'elie', 'destructi', 'choose_', 'characteriz', 'char', 'cent_', 'books', 'bby', 'appreciated', 'allo', 'Neve', 'Nee', 'Jackson_', 'Irish', 'IN_', 'During_', 'Devil', 'Count', 'yes_', 'user', 'unpr', 'tual', 'treasure', 'stronge', 'sorr', 'ruined_', 'reputation', 'rently', 'related', 'quel', 'produce_', 'presum', 'politics', 'plans', 'painting', 'killers', 'initial_', 'impli', 'ify', 'hooke', 'funnie', 'fad', 'empty_', 'driver', 'di_', 'detect', 'designed', 'deserve', 'believ', 'awesome', 'accents', 'Your', 'Thank_', 'RE_', 'Pacino', 'Movies', 'Jay', 'IMDb', 'Hugh', 'Festival', 'Enter', 'Donn', 'Christi', 'Alm', 'Academy_', '000_', 'ycl', 'vivi', 'upset', 'ups_', 'unp', 'tiny', 'surprises', 'study_', 'strongly_', 'speaks', 'size', 'riv', 'relation', 'quee', 'py', 'never', 'mainstream', 'libera', 'latest', 'ising', 'insu', 'icia', 'hurt_', 'freedom', 'estl', 'emotionally_', 'dust', 'desc', 'convinced_', 'compell', 'cock', 'clothes_', 'cameo_', 'blind_', 'besides', 'attacke', 'Victor_', 'Return', 'Poo', 'Never_', 'Nel', 'Hey', 'Caine', 'Brando', 'ually_', 'tive', 'silen', 'rew', 'quate', 'preach', 'ological', 'nude', 'multiple', 'link', 'lge', 'ledge', 'laz', 'integr', 'hn', 'hie', 'folks_', 'experiences', 'emphasi', 'earlier', 'delivered_', 'deco', 'deaths', 'continuity', 'complicate', 'burne', 'boyfriend', 'awkward_', 'atrocious', 'amuse', 'ack_', 'Wilson', 'Turn', 'Robin_', 'Pr', 'Om', 'Mun', 'Meanwhile', 'Jessi', 'Jess', 'Jenn', 'Gand', 'Et', 'Canadian_', 'Brothers', 'Bake', 'Ah', '1990', 'wreck', 'unif', 'toi', 'teens', 'smart', 'shir', 'serves_', 'sati', 'rix', 'remain_', 'pub', 'propaganda', 'players_', 'plas', 'ping', 'overcom', 'orious', 'minde', 'meeting_', 'lph', 'loyal', 'lm', 'llin', 'lake', 'kar', 'istic', 'instru', 'included_', 'hire', 'graph', 'gory_', 'favour', 'elde', 'dum', 'destroy_', 'destin', 'denti', 'consistent', 'cameo', 'betr', 'arrest', 'appea', 'animal', 'amen', 'accidentally', 'acce', 'Silv', 'Saturday_', 'ST_', 'Res', 'MGM', 'Korea', 'Fam', 'Asian_', 'Alle', 'zu', 'weeks', 'ticke', 'terrifi', 'table_', 'storytell', 'stopped_', 'steal', 'slash', 'shoe', 'select', 'rocke', 'roa', 'record_', 'previously', 'participa', 'okay_', 'ogr', 'official', 'nke', 'mistakes', 'misca', 'memorabl', 'logue', 'itat', 'ists_', 'intelligence_', 'ien', 'greate', 'ggy', 'gangster_', 'critical', 'closer', 'cartoons', 'boot', 'accepta', 'abu', 'TER', 'States', 'Roberts', 'LER', 'Jones_', 'Hat', 'Eri', 'Eliza', 'Coop', 'wes', 'uninteresting', 'tense', 'teet', 'suffers_', 'stranger', 'station_', 'scu', 'resid', 'rand', 'popula', 'ours', 'opene', 'occurr', 'non_', 'nominated_', 'mol', 'missi', 'memory_', 'memories_', 'maid', 'intri', 'inju', 'inevitabl', 'humans_', 'hanging_', 'gratuitous_', 'gas_', 'forme', 'direct', 'difficult', 'department', 'damag', 'creatures', 'cif', 'Warner', 'Titan', 'Matt_', 'Larr', 'KI', 'Hor', 'Holm', 'Fair', 'Drew', 'Andr', '1960', 'wri', 'vely', 'uls', 'travel_', 'trat', 'transf', 'timi', 'suspen', 'struggling', 'spoil_', 'slaps', 'sink', 'reti', 'reaction_', 'quest_', 'pilot_', 'narration', 'invite', 'hearing_', 'gm', 'gai', 'full', 'frankly', 'fairy', 'expe', 'dimension', 'dent_', 'deme', 'contest', 'conscious', 'cked', 'below_', 'ations', 'angel', 'alive', 'absurd_', 'Wer', 'Tha', 'Stewar', 'Play', 'Picture', 'Part', 'Martin', 'Franc', 'Fir', 'Fas', 'Ev', 'Cos', 'Carre', 'Bog', 'BU', 'Anne_', 'yan', 'writ', 'vit', 'vai', 'summ', 'ston', 'stin', 'stif', 'sensitive', 'rules', 'provided_', 'prostitut', 'pretentious_', 'poignan', 'pai', 'paced_', 'offi', 'nds_', 'mig', 'laughable_', 'instal', 'inati', 'forget', 'eit', 'defend', 'conse', 'beaut', 'Spr', 'Rol', 'Our_', 'NOT', 'Lugosi', 'Luci', 'Las', 'Imp', 'Ic', 'Earl', 'Davis_', 'Cod', '!)', 'twiste', 'sincer', 'sacrifice', 'references_', 'range_', 'purchase', 'orn', 'noise', 'neo', 'mecha', 'lun', 'insult_', 'fully', 'flicks_', 'fair', 'endless_', 'eeri', 'devot', 'curious_', 'comical', 'beth_', 'begin', 'aura', 'ase_', 'ach_', 'Sullivan', 'St', 'Sarah', 'London', 'Liv', 'Kee', 'Jackie_', 'Hong', 'Emil', 'Clair', 'China', 'California', 'Atlant', 'Alice', '\"?', '!!!!!!', 'xico', 'wick', 'visi', 'viewed_', 'uish', 'tribu', 'theatrical_', 'talks_', 'smile_', 'seven', 'reminisce', 'relie', 'rci', 'rah', 'pleasant_', 'plague', 'picio', 'ounce', 'murdered_', 'mul', 'mous_', 'mock', 'mira', 'mete', 'loss_', 'initia', 'iest_', 'health', 'harde', 'gran', 'goal', 'ghe', 'fy', 'fix', 'experienced', 'edy', 'deci', 'conflict_', 'compe', 'committed', 'cele', 'brick', 'bour', 'bers', 'berate', 'artist_', 'anth', 'Woody_', 'WWI', 'V_', 'TT', 'Sunday', 'Story_', 'Rob_', 'Rachel', 'Nin', 'Gree', 'Friday', 'Dev', 'Bros', 'Brana', ' : ', 'wha', 'vig', 'views', 'unconvincing', 'smi', 'sibl', 'quen', 'pointless', 'perp', 'particular', 'overwhelm', 'offered', 'nominat', 'naturally', 'locke', 'left', 'lady', 'ilt', 'iel', 'ication', 'historic', 'haunting', 'gem_', 'figures', 'figured_', 'evol', 'ery', 'eco', 'dynami', 'duct', 'doi', 'description', 'cultural', 'contrac', 'confide', 'combined', 'coin', 'cke', 'chosen_', 'amed', 'agon', 'Thomas_', 'THI', 'Nation', 'MOVIE', 'Lev', 'Jeff', 'Hoffman', 'Glen', 'Even', '1st_', ' ! ', 'yu', 'trappe', 'thir', 'tension', 'tail', 'table', 'split', 'sides', 'settle', 'schem', 'save', 'ruc', 'prime', 'posit', 'painte', 'ndi', 'marry_', 'kun', 'killing', 'isol', 'iot', 'intend', 'impres', 'horribly_', 'hing', 'heroi', 'gle_', 'fri', 'fitt', 'fighter', 'estin', 'ee_', 'drunk_', 'directly', 'dinos', 'chose_', 'changing', 'blonde_', 'benefi', 'award_', 'av', 'aki', 'ages', 'acter', 'VERY_', 'Ur', 'Tel', 'Superman_', 'Real', 'Phi', 'Palm', 'Nicol', 'Johnson', 'Jesus_', 'J_', 'Hes', 'Helen', 'Fun', 'Fle', 'Dir', 'Chap', 'vag', 'uncon', 'ues', 'types_', 'tical', 'sprin', 'sorts', 'securi', 'previ', 'porno', 'party', 'pare', 'method', 'medica', 'mber', 'landscape', 'jor', 'jail', 'imper', 'hunter', 'happening', 'gritty', 'gain_', 'flaws_', 'fak', 'extra', 'edited_', 'ecc', 'dragg', 'chie', 'cant_', 'breast', 'authorit', 'ated', 'ality', 'advise', 'advan', 'according_', 'Wors', 'Unlike', 'United_', 'Simon_', 'Riv', 'Pea', 'Michell', 'Exp', 'Child', 'Cham', 'Bourne', 'Basi', 'widow', 'walked_', 'upp', 'unforg', 'uld_', 'tting', 'till_', 'thy_', 'talents_', 'suspenseful', 'summer_', 'storm', 'screening', 'scare_', 'realizes_', 'rce', 'raw', 'qu', 'ngl', 'magic', 'lac', 'jobs', 'ister_', 'inti', 'inha', 'ill_', 'hands', 'grin', 'forward', 'examin', 'equent', 'emi', 'contact', 'concentrat', 'compu', 'competen', 'biograph', 'attach', 'amus', 'alik', 'activi', 'William', 'Myst', 'Luke_', 'Live', 'Life_', '15', 'zes', 'werewolf', 'warne', 'uring_', 'trilogy', 'swim', 'stumble', 'spite', 'spends_', 'sleep_', 'sist', 'sentence', 'rma', 'reward', 'reviewer_', 'pul', 'preten', 'performed', 'passing', 'par_', 'oph', 'livi', 'kinds_', 'journal', 'isticat', 'inva', 'idi', 'ham_', 'fte', 'few', 'featured', 'ern_', 'eag', 'dollars', 'disb', 'depth', 'cryin', 'cross_', 'content', 'contemporary_', 'colors', 'chee', 'because', 'asy', 'agent_', 'Willi', 'Warr', 'Ven', 'Vamp', 'Roch', 'ONE', 'Movie', 'Mau', 'Mass', 'MST', 'Hin', 'Hear', 'Gue', 'Gl', 'Freddy_', 'Definite', 'Captain_', 'BBC', '??? ', '80s_', '\"), ', 'wol', 'weekend', 'vampires', 'underst', 'tial_', 'terrorist', 'strength_', 'starre', 'soldier_', 'snow', 'sity', 'ruin_', 'retar', 'resu', 'required', 'recommended', 'ques', 'propo', 'presents_', 'perm', 'overt', 'olds', 'occas', 'nn_', 'nen', 'nei', 'mail', 'lost', 'lion', 'libr', 'inner_', 'headed', 'happy', 'guest', 'govern', 'friendly', 'explains', 'ens_', 'effectively', 'draw_', 'downright', 'dete', 'dde', 'dare', 'cring', 'courag', 'conspi', 'comedie', 'claims_', 'cide', 'chas', 'captivat', 'bite', 'bare', 'author_', 'addition', 'Vid', 'Rh', 'Oliv', 'Nata', 'Mexican', 'Keaton_', 'Iron', 'Barb', 'ALL_', '12', '!), ', 'worthwhile', 'weake', 'ung', 'understood_', 'unbelievable', 'superf', 'stolen', 'stereotypic', 'spoiler', 'sight', 'scares', 'rut', 'remove', 'remotely_', 'releva', 'prese', 'poke', 'ndou', 'mbla', 'lucky_', 'lling_', 'legendary', 'imagery', 'humou', 'hug', 'hired', 'heck', 'guilty', 'extras', 'expected', 'everywhere', 'dry_', 'drea', 'directed', 'dimensional_', 'ddi', 'dden', 'communica', 'cham', 'buddy', 'bank_', 'azi', 'algi', 'adventures', 'accurate_', 'accompan', 'Thom', 'Still_', 'Someone', 'Serious', 'SU', 'Phill', 'Perso', 'Patrick_', 'Lei', 'Jus', 'Gho', 'Get_', 'Freeman', 'Especially_', '?).', '...\"']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제4. 위의 코드를 이용해서 dataset 에는 어떤 text 가 있는지 확인하시오 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True,\n",
    "                          as_supervised=True)\n",
    "\n",
    "encoder = dataset.features['text'].encoder\n",
    "print(encoder.subwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제5.  아래의 스크립트를 수행해보고  You say goodbye and i say hello. 에서 단어 id  를 출력하시오 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = info.features['text'].encoder\n",
    "print('Vocabulary size: {}'.format(encoder.vocab_size))\n",
    "\n",
    "sample_string = 'You say goodbye and I say hello.'\n",
    "\n",
    "encoded_string = encoder.encode(sample_string)\n",
    "print('Encoded string is {}'.format(encoded_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제6. 다시 decode 해서 문자로 출력하시오 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_string = 'You say goodbye and i say hello'\n",
    "\n",
    "encoded_string = encoder.encode(sample_string)\n",
    "print('Encoded string is {}'.format(encoded_string))\n",
    "\n",
    "\n",
    "original_string = encoder.decode(encoded_string)\n",
    "print('The original string: \"{}\"'.format(original_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제7.아래의 스크립트를 수행하여 LSTM 모델을 학습 시키고 정확도를 보면 다음과 같다 . 층을 RNN 으로 비교해보고 정확도를 확인하시오 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.padded_batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.padded_batch(BATCH_SIZE)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(encoder.vocab_size, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_dataset, epochs=10,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=30)\n",
    "\n",
    "test_loss, test_acc = model.eval‎‎uate(test_dataset)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history['val_'+metric], '')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([metric, 'val_'+metric])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True,\n",
    "                          as_supervised=True)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "\n",
    "encoder = info.features['text'].encoder\n",
    "\n",
    "print('Vocabulary size: {}'.format(encoder.vocab_size))\n",
    "\n",
    "sample_string = 'Hello TensorFlow.'\n",
    "\n",
    "encoded_string = encoder.encode(sample_string)\n",
    "print('Encoded string is {}'.format(encoded_string))\n",
    "\n",
    "original_string = encoder.decode(encoded_string)\n",
    "print('The original string: \"{}\"'.format(original_string))\n",
    "\n",
    "assert original_string == sample_string\n",
    "\n",
    "for index in encoded_string:\n",
    "    print('{} ----> {}'.format(index, encoder.decode([index])))\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.padded_batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = test_dataset.padded_batch(BATCH_SIZE)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(encoder.vocab_size, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_dataset, epochs=10,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=30)\n",
    "\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))\n",
    "\n",
    "\n",
    "def pad_to_size(vec, size):\n",
    "    zeros = [0] * (size - len(vec))\n",
    "    vec.extend(zeros)\n",
    "    return vec\n",
    "\n",
    "\n",
    "def sample_predict(sample_pred_text, pad):\n",
    "    encoded_sample_pred_text = encoder.encode(sample_pred_text)\n",
    "\n",
    "    if pad:\n",
    "        encoded_sample_pred_text = pad_to_size(encoded_sample_pred_text, 64)\n",
    "    encoded_sample_pred_text = tf.cast(encoded_sample_pred_text, tf.float32)\n",
    "    predictions = model.predict(tf.expand_dims(encoded_sample_pred_text, 0))\n",
    "\n",
    "    return (predictions)\n",
    "\n",
    "# predict on a sample text without padding.\n",
    "\n",
    "\n",
    "sample_pred_text = ('The movie was cool. The animation and the graphics '\n",
    "                    'were out of this world. I would recommend this movie.')\n",
    "predictions = sample_predict(sample_pred_text, pad=False)\n",
    "print(predictions)\n",
    "\n",
    "# predict on a sample text with padding\n",
    "\n",
    "sample_pred_text = ('The movie was cool. The animation and the graphics '\n",
    "                    'were out of this world. I would recommend this movie.')\n",
    "predictions = sample_predict(sample_pred_text, pad=True)\n",
    "print(predictions)\n",
    "\n",
    "\n",
    "plot_graphs(history, 'accuracy')\n",
    "\n",
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ (오늘의 마지막 문제) 아래의 글을 다른 글로 바꿔서 수행하는데 긍정글과 부정글 2가지로 수행하고 예상하는 숫자가 어떻게 나오는지 확인하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history['val_'+metric], '')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([metric, 'val_'+metric])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True,\n",
    "                          as_supervised=True)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "\n",
    "encoder = info.features['text'].encoder\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.padded_batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = test_dataset.padded_batch(BATCH_SIZE)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(encoder.vocab_size, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_dataset, epochs=10,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=30)\n",
    "\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))\n",
    "\n",
    "\n",
    "def pad_to_size(vec, size):\n",
    "    zeros = [0] * (size - len(vec))\n",
    "    vec.extend(zeros)\n",
    "    return vec\n",
    "\n",
    "\n",
    "def sample_predict(sample_pred_text, pad):\n",
    "    encoded_sample_pred_text = encoder.encode(sample_pred_text)\n",
    "\n",
    "    if pad:\n",
    "        encoded_sample_pred_text = pad_to_size(encoded_sample_pred_text, 64)\n",
    "    encoded_sample_pred_text = tf.cast(encoded_sample_pred_text, tf.float32)\n",
    "    predictions = model.predict(tf.expand_dims(encoded_sample_pred_text, 0))\n",
    "\n",
    "    return (predictions)\n",
    "\n",
    "# predict on a sample text without padding.\n",
    "\n",
    "\n",
    "sample_pred_text = ('The movie was cool. The animation and the graphics '\n",
    "                    'were out of this world. I would recommend this movie.')\n",
    "predictions = sample_predict(sample_pred_text, pad=False)\n",
    "print(predictions)\n",
    "\n",
    "# predict on a sample text with padding\n",
    "\n",
    "sample_pred_text = ('The movie was cool. The animation and the graphics '\n",
    "                    'were out of this world. I would recommend this movie.')\n",
    "predictions = sample_predict(sample_pred_text, pad=True)\n",
    "print(predictions)\n",
    "\n",
    "\n",
    "plot_graphs(history, 'accuracy')\n",
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Stack two or more LSTM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(encoder.vocab_size, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_dataset, epochs=10,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=30)\n",
    "\n",
    "test_loss, test_acc = model.eval‎uate(test_dataset)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))\n",
    "\n",
    "\n",
    "# predict on a sample text without padding.\n",
    "\n",
    "\n",
    "sample_pred_text = ('The movie was not good. The animation and the graphics '\n",
    "                    'were terrible. I would not recommend this movie.')\n",
    "\n",
    "predictions = sample_predict(sample_pred_text, pad=False)\n",
    "print(predictions)\n",
    "\n",
    "# predict on a sample text with padding\n",
    "\n",
    "sample_pred_text = ('The movie was not good. The animation and the graphics '\n",
    "                    'were terrible. I would not recommend this movie.')\n",
    "\n",
    "predictions = sample_predict(sample_pred_text, pad=True)\n",
    "print(predictions)\n",
    "\n",
    "\n",
    "plot_graphs(history, 'accuracy')\n",
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-03T09:07:16.539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, None, 16)          64        \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, None, 64)          12544     \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, None, 64)          24832     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, None, 16)          1040      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, None, 1)           17        \n",
      "=================================================================\n",
      "Total params: 198,497\n",
      "Trainable params: 198,465\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n",
      "Start\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "14000/15000 [===========================>..] - ETA: 4s - loss: 0.6921 - accuracy: 0.5190"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "imdb = keras.datasets.imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "\n",
    "\n",
    "# # 단어와 정수 인덱스를 매핑한 딕셔너리\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# # 처음 몇 개 인덱스는 사전에 정의되어 있습니다\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "# reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "# def decode_review(text):\n",
    "#     return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
    "                                                        value=word_index[\"<PAD>\"],\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=256)\n",
    "\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
    "                                                       value=word_index[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=256)\n",
    "vocab_size = 10000\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 16, input_shape=(None,)))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "# model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Bidirectional(tf.keras.layers.LSTM(32,return_sequences=True, dropout=0.5)))\n",
    "model.add(keras.layers.Bidirectional(tf.keras.layers.LSTM(32,return_sequences=True, activation='relu', dropout=0.5)))\n",
    "model.add(keras.layers.Dense(16, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print('Start')\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "x_val = train_data[:10000]\n",
    "partial_x_train = train_data[10000:]\n",
    "\n",
    "y_val = train_labels[:10000]\n",
    "partial_y_train = train_labels[10000:]\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=50,\n",
    "                    batch_size=500,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)\n",
    "\n",
    "results = model.evaluate(test_data,  test_labels, verbose=2)\n",
    "\n",
    "print(results)\n",
    "\n",
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
