{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>■ 복습</b>\n",
    "    컴퓨터에게 사람의 말(자연어)을 이해시키기 위해서\n",
    "        비교. CNN: 컴퓨터에게 이미지를 이해시키기 위해서\n",
    "    \n",
    "    2장. 개선된 통계기법 -> 단어의 분산표현 생성(밀집벡터) -> 단어와 단어 사이의 유사성\n",
    "    3장. CBOW 신경망을 이용해서 문장을 학습 시킴 -> 단어의 분산 표현 생성(가중치)\n",
    "    4장. CBOW 신경망에 큰 말뭉치가 입력될 수 있도록 개선\n",
    "        CBOW 신경망의 문제점\n",
    "            1. 말뭉치가 크면 메모리 사용량이 많다\n",
    "            2. 성능이 느림\n",
    "            \n",
    "    5장. RNN 신경망 -> 기억하는 신경망\n",
    "        실습\n",
    "            작은 말뭉치                  --------> RNN 신경망에 입력하고 학습\n",
    "            큰 말뭉치(스티브잡스 연설문)\n",
    "                                  문장을 one hot 표현으로 변경\n",
    "        RNN 신경망의 문제점(한계점)\n",
    "            장기기억 취약\n",
    "            기울기 소실 / 폭발\n",
    "            \n",
    "    6장. LSTM 신경망\n",
    "        RNN과 LSTM 신경망의 차이가 무엇\n",
    "            기억셀 존재\n",
    "            게이트를 추가한 RNN LSTM\n",
    "                forget\n",
    "                input\n",
    "                output\n",
    "                새로운 기억\n",
    "        기울기 소실 문제의 원인 : tanh 함수 -> relu함수\n",
    "        기울기 폭발 문제의 원인 : matmul 연산 -> 게이트 이용해서 크기 조정\n",
    "        LSTM 실습\n",
    "            1. 긍정단어/부정단어를 컴퓨터가 인식할 수 있는지\n",
    "            2. 주가예측을 LSTM 신경망으로 구현\n",
    "            \n",
    "    지금까지 배운 내용으로 생활에 필요한 것을 구현한다면\n",
    "        주가예측 또는 시계열 데이터 분석\n",
    "        \n",
    "    책을 공부하는 방법\n",
    "        RNN 원리를 책의 이론으로 배우고 실습은 keras로 구현\n",
    "    \n",
    "    7장. RNN을 사용해서 문장 생성 : 기사작성, 소설을 쓰는 AI 생성, 번역하는 신경망\n",
    "        실습\n",
    "            1. 영어 문장 생성\n",
    "            2. 덧셈을 하는 신경망 구현\n",
    "    8장. 어텐션 : 인공지능에게 질문을 하면 대답을 하는 것을 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 1. 7장의 큰 그림\n",
    "    이번 7장은 6장에서 다룬 RNN 을 사용한 언어 모델을 쌀째 손질하여, 문장을 생성하는 기능을 구현합니다.\n",
    "    그리고  seq2seq 에게 간단한 덧셈 문제를 학습시켜 구현합니다. seq2seq 는 encoder 와 decoder 를 연결한 모델로, 결국 2개의 RNN 을 조합한 단순한 구조입니다. \n",
    "    하지만 그 단순함에도 불구하고 seq2seq 는 매우 큰 가능성을 지니고 있어서 다양한 에플리케이션에 적용할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 2. seq2seq 란 무엇인가요 ?\n",
    "    seqeunce to sequence (시계열에서 시계열)를 뜻하는 말로 한 시계열 데이터를 다른 시계열 데이터로 변환하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 3. seq2seq  구현은 어떻게 하나요 ? \n",
    "    RNN 2개를 연결하는 아주 간단한 방법으로 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 4. seq2seq  는 어디에 응용이 되나요 ?\n",
    "    번역, 챗봇, 뉴스기사 자동 생성, 음성 인식 등에 다양하게 응용되어진다\n",
    "                  ↓ \n",
    "             문장 자동 생성(7장)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 5. 5장에서 배웠던 RNN 신경망의 구조는 어떻게 되나요?\n",
    "    You say goodbye and I say hello. 라는 말뭉치가 들어오면 특정 단어가 주어졌을 때 다음으로 나올 확률이 가장 높은 단어가 무엇인지를 예측하는 구조. \n",
    "    확률이 높은 단어는 선택되기 쉽고 확률이 낮은 단어는 선택되게 어려워지게끔 가중치가 생성되는 구조\n",
    "\n",
    "![fig](http://cfile265.uf.daum.net/image/993A34455F543E9F1EDD39)\n",
    "![fig2](http://cfile261.uf.daum.net/image/9943D5495F5440301FF6C8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 6. 그런데 다르게 테스트를 하기 위해서 책에서 결정적이란 용어와 확률적이란 용어를 설명하면서 저자는 결정적인 방법으로 다음 단어를 선택하지 않고 확률적 방법으로 선택하겠다고 하는데 두 용어의 차이가 무엇입니까?\n",
    "    결정적 : 결과가 예측 가능한 것. 기존에 배웠던 것처럼 특정 단어가 주어졌을 때 다음 단어를 예측 가능 확률이 가장 높은 단어를 선택하도록 하는 것\n",
    "    확률적 : 결과가 확률에 따라 정해지는 것. 선택되는 단어는 실행할 때 마다 달라진다.\n",
    "![fig](http://cfile281.uf.daum.net/image/99E3BC355F5442D620D53C)\n",
    "\n",
    "    you say goodbye and i say hello. 라는 문장을 그대도 다 넣는게 아니라 \n",
    "    I 다음에 나올 확률이 높은 say 를 그 다음 입력 단어로 넣고 say 를 입력한 후 다음 단어로 확률이 높은 단어가 hello 이므로 \n",
    "    hello 가 다음 단어로 입력되어집니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 7. 위와 같이 학습하면 어떤 효과가 있습니까 ?\n",
    "    이렇게 생성된 문장은 훈련 데이터에 존재하지 않는 말. 말 그대로 새로 생성된 문장\n",
    "    언어 모델은 훈련 데이터를 암기한 것이 아니라 훈련 데이터에서 사용된 단어의 패턴을 학습한 것이기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 8. 문장 자동 생성을 영어 문장 자동 생성으로 테스트 해봅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T02:23:57.930614Z",
     "start_time": "2020-09-08T02:23:57.908610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "929589\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "from dataset import ptb\n",
    "\"\"\"\n",
    "929_000개의 학습 단어, 73_000개의 validation 단어, 82_000개의 테스트 단어로 구성\n",
    "예: 일기예보 기사를 자동으로 쓰는 AI를 만들고 싶다면 일기예보 기사의 용어(단어)가 들어있는 데이터 셋을 import 해야한다.\n",
    "\"\"\"\n",
    "from ch07.rnnlm_gen import RnnlmGen\n",
    "\"\"\"\n",
    "훈련 데이터의 문장을 암기하는 게 아니라 사용된 단어의 정렬 패턴을 학습하게 한 클래스\n",
    "You say goodbye and I say hello.\n",
    "\"\"\"\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "corpus_size = len(corpus)\n",
    "print(vocab_size) # 10000\n",
    "print(corpus_size) # 929589"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T02:14:32.824716Z",
     "start_time": "2020-09-08T02:14:32.597673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you 'll take any transformed.\n",
      " once hollywood 's sense is the table in the installed and streak the big board failure in a parental case of fcc lane in five years against mr. roman.\n",
      " upgrade says mr. gorbachev nor blacks and mr. white to veto a oliver gulf a book again.\n",
      " the attack is expected to restrict mr. provinces 's team generation because of making internal material would be pushed and warned between its racketeering changes school.\n",
      " mr. green is only a finnish investor he will use l.a. oliver robinson 's president.\n",
      " material for the\n"
     ]
    }
   ],
   "source": [
    "model = RnnlmGen()\n",
    "model.load_params('ch06/Rnnlm.pkl')\n",
    "\n",
    "# start 문자와 skip 문자 설정\n",
    "start_word = 'you'\n",
    "start_id = word_to_id[start_word] # you의 단어 id를 추출\n",
    "skip_words = ['N', '<unk>', '$'] # 샘플링 하지 않을 단어를 지정\n",
    "skip_ids = [word_to_id[w] for w in skip_words] # 샘플링하지 않을 단어들의 단어 id를 추출\n",
    "\n",
    "# 문장 생성\n",
    "word_ids = model.generate(start_id, skip_ids)\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>', '.\\n') # 한 문장의 끝을 나타내는 <eos> 표시 대신에 줄바꿈\n",
    "\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제. ch07/generate_better_text.py를 수행해서 이번에는 더 좋은 문장이 나오는지 테스트 하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T02:37:02.365673Z",
     "start_time": "2020-09-08T02:37:02.009602Z"
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file: ch06\\BetterRnnlm.pkl",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-94d633ab706d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBetterRnnlmGen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ch06/BetterRnnlm.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# start 문자와 skip 문자 설정\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Desktop\\Itwill ws\\rnn\\common\\base_model.py\u001b[0m in \u001b[0;36mload_params\u001b[1;34m(self, file_name)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No file: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: No file: ch06\\BetterRnnlm.pkl"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "from dataset import ptb\n",
    "from ch07.rnnlm_gen import BetterRnnlmGen\n",
    "from common.np import *\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "corpus_size = len(corpus)\n",
    "\n",
    "\n",
    "model = BetterRnnlmGen()\n",
    "model.load_params('ch06/BetterRnnlm.pkl')\n",
    "\n",
    "# start 문자와 skip 문자 설정\n",
    "start_word = 'you'\n",
    "start_id = word_to_id[start_word]\n",
    "skip_words = ['N', '<unk>', '$']\n",
    "skip_ids = [word_to_id[w] for w in skip_words]\n",
    "# 문장 생성\n",
    "word_ids = model.generate(start_id, skip_ids)\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>', '.\\n')\n",
    "\n",
    "print(txt)\n",
    "\n",
    "model.reset_state()\n",
    "\n",
    "start_words = 'the meaning of life is'\n",
    "start_ids = [word_to_id[w] for w in start_words.split(' ')]\n",
    "\n",
    "for x in start_ids[:-1]:\n",
    "    x = np.array(x).reshape(1, 1)\n",
    "    model.predict(x)\n",
    "\n",
    "word_ids = model.generate(start_ids[-1], skip_ids)\n",
    "word_ids = start_ids[:-1] + word_ids\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>', '.\\n')\n",
    "print('-' * 50)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>[쉬움주의] 문장 자동 생성_한글_문제</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제1. 다음 text 의 문장을 단어와 단어 id 로 변환하시오 !\n",
    "    text = 경마장에 있는 말이 뛰고 있다 그의 말이 법이다 가는 말이 고와야 오는 말이 곱다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T02:54:14.377357Z",
     "start_time": "2020-09-08T02:54:14.362354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'말이': 1, '경마장에': 2, '있는': 3, '뛰고': 4, '있다': 5, '그의': 6, '법이다': 7, '가는': 8, '고와야': 9, '오는': 10, '곱다': 11}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "\"\"\"\n",
    "문장마다 길이가 다르므로 패딩을 해서 일관된 문장 길이로 만들어주는 모듈\n",
    "신경망에 배치처리를 하려면 일관된 사이즈로 문장을 만들어줘야 하기 때문에 필요\n",
    "\"\"\"\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer # text를 숫자로 변환하는 모듈\n",
    "\n",
    "text = \"\"\"경마장에 있는 말이 뛰고 있다\\n 그의 말이 법이다\\n 가는 말이 고와야 오는 말이 곱다\"\"\"\n",
    "\n",
    "t = Tokenizer() \n",
    "t.fit_on_texts([text]) # text 문자를 가지고 corpus, word_to_id, id_to_word를 생성\n",
    "print(t.word_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제2.  '경마장에 있는 말이 뛰고 있다'와 '그의 말이 법이다'와 '가는 말이 고와야 오는 말이 곱다'라는 세 가지 문장이 있다고 해봅시다. 모델이 문맥을 학습할 수 있도록 전체 문장의 앞의 단어들을 전부 고려하여 학습하도록 데이터를 재구성한다면 아래와 같이 총 11개의 샘플이 구성됩니다.  아래의 11개의 샘플을 아래와 같이 단어id로 출력되게 하시오 ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T02:59:18.165367Z",
     "start_time": "2020-09-08T02:59:18.153365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 1, 4, 5]]\n",
      "[[6, 1, 7]]\n",
      "[[8, 1, 9, 10, 1, 11]]\n"
     ]
    }
   ],
   "source": [
    "sequences = list()\n",
    "for line in text.split('\\n'):\n",
    "    encoded = t.texts_to_sequences([line])\n",
    "    print(encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ※ 문제3. (점심시간 문제) 아래의 결과를 아래와 같이 출력되게 코드를 수정하시오\n",
    "    [[2, 3, 1, 4, 5]]                             [2, 3, 1, 4, 5]\n",
    "    [[6, 1, 7]]                       ->          [6, 1, 7]\n",
    "    [[8, 1, 9, 10, 1, 11]]                        [8, 1, 9, 10, 1, 11]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T03:03:38.899700Z",
     "start_time": "2020-09-08T03:03:38.886699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 1, 4, 5]\n",
      "[6, 1, 7]\n",
      "[8, 1, 9, 10, 1, 11]\n"
     ]
    }
   ],
   "source": [
    "sequences = list()\n",
    "for line in text.split('\\n'):\n",
    "    encoded = t.texts_to_sequences([line])\n",
    "    for sent in encoded:\n",
    "        print(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
