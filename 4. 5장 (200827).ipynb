{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>■ 복습</b>\n",
    "    1장. CNN 복습\n",
    "    2장. 개선된 통계기법으로 단어의 분산 표현을 얻어내는 방법\n",
    "        컴퓨터가 단어의 의미를 파악할 수 있게 해주는 정보\n",
    "    3장. word2vec 신경망인 CBOW를 사용해서 단어의 분산표현을 얻어냄\n",
    "    4장. CBOW 신겸앙에 큰 말뭉치를 넣을 수 있도록 개선\n",
    "        1. 입력층 ----> 은닉층 : Embedding 기법\n",
    "        2. 은닉층 ----> 출력층, 오차함수 : sigmoid 함수\n",
    "                               네거티브 샘플링\n",
    "        큰 말뭉치(PTB 데이터셋) ----> 개선된 CBOW 신경망 학습\n",
    "                                    입력층의 가중치 -> 단어의 분산표현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>▩ 1. 5장의 큰그림</b>\n",
    "    RNN 은 데이터를 순환 시킴으로써 과거에서 현재, 그리고 미래로 데이터를 계속해서 흘려 보냅니다.\n",
    "    이를 위해 RNN 계층 내부에는 '은닉상태' 를 기억하는 능력이 추가 되었습니다.\n",
    "    '언어모델' 은 단어 시퀀스에 확률을 부여하며, 특히 조건부 언어 모델은 지금까지의 단어 시퀀스로 부터 다음에 출현할 단어의 확률을 계산해줍니다. \n",
    "    여기서 RNN 을 이용한 신경망 구성이 등장하며, 이론적으로 아무리 긴 시계열 데이터라도 중요 정보를 RNN 의 은닉 상태에 기억해 둘 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b> ▩ 2. CBOW 신경망의 역할이 무엇이었습니까 ?</b>\n",
    "![fig5-1](dl2_images/fig5-1.png)\n",
    "\n",
    "$\\qquad$ <b>답</b>: 그림처럼 맥락 $\\rm{W_{t-1}}$와 $\\rm{W_{t+1}}$로 부터 타깃 $\\rm{W_{t}}$를 추측하는 일을 수행<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>▩ 2. 그럼 $W_{t-1}$ 과 $W_{t+1}$ 가 주어졌을 때 타깃이 Wt 가 될 확률을 수식으로 나타내면 어떻게 됩니까 ?</b>\n",
    "    답:  \n",
    "<h1>$$P(\\rm{W_{t}\\;|\\;W_{t-1},\\;W_{t+1}})$$</h1><br><br>\n",
    "\n",
    "<b>You  &nbsp;&nbsp;&nbsp;    ?   &nbsp;&nbsp;&nbsp;  say &nbsp;&nbsp;&nbsp;  goodbye &nbsp;&nbsp;  and  &nbsp;&nbsp;  I &nbsp;&nbsp;  say &nbsp;&nbsp;  hello.</b><br><br>\n",
    "$W_{t-1} \\quad W_{t} \\quad W_{t+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b> ▩ 3. 그런데 이번에는 왼쪽 두 단어만을 맥락으로 생각해서 다음에 나오는 단어가 무슨 단어인지 예측해 보겠습니다. 이를 그림과 수식으로 나타내면 어떻게 됩니까 ?</b>\n",
    "    답:\n",
    "![fig5-2](dl2_images/fig5-2.png)\n",
    "\n",
    "<h1>$$P(\\rm{W_{t}\\;|\\;W_{t-2},\\;W_{t-1}})$$<br><br></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b> ▩ 4. 언어 모델이 무엇 입니까 ?</b>\n",
    "    답:  단어 나열에 확률을 부여하는 모델\n",
    "    \n",
    "    예: (cont1) (cont2)  (target)\n",
    "         You     say     goodbye.  (prob : 0.092)\n",
    "         You     say     good die. (prob : 0.00000000000032)\n",
    "         \n",
    "    언어모델은 새로운 문장을 생성하는 용도로 이용할 수 있다.\n",
    "              새로운 음악을 생성하는 용도로"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b> ▩ 5. 아래의 식이 의미하는게 무엇입니까 ?</b>\n",
    "<h2>$$P(A\\;|\\;B) = {{P(A,B)}\\over {P(B)}}$$</h2>\n",
    "\n",
    "    답:\n",
    "        P(A,B)는 A와 B가 동시에 일어날 확률\n",
    "        A와 B가 모두 일어날 확률은 B가 일어날 확률 P(B)와 B가 일어난 후 A가 일어날 확률 P(A|B)를 곱한 것과 같다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b> ▩ 6. 아래의 식이 의미하는게 무엇입니까 ?</b>\n",
    "![e5-6](dl2_images/e5-6.png)\n",
    "\n",
    "$\\qquad$<b>답</b>: $m$개의 단어가 동시에 일어날 확률을 확률의 곱셈으로 변경해서 $P(\\rm{W_{m}|A})$처럼 사후 확률로 나타냄<br><br>\n",
    "\n",
    "    동시확률은 사후 확률의 총 곱과 같다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>▩ 7. 지금까지의 내용을 정리하면 아래의 그림과 같습니다. 우리의 목표는 무엇입니까 ?</b>\n",
    "![fig5-3](dl2_images/fig5-3.png)\n",
    "\n",
    "$\\qquad$ <b>답</b>: $P(W_{t}|W_{1},...,W_{t-1})$이라는 확률을 얻는 것<br><br>\n",
    "\n",
    "    You say goodbye and I say hello. 면 어떻게 나타낼 수 있는가?\n",
    "$\\rm{P}(\\quad \\rm{hello}\\qquad | \\quad \\rm{You, say, goodbye, and, I, say)} = \\; ?$<br>\n",
    "$\\qquad$ <b><u>target</u> $\\qquad \\qquad$ <u>contexts</u></b><br>\n",
    "$\\rm{P}(hello \\; | \\; You, say, goodbye, and, I, say) * \\\\ \\rm{P}(say \\; | \\;  You, say, goodbye, and, I) * \\\\ \\rm{P}(I \\; | \\;  You, say, goodbye, and) * \\\\ \\rm{P}(and \\; | \\;  You, say, goodbye) * \\\\ \\rm{P}(goodbye \\; | \\;  You, say) * \\\\ \\rm{P}(say \\; | \\;  You) * \\\\ \\rm{P}(You) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>▩ 8. 아래의 문장에서 ?  왼쪽 10개의 단어를 맥락으로 CBOW 모델을 만든다고 하면 어떤 문제가 생깁니까?</b>\n",
    "![fig5-4](dl2_images/fig5-4.png)\n",
    "\n",
    "    답:  단어 10개보다 더 왼쪽에 있는 단어의 정보는 무시된다. ?는 Tom인데 Tom을 알 수가 없다.\n",
    "    \n",
    "    그래서 해결 방법은 그 전 문장도 기억을 해줘야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>▩ 8. 아래의 왼쪽 모델과 오른쪽 모델의 차이가 무엇입니까 ?</b>\n",
    "![fig5-5](dl2_images/fig5-5.png)\n",
    "\n",
    "    답 : 왼쪽 모델은 은닉층에서 단어 벡터들이 더해지므로 맥락의 단어 순서는 무시된다. 예를들어 (You, say), (say, You)라는 맥락을 똑같이 취급한다.\n",
    "         오른쪽 모델은 맥락의 단어 순서를 고려한 모델. 그런데 오른쪽 모델을 선택하게되면 가중치 매개변수도 늘어나게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>▩ 9. 그렇다면 이 문제를 어떻게 해결해야할까요 ?   </b>\n",
    "    답 : RNN.\n",
    "        RNN은 왼쪽 맥락이 아무리 길더라도 그 맥락의 정보를 기억하는 메커니즘을 갖추고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>▩ 10. RNN 이 무엇인가요 ? </b>\n",
    "    답: RNN은 Recurrent Nueral Net. 순환하는 신경망이라는 뜻.\n",
    "![fig5-8](dl2_images/fig5-8.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>▩ 11. RNN 계층을 '기억력을 가지는 계층' 이라고 합니다. 기억력을 가지게끔 RNN 계층에서 일어나는 계산은 무엇인가요?</b>\n",
    "![fig5-19](dl2_images/fig5-19.png)\n",
    "\n",
    "    답: RNN에는 2개의 가중치가 있다. \n",
    "<tr>\n",
    "    $\\qquad$하나는 입력 $x$를(새로 배우는 것을) 출력 $\\rm{h}$로 변환하기 위한 가중치 $\\rm{W}_{x}$이고, <br> \n",
    "    $\\qquad$다른 하나는 RNN 출력(복습)을 다음 시각의 출력으로 변환하기 위한 가중치 $\\rm{W_{h}}$이다.<br>\n",
    "    $\\qquad$이를 이용해서 행렬곱을 계산하고 그 합을 $tanh(x)$함수를 이용해 반환.<br>\n",
    "    $\\qquad$그 결과가 시각 $t$의 출력 $h_{t}$가 된다.<br>\n",
    "    $\\qquad$이 $\\rm{h}$에는 <b>'상태'</b>를 가지고 있으며 이 상태가 바로 <b>'기억력'</b>이다.<br>\n",
    "    $\\qquad$식 5.9가 기억력을 만드는 식이다.\n",
    "    </tr>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>▩ 12. RNN 계층에서의 오차 역전파시 발생하는 문제점이 무엇입니까 ?</b>\n",
    "![fig5-10](dl2_images/fig5-10.png)\n",
    "<tr>\n",
    "    <b>답</b>: RNN도 CNN과 마찬가지로 위의 그림처럼 먼저 순전파를 수행하고 역전파를 수행하면서 기울기를 구한다. <br>\n",
    "    그리고 여기서의 오차 역전파법을 <b>'시간 방향으로 펼친 신경망의 오차 역전파법 (BackPropagation Through Time, BPTT)'</b><br>\n",
    "    <br>\n",
    "</tr>\n",
    "<tr>\n",
    "    <b>BPTT 신경망의 문제점 2가지</b><br>\n",
    "    &nbsp;&nbsp;1. 메모리 사용량의 증가가 발생<br>\n",
    "    <td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>원인</b> : 순조로운 역전파가 이뤄지려면 순전파일 때의 정보를 메모리에 계속 유지해야하는데 시계열 데이터가 길어짐에 따라 메모리 사용량이 증가하게 된다.<br><br>\n",
    "    &nbsp;&nbsp;2. 기울기 소실 문제<br>\n",
    "    <td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>원인</b> : 긴 시계열 데이터를 학습하게 되면 길이만큼의 시계열 데이터를 다루게 된다.<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 그러면 가로로 길이만큼 늘어선 신경망이 되는데 계층이 길어지게되면 오차가 역전파 될 때 아래의 그림처럼 기울기 소실문제가 발생하게 된다.</td>\n",
    "</tr>\n",
    "\n",
    "![lossofbp](https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F997E1B4C5BB6EAF239)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>▩ 13. 그래서 이를 해결하기 위한 방법이 무엇입니까 ?</b>\n",
    "    답: Truncated BPTT 기법을 사용한다.\n",
    "![fig5-11](dl2_images/fig5-11.png)    \n",
    "\n",
    "        순전파일 때는 흐름이 끊어지지 않게 계속 전달하고\n",
    "        역전파일 때는 연결을 적당한 길이로 잘라내 그 잘라낸 신경망 단위로 학습을 수행한다.\n",
    "        맨 끝에서만 기울기가 온다고 하면 기울기 소실 문제가 발생하므로 각각의 블록 단위로 독립적으로 오차역전파를 구현한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>▩ 14. 다시 위의 오차역전파 법을 그림을 그리면 어떻게 됩니까?</b>\n",
    "![fig5-12](dl2_images/fig5-12.png)![fig5-13](dl2_images/fig5-13.png)\n",
    "\n",
    "    답: 위의 그림처럼 순전파 계산일 때는 앞 블럭의 은닉 상태인 h9가 필요했는데 역전파일 때는 첫번째 블럭에서 마지막 은닉상태인 h9를 이용해서 미분해서 역전파 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>▩ 15. Truncated BPTT 의 미니배치 학습은 어떻게 됩니까?</b>\n",
    "    답: 지금까지의 신경망의 데이터 입력은 미니배치수 1개 였다.\n",
    "        이번에는 미니배치수가 2개이고 두 문장을 신경망에 넣는다\n",
    "\n",
    "    첫번째 미니배치는 처음부터 순서대로 입력 (x0~x9)\n",
    "    두번째 미니배치는 500번째의 데이터를(x500~x509)를 시작위치로 정하고 입력\n",
    "    다음번에 데이터를 넘길때는 x10~x19번까지 이어서 입력하고\n",
    "                             x500~x519번까지 이어서 입력을 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>▩ 16. Time RNN 계층이란 무엇입니까 ?</b>\n",
    "    답:  아래의 그림처럼 우리가 다룰 신경망은 길이가 T인 시계열 데이터를 받는다\n",
    "![fig5-16](dl2_images/fig5-16.png)\n",
    "\n",
    "    그리고 아래의 (x0, x1, ....., xT-1) 을 묶은 xs 라고 하고 이 묶은 xs를 입력하면 ( h0, h1, ..., hT-1) 을 묶은 hs 를 출력하는 단일 계층을 볼수 있는데 \n",
    "    이 T 개의 단계분을 한꺼번에 처리하는 계층을 'Time RNN 계층' 이라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>▩ 17. 그러면 Time RNN 계층에 기억력을 갖는 계산은 어떻게 되나요 ?</b>\n",
    "![fig5-17](dl2_images/fig5-17.png)\n",
    "![fig5-18](dl2_images/fig5-18.png)\n",
    "![fig5-21](dl2_images/fig5-21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>▩ 18. 위의 식을 바탕으로 만든 아래의 RNN 클래스의 순전파를 실행하세요.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T05:21:00.648881Z",
     "start_time": "2020-08-27T05:21:00.530953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.23801565 -1.02135371  1.55705273 -1.44544914]\n",
      " [ 0.24232253 -1.29133195 -1.76005261 -0.99675716]\n",
      " [-1.75409354  1.52847472  0.44126969 -0.91829649]]\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "Wx = np.random.randn(3,4)\n",
    "print(Wx)\n",
    "print(np.zeros_like(Wx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T05:17:20.430141Z",
     "start_time": "2020-08-27T05:17:20.319209Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n",
    "        h_next = np.tanh(t)\n",
    "        self.cache = (x, h_prev, h_next)\n",
    "\n",
    "        return h_next # Affine으로 가서 softmax를 통과하고 그 다음에 나올 단어를 추측하는데 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T05:38:47.854605Z",
     "start_time": "2020-08-27T05:38:47.674690Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,4) (3,4) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-172bbfcb357c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mrnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_prev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-0ef11ae70426>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, h_prev)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_prev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mWx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWh\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mh_next\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_next\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,4) (3,4) "
     ]
    }
   ],
   "source": [
    "# You say goodbye\n",
    "You = [1,0,0]\n",
    "say = [0,1,0]\n",
    "goodbye = [0,0,1]\n",
    "\n",
    "x = np.array([[1,0,0], [0,1,0], [0,0,1]])\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "Wx = np.random.randn(3,4)\n",
    "Wh = np.random.randn(3,4) # 이전 단어를 기억하기 위한 가중치\n",
    "b = np.random.randn(4)\n",
    "h_prev = np.random.randn(4,3)\n",
    "\n",
    "rnn = RNN(Wx, Wh, b)\n",
    "print(rnn.forward(x, h_prev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>▩ 19. 아래의 그림으로 구현한 역전파를 실행해보세요.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "\n",
    "        self.params = [Wx, Wh, b]\n",
    "\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "\n",
    "        Wx, Wh, b = self.params\n",
    "\n",
    "        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n",
    "\n",
    "        h_next = np.tanh(t)\n",
    "\n",
    "        self.cache = (x, h_prev, h_next)\n",
    "\n",
    "        return h_next\n",
    "\n",
    "    def backward(self, dh_next):\n",
    "\n",
    "        Wx, Wh, b = self.params\n",
    "\n",
    "        x, h_prev, h_next = self.cache\n",
    "\n",
    "        dt = dh_next * (1 - h_next ** 2)\n",
    "\n",
    "        db = np.sum(dt, axis=0)\n",
    "\n",
    "        dWh = np.dot(h_prev.T, dt)\n",
    "\n",
    "        dh_prev = np.dot(dt, Wh.T)\n",
    "\n",
    "        dWx = np.dot(x.T, dt)\n",
    "\n",
    "        dx = np.dot(dt, Wx.T)\n",
    "\n",
    "        self.grads[0][...] = dWx\n",
    "\n",
    "        self.grads[1][...] = dWh\n",
    "\n",
    "        self.grads[2][...] = db\n",
    "\n",
    "        return dx, dh_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>▩ 20.아래의 그림의 TimeRNN 클래스에서 stateful 이 필요한 이유가 무엇인가요 ?</b>\n",
    "    답: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeRNN:\n",
    "\n",
    "    def __init__(self, Wx, Wh, b, stateful=False):\n",
    "\n",
    "        self.params = [Wx, Wh, b]\n",
    "\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "\n",
    "        self.layers = None\n",
    "\n",
    "        self.h, self.dh = None, None\n",
    "\n",
    "        self.stateful = stateful\n",
    "\n",
    "    def forward(self, xs):\n",
    "\n",
    "        Wx, Wh, b = self.params\n",
    "\n",
    "        N, T, D = xs.shape\n",
    "\n",
    "        D, H = Wx.shape\n",
    "\n",
    "        self.layers = []\n",
    "\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "\n",
    "        if not self.stateful or self.h is None:\n",
    "\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "\n",
    "        for t in range(T):\n",
    "\n",
    "            layer = RNN(*self.params)\n",
    "\n",
    "            self.h = layer.forward(xs[:, t, :], self.h)\n",
    "\n",
    "            hs[:, t, :] = self.h\n",
    "\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return hs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>▩ 21. RNN 계층의 전체 큰 그림이 어떻게 됩니까 ?</b>\n",
    "    답: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>▩ 22. 그러면 시계열 데이터를 한꺼번에 처리하는 time rnn 의 큰 그림은 어떻게 됩니까?</b>\n",
    "    답:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>▩ 23. 마지막으로 RNN 신경망의 평가는 어떻게 합니까 ?</b>\n",
    "    답:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "답: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
