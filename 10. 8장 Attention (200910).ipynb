{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 1. seq2seq 의 문제점은 무엇인가요 ?\n",
    "    고정길이 벡터로 변환해야만 한다\n",
    "![fig](http://cfile265.uf.daum.net/image/996DF2395F576B82063808)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 2. 그러면 어떻게 개선을 해야하나요 ?\n",
    "    seq2seq를 이루고 있는 Encoder와 Decoder를 개선\n",
    "    Encoder 출력의 길이를 입력 문장의 길이에 따라 바꿔준다\n",
    "        Encoder는 아무리 긴 문장이라도 하나의 고정길이 벡터로 변환. 개선된 Encoder는 입력된 단어와 같은 수의 벡터를 얻을 수 있다.\n",
    "![fig](http://cfile289.uf.daum.net/image/990CAC395F576D2E05FE3C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 3. 위의 그림 8-2 에서 주목할 내용이 무엇입니까 ?\n",
    "    LSTM 계층의 은닉 상태의 '내용'\n",
    "        은닉 상태의 내용은 각 시각의 직전에 입력된 단어에 대한 정보가 많이 포함되어져 있다.\n",
    "        바로 직전에 '고양이'라는 단어를 입력했다면 이때 출력되는 은닉 상태 벡터는 '고양이' 성분이 많이 들어간 벡터라고 생각할 수 있다.\n",
    "![fig](http://cfile277.uf.daum.net/image/99C299345F5771D10E0EAF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 4. Decoder 는 어떻게 개선해야하나요? \n",
    "    Encoder는 각 단어에 대응하는 LSTM 계층의 은닉상태 벡터를 hs로 모아 출력\n",
    "    hs가 Decoder에 전달되어 시계열 변환이 이루어진다.\n",
    "    Encoder의 LSTM 계층의 '마지막' 은닉상태를 Decoder의 LSTM 계층의 '첫' 은닉상태로 설정\n",
    "![fig](http://cfile292.uf.daum.net/image/99A4EC4F5F5789A9055A94)\n",
    "\n",
    "    인코더가 단어에 해당하는 벡터들의 집합을 hs 로 리턴했으면 이 집합을 다 활용해야하는데 그런데 아래의 그림 처럼 hs 에서 마지막 줄만 빼내어 Decoder 에게 전달 합니다. 바로 이부분을 개선해야합니다. 다 활용되게 해야합니다. \n",
    "![fig](http://cfile254.uf.daum.net/image/99810E4F5F578A7E0EA7B1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 5. 그러면 어떻게 해야 Encoder 의 hs 를 다 활용하게 할 수 있을까요?\n",
    "    나 = I, 고양이 = cat 처럼 입력과 출력의 여러 단어 중 어떤 단어끼리 서로 관련되어 있는가 라는 대응관계를 seq2seq에게 학습시키면 된다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 6. 이런 대응관계를 나타내는 정보를 무엇이라고 합니까?\n",
    "    얼라인먼트(Alignment)\n",
    "        지금까지는 사람이 수작업으로 했는데 이제는 기계를 통한 자동화 되어지고 있다 -> 어텐션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 7. 앞으로 우리의 목표는 무엇입니까?\n",
    "    도착어 단어와 대응관계에 있는 출발어 단어의 정보를 골라내는 것\n",
    "        예: cat(도착어) = 고양이(출발어)\n",
    "            이 정보를 이용하여 번역을 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 8. 이렇게 필요한 정보에만 주목하여 그 정보로 부터 시계열 변환을 수행하는 것을 무엇이라고 합니까?\n",
    "    Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 9. 아래의 그림 8-6의 신경망으로 하고 싶은일은 단어들의 얼라인먼트 추출합니다. 각 시각에서 Decoder 에 입력된 단어와 대응관계인 단어의 백터를 hs 에서 골라내겠다는 뜻입니다.  이러한 선택작업이 아래의 그림에서 어느 부분입니까 ?\n",
    "    어떤 계산 부분. \n",
    "    Decoder가 'I'를 출력할 때 hs에서 '나'에 대해 대응하는 벡터를 선택하게 하면 되는데 이 선택 작업을 '어떤 계산'으로 해낸다\n",
    "![fig](http://cfile283.uf.daum.net/image/99E28A4B5F578EBC13A982)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 10. decoder 가 I 를 출력할 때 hs 에서 '나' 에 대응하는 벡터를 선택하게 하려면 구체적으로 어떻게 구현해야 할까요 ?\n",
    "    각 단어에 대해서 그것이 얼마나 중요한지를 나타내는 '가중치'를 이용\n",
    "        '나'에 해당되는 가중치는 0.8\n",
    "![fig](http://cfile270.uf.daum.net/image/995A84465F579209149F19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 11. 그림 8-6의 어떤 계산은 무엇입니까 ?\n",
    "    단어의 벡터 hs와 가중치 a와의 가중합을 구하여 원하는 벡터 c를 얻는 것\n",
    "        c : 맥락 벡터. '나'에 대응하는 가중치가 0.8이므로 이 맥락 벡터에는 '나' 벡터 성분이 많이 포함되어 있다.\n",
    "![fig](http://cfile297.uf.daum.net/image/991517475F5793610DFA4B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 12. 지금까지의 애기를 코드로 구현해 봅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "T, H = 5, 4\n",
    "hs = np.random.randn(T, H)\n",
    "a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])\n",
    "\n",
    "ar = a.reshape(5, 1).repeat(4, axis=1)\n",
    "print(ar.shape)\n",
    "\n",
    "t = hs * ar\n",
    "print(t.shape)\n",
    "\n",
    "c = np.sum(t, axis=0)\n",
    "print(c)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (점심시간 문제) 하나의 문장에 대해서만 맥락벡터를 구한 것이고 점심시간 문제는 배치단위로 구현하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미치배치 10개로 hs를 생성\n",
    "hs = np.random.randn(10,5,4)\n",
    "a = np.random.randn(10,5)\n",
    "ar = a.reshape(10,5,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 14. 그럼 가중치 a 는 어떻게 구해야하는가요?\n",
    "    데이터로부터 자동으로 학습되어져 구해질 수 있도록 해야한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 15. 그럼 어떻게 해야 가중치 a 가 자동으로 구해질 수 있는것인가요 ?\n",
    "    아래의 그림과 같이 hs의 5개의 벡터에는 각각 입력된 단어에 대한 정보가 많이 포함되어져 있다.\n",
    "![fig](http://cfile299.uf.daum.net/image/99F39D3F5F594A23081566)\n",
    "\n",
    "    위의 hs를 decoder 계층에서 전부 사용하면서 decoder에 입력된 단어와 대응관계인 단어의 벡터를 hs에서 골라내려는 alignment를 추출\n",
    "    아래와 같이 계산이 이루어지는데\n",
    "![fig](http://cfile268.uf.daum.net/image/9915B7405F5949ED0A47C2)\n",
    "    \n",
    "    위 계산은 다음 그림과 같이 각 단어의 중요도(기여도)를 나타내는 '가중치 a'를 별도로 계산하도록해서 hs와 a의 가중합으로 맥락벡터를 얻어낸다\n",
    "    맥락벡터에는 각 단어에 대한 성분이 많이 포함되어져 있다. \n",
    "    따라서 맥락벡터(c)를 통해서 '나'가 선택되는 맥락벡터(c)가 출력층에 들어오면 'I'가 선택될 확률이 높아지도록 출력층의 가중치인 W가 학습되어지도록 한다.\n",
    "![fig](http://cfile251.uf.daum.net/image/9901BF395F594BF307FCEF)\n",
    "\n",
    "    가중치 a가 자동으로 계산되어지게 해야하는데 이 a를 자동으로 알아내야하는 가중치로 본다면 이 가중치가 좋은 가중치인지 아닌지를 먼저 판단\n",
    "    좋은 가중치가 되도록 자동으로 갱신되어져야 한다. 좋은 가중치인지 판단하는 것은 h가 hs의 각 단어벡터와 얼마나 '비슷한가' 수치로 나타내는데\n",
    "    이 때 벡터의 내적을 이용\n",
    "![fig2](http://cfile292.uf.daum.net/image/99CD44385F5949400993A4)\n",
    "\n",
    "    학습을 시키게 되면 점점 softmax의 결과가 해당 단어의 확률값이 점점 1에 가까워지고 나머지는 0에 가까워지게끔 학습이 된다.\n",
    "![fig3](http://cfile277.uf.daum.net/image/99155A405F59495D0A7620)\n",
    "![fig4](http://cfile250.uf.daum.net/image/99B8113D5F59513A240E40)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 16. 지금까지의 순서를 종합한 아래의 그림으로 Attention 을 설명하세요 \n",
    "![fig](http://cfile277.uf.daum.net/image/993508335F59520D0ACC68)\n",
    "\n",
    "    Encoder에서 출력된 단어들의 정보를 담은 hs가 decoder의 특정 시계열 시점에서 LSTM층에서 출력된 h와의 유사도를 구해서\n",
    "    그 유사도를 softmax 함수를 통과시켜 가중치(기여도) a를 구하고 a는 hs과 곱해져서 맥락벡터인 c를 산출\n",
    "    \n",
    "![fig](http://cfile248.uf.daum.net/image/992A54355F5952280BC264)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 17. 결국 아래의 그림의 Attention 층의 역할이 무엇인가요 ?\n",
    "    Encoder가 산출한 hs를 Decoder의 모든 시계열 t 시점에서 다 활용할 수 있도록 hs에서 필요한 정보(특정 벡터)를 골라내기 위해 집중시키는 역할"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install customized_konlpy\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "from ckonlpy.tag import Twitter\n",
    "from tensorflow.keras.utils import get_file\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from nltk import FreqDist\n",
    "from functools import reduce\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 훈련 데이터 : https://bit.ly/31SqtHy\n",
    "# 테스트 데이터 : https://bit.ly/3f7rH5g\n",
    "\n",
    "TRAIN_FILE = os.path.join(\"D:/Desktop/Itwill ws/rnn/qa1_single-supporting-fact_train_kor.txt\")\n",
    "TEST_FILE = os.path.join(\"D:/Desktop/Itwill ws/rnn/qa1_single-supporting-fact_test_kor.txt\")\n",
    "\n",
    "# conda install nltk\n",
    "\n",
    "i = 0\n",
    "lines = open(TRAIN_FILE, \"rb\")\n",
    "for line in lines:\n",
    "    line = line.decode(\"utf-8\").strip()\n",
    "    i = i + 1\n",
    "    print(line)\n",
    "    if i == 20:\n",
    "        break\n",
    "\n",
    "\n",
    "def read_data(dir):\n",
    "    stories, questions, answers = [], [], []  # 각각 스토리, 질문, 답변을 저장할 예정\n",
    "    story_temp = []  # 현재 시점의 스토리 임시 저장\n",
    "    lines = open(dir, \"rb\")\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.decode(\"utf-8\")  # b' 제거\n",
    "        line = line.strip()  # '\\n' 제거\n",
    "        idx, text = line.split(\" \", 1)  # 맨 앞에 있는 id number 분리\n",
    "        # 여기까지는 모든 줄에 적용되는 전처리\n",
    "\n",
    "        if int(idx) == 1:\n",
    "            story_temp = []\n",
    "\n",
    "        if \"\\t\" in text:  # 현재 읽는 줄이 질문 (tab) 답변 (tab)인 경우\n",
    "            question, answer, _ = text.split(\"\\t\")  # 질문과 답변을 각각 저장\n",
    "            stories.append([x for x in story_temp if x])  # 지금까지의 누적 스토리를 스토리에 저장\n",
    "            questions.append(question)\n",
    "            answers.append(answer)\n",
    "\n",
    "        else:  # 현재 읽는 줄이 스토리인 경우\n",
    "            story_temp.append(text)  # 임시 저장\n",
    "\n",
    "    lines.close()\n",
    "    return stories, questions, answers\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "train_data = read_data(TRAIN_FILE)\n",
    "test_data = read_data(TEST_FILE)\n",
    "\n",
    "# %%\n",
    "train_stories, train_questions, train_answers = read_data(TRAIN_FILE)\n",
    "test_stories, test_questions, test_answers = read_data(TEST_FILE)\n",
    "\n",
    "\n",
    "print('훈련용 스토리의 개수 :', len(train_stories))\n",
    "print('훈련용 질문의 개수 :', len(train_questions))\n",
    "print('훈련용 답변의 개수 :', len(train_answers))\n",
    "print('테스트용 스토리의 개수 :', len(test_stories))\n",
    "print('테스트용 질문의 개수 :', len(test_questions))\n",
    "print('테스트용 답변의 개수 :', len(test_answers))\n",
    "\n",
    "print(train_stories[3572])\n",
    "\n",
    "print(train_questions[3572])\n",
    "\n",
    "print(train_answers[3572])\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "# def tokenize(sent):\n",
    "#     print(re.split('(\\W+)?', sent))\n",
    "#     return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
    "\n",
    "def tokenize(sent):\n",
    "    return [x.strip() for x in re.split(r'(\\W+)?', sent) if x and x.strip()]\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "def preprocess_data(train_data, test_data):\n",
    "    counter = FreqDist()\n",
    "\n",
    "    # 두 문장의 story를 하나의 문장으로 통합하는 함수\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "\n",
    "    # 각 샘플의 길이를 저장하는 리스트\n",
    "    story_len = []\n",
    "    question_len = []\n",
    "\n",
    "    for stories, questions, answers in [train_data, test_data]:\n",
    "        for story in stories:\n",
    "            stories = tokenize(flatten(story))  # 스토리의 문장들을 펼친 후 토큰화\n",
    "            story_len.append(len(stories))  # 각 story의 길이 저장\n",
    "            for word in stories:  # 단어 집합에 단어 추가\n",
    "                counter[word] += 1\n",
    "        for question in questions:\n",
    "            question = tokenize(question)\n",
    "            question_len.append(len(question))\n",
    "            for word in question:\n",
    "                counter[word] += 1\n",
    "        for answer in answers:\n",
    "            answer = tokenize(answer)\n",
    "            for word in answer:\n",
    "                counter[word] += 1\n",
    "\n",
    "    # 단어 집합 생성\n",
    "    word2idx = {word: (idx + 1) for idx, (word, _) in enumerate(counter.most_common())}\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "    # 가장 긴 샘플의 길이\n",
    "    story_max_len = np.max(story_len)\n",
    "    question_max_len = np.max(question_len)\n",
    "\n",
    "    return word2idx, idx2word, story_max_len, question_max_len\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "word2idx, idx2word, story_max_len, question_max_len = preprocess_data(train_data, test_data)\n",
    "print(word2idx)\n",
    "\n",
    "twitter = Twitter()\n",
    "\n",
    "twitter.add_dictionary('은경이', 'Noun')\n",
    "twitter.add_dictionary('경임이', 'Noun')\n",
    "twitter.add_dictionary('수종이', 'Noun')\n",
    "\n",
    "\n",
    "# print(twitter.morphs('은경이는 화장실로 이동했습니다.'))\n",
    "# print(twitter.morphs('경임이는 정원으로 가버렸습니다.'))\n",
    "# print(twitter.morphs('수종이는 복도로 뛰어갔습니다.'))\n",
    "# print(twitter.morphs('필웅이는 부엌으로 복귀했습니다.'))\n",
    "# print(twitter.morphs('수종이는 사무실로 갔습니다.'))\n",
    "# print(twitter.morphs('은경이는 침실로 갔습니다.'))\n",
    "\n",
    "\n",
    "def tokenize(sent):\n",
    "    return twitter.morphs(sent)\n",
    "\n",
    "\n",
    "word2idx, idx2word, story_max_len, question_max_len = preprocess_data(train_data, test_data)\n",
    "# print(word2idx)\n",
    "\n",
    "vocab_size = len(word2idx) + 1\n",
    "\n",
    "\n",
    "# print(vocab_size)\n",
    "\n",
    "# print('스토리의 최대 길이 :', story_max_len)\n",
    "# print('질문의 최대 길이 :', question_max_len)\n",
    "\n",
    "\n",
    "def vectorize(data, word2idx, story_maxlen, question_maxlen):\n",
    "    Xs, Xq, Y = [], [], []\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "\n",
    "    stories, questions, answers = data\n",
    "    for story, question, answer in zip(stories, questions, answers):\n",
    "        xs = [word2idx[w] for w in tokenize(flatten(story))]\n",
    "        xq = [word2idx[w] for w in tokenize(question)]\n",
    "        Xs.append(xs)\n",
    "        Xq.append(xq)\n",
    "        Y.append(word2idx[answer])\n",
    "\n",
    "        # 스토리와 질문은 각각의 최대 길이로 패딩\n",
    "        # 정답은 원-핫 인코딩\n",
    "    return pad_sequences(Xs, maxlen=story_maxlen), \\\n",
    "           pad_sequences(Xq, maxlen=question_maxlen), \\\n",
    "           to_categorical(Y, num_classes=len(word2idx) + 1)\n",
    "\n",
    "\n",
    "Xstrain, Xqtrain, Ytrain = vectorize(train_data, word2idx, story_max_len, question_max_len)\n",
    "Xstest, Xqtest, Ytest = vectorize(test_data, word2idx, story_max_len, question_max_len)\n",
    "\n",
    "# print(Xstrain.shape, Xqtrain.shape, Ytrain.shape, Xstest.shape, Xqtest.shape, Ytest.shape)\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Permute, dot, add, concatenate\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Activation\n",
    "\n",
    "# 에포크 횟수\n",
    "train_epochs = 120\n",
    "# 배치 크기\n",
    "batch_size = 32\n",
    "# 임베딩 크기\n",
    "embed_size = 50\n",
    "# LSTM의 크기\n",
    "lstm_size = 64\n",
    "# 과적합 방지 기법인 드롭아웃 적용 비율\n",
    "dropout_rate = 0.30\n",
    "\n",
    "# 플레이스 홀더. 입력을 담는 변수\n",
    "input_sequence = Input((story_max_len,))\n",
    "question = Input((question_max_len,))\n",
    "\n",
    "# print('Stories :', input_sequence)\n",
    "# print('Question:', question)\n",
    "\n",
    "# 스토리를 위한 첫번째 임베딩. 그림에서의 Embedding A\n",
    "input_encoder_m = Sequential()\n",
    "input_encoder_m.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=embed_size))\n",
    "input_encoder_m.add(Dropout(dropout_rate))\n",
    "# 결과 : (samples, story_max_len, embedding_dim) / 샘플의 수, 문장의 최대 길이, 임베딩 벡터의 차원\n",
    "\n",
    "# 스토리를 위한 두번째 임베딩. 그림에서의 Embedding C\n",
    "# 임베딩 벡터의 차원을 question_max_len(질문의 최대 길이)로 한다.\n",
    "input_encoder_c = Sequential()\n",
    "input_encoder_c.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=question_max_len))\n",
    "input_encoder_c.add(Dropout(dropout_rate))\n",
    "# 결과 : (samples, story_max_len, question_max_len) / 샘플의 수, 문장의 최대 길이, 질문의 최대 길이(임베딩 벡터의 차원)\n",
    "\n",
    "# 질문을 위한 임베딩. 그림에서의 Embedding B\n",
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(input_dim=vocab_size,\n",
    "                               output_dim=embed_size,\n",
    "                               input_length=question_max_len))\n",
    "question_encoder.add(Dropout(dropout_rate))\n",
    "# 결과 : (samples, question_max_len, embedding_dim) / 샘플의 수, 질문의 최대 길이, 임베딩 벡터의 차원\n",
    "\n",
    "# 실질적인 임베딩 과정\n",
    "input_encoded_m = input_encoder_m(input_sequence)\n",
    "input_encoded_c = input_encoder_c(input_sequence)\n",
    "question_encoded = question_encoder(question)\n",
    "\n",
    "print('Input encoded m', input_encoded_m)\n",
    "print('Input encoded c', input_encoded_c)\n",
    "print('Question encoded', question_encoded)\n",
    "\n",
    "# 스토리 단어들과 질문 단어들 간의 유사도를 구하는 과정\n",
    "# 유사도는 내적을 사용한다.\n",
    "match = dot([input_encoded_m, question_encoded], axes=-1, normalize=False)\n",
    "match = Activation('softmax')(match)\n",
    "print('Match shape', match)\n",
    "# 결과 : (samples, story_maxlen, question_max_len) / 샘플의 수, 문장의 최대 길이, 질문의 최대 길이\n",
    "\n",
    "# add the match matrix with the second input vector sequence\n",
    "response = add([match, input_encoded_c])  # (samples, story_max_len, question_max_len)\n",
    "response = Permute((2, 1))(response)  # (samples, question_max_len, story_max_len)\n",
    "print('Response shape', response)\n",
    "\n",
    "# concatenate the response vector with the question vector sequence\n",
    "answer = concatenate([response, question_encoded])\n",
    "print('Answer shape', answer)\n",
    "\n",
    "answer = LSTM(lstm_size)(answer)  # Generate tensors of shape 32\n",
    "answer = Dropout(dropout_rate)(answer)\n",
    "answer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n",
    "# we output a probability distribution over the vocabulary\n",
    "answer = Activation('softmax')(answer)\n",
    "\n",
    "# build the final model\n",
    "model = Model([input_sequence, question], answer)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# start training the model\n",
    "history = model.fit([Xstrain, Xqtrain],\n",
    "                    Ytrain, batch_size, train_epochs,\n",
    "                    validation_data=([Xstest, Xqtest], Ytest))\n",
    "\n",
    "# save model\n",
    "model.save('model.h5')\n",
    "\n",
    "# plot accuracy and loss plot\n",
    "plt.subplot(211)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(history.history[\"acc\"], color=\"g\", label=\"train\")\n",
    "plt.plot(history.history[\"val_acc\"], color=\"b\", label=\"validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(history.history[\"loss\"], color=\"g\", label=\"train\")\n",
    "plt.plot(history.history[\"val_loss\"], color=\"b\", label=\"validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# labels\n",
    "ytest = np.argmax(Ytest, axis=1)\n",
    "\n",
    "# get predictions\n",
    "Ytest_ = model.predict([Xstest, Xqtest])\n",
    "ytest_ = np.argmax(Ytest_, axis=1)\n",
    "\n",
    "NUM_DISPLAY = 30\n",
    "\n",
    "print(\"{:18}|{:5}|{}\".format(\"질문\", \"실제값\", \"예측값\"))\n",
    "print(39 * \"-\")\n",
    "\n",
    "for i in range(NUM_DISPLAY):\n",
    "    question = \" \".join([idx2word[x] for x in Xqtest[i].tolist()])\n",
    "    label = idx2word[ytest[i]]\n",
    "    prediction = idx2word[ytest_[i]]\n",
    "    print(\"{:20}: {:7} {}\".format(question, label, prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
