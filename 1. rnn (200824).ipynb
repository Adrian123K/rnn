{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ※ 딥러닝_CNN_NCS_노트정리_문제168_.pdf 다음주 목요일까지 제출\n",
    "\n",
    "## <b>■ CNN으로 할 수 있는 일</b>\n",
    "    1. 불량품 선별을 인공지능이 자동화\n",
    "    2. 영상의학과에서 환자의 CT, MRI 사진을 인공지능이 판독\n",
    "    3. 영상에서 마스크 쓴 사람과 안 쓴 사람을 분류\n",
    "    4. 기타\n",
    "    \n",
    "    CNN 복습\n",
    "        1. 밑바닥 딥러닝 1권\n",
    "        2. Tensorflow 1.x, 1.x+keras, \n",
    "           Tensorflow 2.x\n",
    "           \n",
    "    Tensorflow 2.x의 가장 큰 특징\n",
    "        1. 즉시 실행모드\n",
    "        2. @tf.function으로 파이썬 코드로 gpu를 사용\n",
    "        3. 이미지 증식\n",
    "        4. 가장 최적화된 가중치를 내려받는 기능 (early stop)\n",
    "        5. 전이학습 (미리 만들어 놓은 가중치와 신경망을 가져와서 쓰는 기능)\n",
    "            ex. import VGG16\n",
    "\n",
    "# <b>■ RNN</b>\n",
    "## <b> ■1장. RNN을 배워야하는 이유</b>\n",
    "    1. 번역 신경망 생성\n",
    "        https://github.com/OValery16/Language-Translation-with-deep-learning-\n",
    "    2. 한국어 번역 신경망\n",
    "        https://github.com/haven-jeon/ko_en_neural_machine_translation\n",
    "    3. 주가 예측 \n",
    "        시계열 데이터 분석 가능\n",
    "        https://www.youtube.com/watch?v=pS91rcB3CdM\n",
    "    4. 인공지능과의 대화\n",
    "        https://www.youtube.com/watch?v=m6sWsd_tkAs\n",
    "        https://www.youtube.com/watch?v=NHA69lCd1ZM&feature=youtu.be\n",
    "    \n",
    "## <b>■ 2장. 자연어와 단어의 분산 표현</b>\n",
    "<br><b><h3>문제1. 빈공간의 단어를 채워넣으시오</h></b><br>\n",
    "<b><h4> 2장은 컴퓨터에게 <u>'단어의 의미'</u> 를 이해시키기 위한 기술 내용입니다. <br><br>\n",
    "    이 목적을 달성하기 위해서 <u>'시소러스'</u> 라는 기법을 이용합니다. <br><br>\n",
    "시소러스 기반 기법에서는 단어들의 관련성을 사람이 수작업으로 하나씩 정의 합니다. <br><br>\n",
    "    이 작업은 매우 힘들고 표현력에 한계가 있습니다. 그래서 나온게 <u>'통계기반기법'</u> 인데 통계 기반 기법은 말뭉치로 부터 단어의 의미를 자동으로 추출하고, 그 의미를 벡터로 표현합니다.<br><br>\n",
    "    구체적으로는 단어의 '동시 발생 행렬' 을 만들고, <u>'PPMI 행렬'</u>로 변환한다음, 안정성을 높이기 위해 <u>'특이값 분해(SVD)'</u> 를 이용해 차원을  감소시켜서 각 단어의 분산 표현을 만들어 냅니다. <br><br>\n",
    "    분산 표현에 따르면 의미가 비슷한 단어들의 벡터 공간에서도 서로 가까이 모여 있음을 확인할 수 있습니다.</h></b><br>\n",
    "\n",
    "    ▩ 1. 자연어 처리란 ?     P.78\n",
    "        답: 우리 말을 컴퓨터에게 이해시키기 위한 기술분야\n",
    "\n",
    "    IBM의 슈퍼컴퓨터 ‘IBM왓슨’이 ‘태양의 후예’나 ‘베테랑’ 같은 한국의 인기 드라마·영화 등을 보면서 한국어 공부를 시작했다. \n",
    "    주로 영어 스페인어 등 서구 언어를 습득해온 왓슨이 아시아 언어를 배우는 것은 일본어에 이어 한국어가 두 번째다. \n",
    "    IBM왓슨이 한국어를 습득하면 국내 주요 기업은 물론 정부 및 공공기관 등도 인공지능(AI)을 활용한 고객 응대, 개인화 서비스 등이 가능해질 것으로 예상된다.\n",
    "    데이비드 케니 IBM왓슨 총괄사장(사진)은 21일 서울 여의도 한국IBM 사무실에서 기자 간담회를 열고 “한국은 글로벌 기업이 많이 진출해 있는 중요한 시장이기 때문에 \n",
    "    슈퍼컴퓨터인 왓슨이 한국어를 배우는 것은 필수”라며 “아직은 어린아이가 한글을 배우듯이 한글을 읽고 드라마 등에서 나오는 언어를 계속 반복해 듣고 있는 수준”이라고 설명했다. \n",
    "    왓슨은 인간의 자연어로 묻는 질문에 답할 수 있는 IBM의 인공지능 컴퓨터다. 컴퓨터 언어가 아닌 인간이 쓰는 언어 그대로 인간과 대화하는 것이 가능하다.\n",
    "                                               - 한국경제 IBM 왓슨 '태양의 후예' 보며 한국어 열공 기사중 중에서....\n",
    "\n",
    "    컴퓨터 언어가 아닌 인간이 쓰는 언어 그대로 인간과 대화하는 것이 가능하다.\n",
    "\n",
    "    https://www.youtube.com/watch?v=GV01B5kVsC0\n",
    "\n",
    "\n",
    "    ▩ 2. 자연어 처리가 추구하는 목표는 ?\n",
    "        답: 사람의 말을 컴퓨터가 이해하도록 만들어서 컴퓨터가 우리에게 도움이 되는 일을 수행하게 하는 것\n",
    "\n",
    "    ▩ 3. 시소러스 기법이란 ? \n",
    "        답: 유의어 사전이라는 뜻으로 같은 단어(동의어)들을 다음과 같이 한 그룹으로 분류해두고 단어들간의 관계를 그래프로 표현하여 단어 사이의 연결을 정의\n",
    "    \n",
    "    ▩ 4. 자연어 처리 분야에서 가장 유명한 시소러스는 ?\n",
    "        답: 프린스턴 대학교 WordNet\n",
    "\n",
    "    ▩ 5. 시소러스의 문제점은 ?\n",
    "        답: 사람을 쓰는 비용이 크다.\n",
    "                시소러스는 만드는 데 엄청난 인력 비용이 발생\n",
    "            단어의 의미가 시대적 변화에 대응하기 어렵다.\n",
    "                ex) 창렬하다.\n",
    "            단어의 미묘한 차이를 표현할 수 없다.\n",
    "                ex) 빈티지와 레트로는 같은 의미지만 용법이 다르다.\n",
    "\n",
    "    ▩ 6. 시소러스의 문제점을 피하기 위해서 필요한 기법은 ?\n",
    "        답: 통계 기반 기법과 신경망을 사용한 추론 기반 기법\n",
    "                강아지/고양이 신경망을 만들고 이 신경망에 한번도 보지못한 강아지 사진을 입력하고 맞춰보라고 하면 신경망이 추론을 한다. [0.612, 0.388]\n",
    "\n",
    "    ▩ 7. 말뭉치란 ?\n",
    "        답: 자연어 처리를 염두해두고 수집된 대량의 텍스트 데이터 \n",
    "\n",
    "    ▩ 8. 통계 기반 기법의 목표는 무엇입니까 ?\n",
    "        답: 사람의 지식으로 가득한 말뭉치에서 자동으로, 효율적으로 그 핵심을 추출하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>■ 자연어와 단어의 분산표현 이론 문제_첫번째</b>\n",
    "#### 1. 아래의 스크립트를 수행해보고 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T02:06:09.988716Z",
     "start_time": "2020-08-24T02:06:09.969728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you say goodbye and i say hello .\n"
     ]
    }
   ],
   "source": [
    "text = 'You say goodbye and I say hello.'\n",
    "\n",
    "text = text.lower()\n",
    "text = text.replace('.', ' .')\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    답:\n",
    "        text의 문장을 소문자로 다 변환한 후 마침표 앞에 공백을 넣은 스크립트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 아래의 스크립트를 수행해보고 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T02:07:04.215388Z",
     "start_time": "2020-08-24T02:07:04.210390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '.']\n"
     ]
    }
   ],
   "source": [
    "words = text.split(' ')\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    답:\n",
    "        공백으로 단어들을 분리하여 리스트화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 아래의 스크립트를 수행해보고 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T02:07:56.721180Z",
     "start_time": "2020-08-24T02:07:56.714189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n",
      "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n"
     ]
    }
   ],
   "source": [
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "\n",
    "for word in words:\n",
    "    if word not in word_to_id:\n",
    "        new_id = len(word_to_id)\n",
    "        word_to_id[word] = new_id\n",
    "        id_to_word[new_id] = word\n",
    "\n",
    "print (id_to_word)        \n",
    "print (word_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    답:\n",
    "        분리된 단어들을 딕셔너리에 넣으면서 아이디(인덱스) 생성, 아이디를 통해 단어를 value로 하는 딕셔너리 생성\n",
    "            단어:아이디\n",
    "            아이디:단어 \n",
    "        단어의 아이디(인덱스)를 이용하여 단어를 불러오거나 단어를 직접 넣어 아이디(인덱스)를 불러올 수 있도록 생성\n",
    "            text 말뭉치를 자연어 처리하기 위한 전처리 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 아래의 스크립트를 수행해보고 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T02:09:51.257660Z",
     "start_time": "2020-08-24T02:09:51.249664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n"
     ]
    }
   ],
   "source": [
    "import  numpy as  np\n",
    "corpus = [word_to_id[w] for w in words]\n",
    "corpus = np.array(corpus)\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    답:\n",
    "        단어를 인덱스로 생성한 것을 이용하여 문장을 인덱스로 표현\n",
    "        words 리스트에서 단어들을 하나씩 뽑아서 word_to_id에 키로 제공, id를 추출, array로 리스트화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 아래의 스크립트를 수행해보고 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T02:11:21.372783Z",
     "start_time": "2020-08-24T02:11:21.363788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n",
      "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    return corpus, word_to_id, id_to_word\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "print (corpus)\n",
    "print (word_to_id)\n",
    "print (id_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    답:\n",
    "        위의 일련 과정들을 preprocess 함수를 생성하여 구현\n",
    "        \n",
    "    지금까지의 작업은 말뭉치 전처리 작업\n",
    "    우리의 목표는 말뭉치를 사용해 '단어의 의미'를 추출하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제. 다른 문장을 직접 넣고 똑같이 수행해보세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T02:23:53.730059Z",
     "start_time": "2020-08-24T02:23:53.724063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 2 7 8]\n",
      "{'adsp': 0, 'is': 1, 'not': 2, 'a': 3, 'useful': 4, 'certification': 5, 'and': 6, 'easy': 7, '.': 8}\n",
      "{0: 'adsp', 1: 'is', 2: 'not', 3: 'a', 4: 'useful', 5: 'certification', 6: 'and', 7: 'easy', 8: '.'}\n"
     ]
    }
   ],
   "source": [
    "text = 'Adsp is not a useful certification and not easy.'\n",
    "t_corpus, t_word_to_id, t_id_to_word = preprocess(text)\n",
    "\n",
    "print(t_corpus)\n",
    "print(t_word_to_id)\n",
    "print(t_id_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>■ 자연어와 단어의 분산표현 이론 문제_두번째</b>\n",
    "#### 1. 단어의 분산 표현이란?\n",
    "    답: 비색이라는 색깔을 표현할 때 (R,G,B) = (170,33,22)라고 표현하듯\n",
    "        단어의 의미를 벡터로 표현하는 것\n",
    "        ex) [0.21, -0.45, 0.83]\n",
    "\n",
    "![fig2-11](dl2_images/fig2-11(e).png)\n",
    "\n",
    "#### 2. 분포가설이란?\n",
    "    답: 단어의 의미는 주변 단어에 의해 형성된다는 것을 말한다.\n",
    "        그 단어가 사용된 맥락이 그 단어의 의미를 형성한다는 것\n",
    "        ex) 사람이 영어를 독해할 때 모르는 단어가 나올 때 마다 바로 사전을 찾는 것이 아니라 그 주변의 단어들을 보고 그 단어의 뜻을 유추해 내듯\n",
    "            컴퓨터에게도 그렇게 학습을 시킴\n",
    "        ex2) I guzzle beer. we guzzle wine. 이라고 하면 guzzle은 drink와 가까운 의미라는 것일 알 수 있다.\n",
    "        \n",
    "        사람처럼 컴퓨터도 위와 같은 방식으로 학습시킴\n",
    "        \n",
    "#### 3. 맥락이란?\n",
    "    답: 주목하는 단어 주변에 놓인 단어\n",
    "![fig](dl2_images/fig2-3.png)\n",
    "\n",
    "#### 4. 분포가설에 기초해 단어를 벡터로 나타내는 방법 중 통계기법이란?\n",
    "    답: 어떤 단어를 주목했을 때 그 주변에 어떤 단어가 몇 번이나 등장하는지 세어 집계하는 방법\n",
    "![fig](dl2_images/fig2-4.png)\n",
    "![fig](dl2_images/fig2-5.png)\n",
    "\n",
    "#### 5. 동시발생 행렬이란?\n",
    "    답: 문장에서 모든 단어에 대해 동시에 발생하는 단어를 표로 정리한 것\n",
    "![fig](dl2_images/fig2-7.png)    \n",
    "\n",
    "#### 6. 책에서 어떤 문장의 동시 발생 행렬을 자동으로 만들어주는 함수 이름이 무엇입니까?\n",
    "    답: create_co_matrix (p.91)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제1. (점심시간 문제) common 폴더안에 util.py에 구현되어 있는 create_co_matrix 함수 코드를 가져와서 아래의 문장의 동시발생행렬을 생성하시오\n",
    "    text = 'you say goodbye and i say hello.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T02:59:09.177991Z",
     "start_time": "2020-08-24T02:59:09.148010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0]\n",
      " [1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    return corpus, word_to_id, id_to_word\n",
    "\n",
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    \n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "\n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "\n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "\n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "\n",
    "    return co_matrix\n",
    "\n",
    "\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "text = 'you say goodbye and i say hello.'\n",
    "vocab_size = 7\n",
    "print(create_co_matrix(corpus,vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    단어의 의미를 파악하는 방법으로 단어의 유사도를 가지고 의미 파악\n",
    "    맥락을 제공하면 그 맥락과 가장 연관성이 높은 단어가 무엇인지를 알아내는게 2장의 목표\n",
    "        짧은 문장 ----------> 동시 발생 행렬 ----------> 단어간 연관성(유사도)을 알아냄\n",
    "                preprocessing             cos_similarity\n",
    "                create_co_matrix          most_similar\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    [[0 1 0 0 0 0 0]\n",
    "     [1 0 1 0 1 1 0]\n",
    "     [0 1 0 1 0 0 0]            say라는 단어와 가장 가까운 단어는 무엇인가?\n",
    "     [0 0 1 0 1 0 0]                1. goodbye\n",
    "     [0 1 0 1 0 0 0]                2. hello\n",
    "     [0 1 0 0 0 0 1]\n",
    "     [0 0 0 0 0 1 0]]\n",
    "\n",
    "\n",
    "#### 7. 벡터 사이의 유사도를 측정하는 방법은 무엇입니까?\n",
    "    답: 코사인 유사도\n",
    "![e2-1](dl2_images/e2-1.png)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T05:02:32.240541Z",
     "start_time": "2020-08-24T05:02:32.174582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071067691154799\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def cos_similarity(x, y, eps=1e-8):\n",
    "    '''코사인 유사도 산출\n",
    "    :param x: 벡터\n",
    "    :param y: 벡터\n",
    "    :param eps: '0으로 나누기'를 방지하기 위한 작은 값\n",
    "    :return:\n",
    "    '''\n",
    "    nx = x / (np.sqrt(np.sum(x ** 2)) + eps)\n",
    "    ny = y / (np.sqrt(np.sum(y ** 2)) + eps)\n",
    "\n",
    "    return np.dot(nx, ny)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "you와 i의 cosine similarity가 어떻게 되는가?\n",
    "\"\"\"\n",
    "c0 = np.array([0, 1, 0, 0, 0, 0, 0])\n",
    "c1 = np.array([0, 1, 0, 1, 0, 0, 0])\n",
    "\n",
    "print(cos_similarity(c0, c1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. 아래의 문장으로 동시 발생 행렬을 만들고 you 라는 단어를 입력했을 때 다른 단어들과의 코사인 유사도를 출력하는 함수가 책에서 무엇입니까 ?\n",
    "    답: most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T05:09:04.039663Z",
     "start_time": "2020-08-24T05:09:03.705866Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      " goodbye: 0.7071067691154799\n",
      " i: 0.7071067691154799\n",
      " hello: 0.7071067691154799\n",
      " say: 0.0\n",
      " and: 0.0\n"
     ]
    }
   ],
   "source": [
    "from common.util import preprocess, create_co_matrix, most_similar\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "most_similar('you', word_to_id, id_to_word, C, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. 동시발생 행렬의 원소는 두 단어가 동시에 발생한 횟수를 나타냅니다. 그런데 car라는 단어가 나올 때는 정관사 the가 발생빈도가 높습니다. <br><br> 그런데 car는 the보다는 drive가 더 관련성이 높습니다.<br><br> 그렇지만 동시발생 행렬로 코사인 유사도를 측정하면 동시발생 행렬은 빈도수로 구성되어져 있기 때문에 drive보다는 the가 관련이 높다고 나옵니다. <br><br>이 문제를 해결하기 위해 나온 것이 무엇입니까?\n",
    "    답: PMI(점별 상호 정보량)이라는 척도를 사용해야한다.\n",
    "        빈도수가 아니라 확률로 관련성을 맺어야 한다.\n",
    "![e2-2](dl2_images/e2-2.png)\n",
    "![e2-3](dl2_images/e2-3.png)\n",
    "<center><b> $P(x)$는 단어 $x$가 일어날 확률을 말하고 $P(y)$는 단어 $y$가 일어날 확률을 말한다.<br> $P(x, y)$는 $x$와 $y$가 동시에 일어날 확률을 뜻한다.<br> PMI값이 높을 수록 관련성이 높다는 의미</b></center>\n",
    "\n",
    "    the와 car와 drive가 각각 1000, 20, 10번 등장했다고 할 때\n",
    "    the와 car의 동시 발생횟수는 10회, car와 drive의 동시 발생횟수는 5회라고 할 때\n",
    "\n",
    "![e2-4](dl2_images/e2-4.png)\n",
    "![e2-5](dl2_images/e2-5.png)\n",
    "\n",
    "    the와 car의 PMI보다 car와 drive의 PMI가 더 높다.\n",
    "    car는 the보다 drive와 더 관련성이 높다고 할 수 있다.\n",
    "    \n",
    "#### 10. 동시발생 행렬을 PPMI 행렬로 바꾸면 어떻게 됩니까 ?\n",
    "        동시발생행렬              PPMI 행렬\n",
    "     [[0 1 0 0 0 0 0]       [[0       1.807 0     0     0     0     0    ]\n",
    "      [1 0 1 0 1 1 0]        [1.807   0     0.807 0     0.807 0.807 0    ]\n",
    "      [0 1 0 1 0 0 0]        [0       0.807 0     1.807 0     0     0    ]\n",
    "      [0 0 1 0 1 0 0]        [0       0     1.807 0     1.807 0     0    ]\n",
    "      [0 1 0 1 0 0 0]        [0       0.807 0     1.807 0     0     0    ]\n",
    "      [0 1 0 0 0 0 1]        [0       0.807 0     0     0     0     2.807]\n",
    "      [0 0 0 0 0 1 0]]       [0       0     0     0     0     2.807 0    ]]\n",
    "      \n",
    "      왜 동시발생 행렬을 PPMI 행렬로 변경했는가\n",
    "          빈도수가 아니라 확률로 연관성을 찾기 위해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제2. 아래의 PPMI 행렬에서 say와 연관성이 높은 단어는 무엇인가?\n",
    "    you say goodbye and i say hello\n",
    "    \n",
    "    [[0       1.807 0     0     0     0     0    ]\n",
    "     [1.807   0     0.807 0     0.807 0.807 0    ]\n",
    "     [0       0.807 0     1.807 0     0     0    ]\n",
    "     [0       0     1.807 0     1.807 0     0    ]\n",
    "     [0       0.807 0     1.807 0     0     0    ]\n",
    "     [0       0.807 0     0     0     0     2.807]\n",
    "     [0       0     0     0     0     2.807 0    ]]\n",
    "    \n",
    "    답: you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. 그런데 위의 행렬을 들여다 보면 원소 대부분이 0인것을 알 수 있습니다. 백터의 원소 대부분이 중요하지 않다는 뜻이죠. 다르게 표현하면 각 원소의 중요도가 낮다는 뜻입니다.<br><br> 이 문제를 해결하기 위한 방법이 무엇입니까? \n",
    "    답: 희소벡터 ----------------------> 밀집벡터\n",
    "                  벡터의 차원 축소(감소)\n",
    "    차원을 감소시키는 방법 중에 특이값 분해(SVD)를 이용하면 결론적으로 단어들간의 관계를 2차원으로 표현할 수 있다.\n",
    "![fig2-11](dl2_images/fig2-11.png)\n",
    "\n",
    "<center><b>i와 goodbye가 겹쳐있다</b></center><br>\n",
    "<center><b>goodbye와 hello가 서로 가깝고 i와 you가 제법 가까이 있음을 알 수 있다.</b></center>\n",
    "\n",
    "    차원을 축소했는데 중요한 정보는 유지시키면서 축소시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. 그런데  차원을 감소 시키는 방법은 무엇이 있습니까 ?\n",
    "    답: 특이값 분해\n",
    "![e2-7](dl2_images/e2-7.png)\n",
    "\n",
    "    SVD는 임의 행렬 X를 U, S, V라는 세 행렬의 곱으로 분해\n",
    "    U와 V는 직교행렬이고 그 열벡터는 서로 직교\n",
    "    S는 대각행렬\n",
    "    \n",
    "    U 행렬을 '단어공간'으로 취급할 수 있고, S는 대각행렬로 그 대각 성분에는 '특이값'이 큰 순서대로 나열되어 있다. 특이값은 중요도로 간주\n",
    "    중요도가 낮은 원소를 깎아내는 방법을 설명\n",
    "![fig2-10](dl2_images/fig2-10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T05:43:31.709476Z",
     "start_time": "2020-08-24T05:43:31.528588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0]\n",
      "[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      "[ 3.409e-01  0.000e+00 -3.886e-16 -1.205e-01 -9.323e-01 -1.110e-16\n",
      "  1.958e-17]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from common.util import preprocess, create_co_matrix, ppmi\n",
    "plt.rcParams['figure.figsize'] = (10,10)\n",
    "plt.rcParams.update({'font.size':20})\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(id_to_word)\n",
    "C = create_co_matrix(corpus, vocab_size, window_size=1)\n",
    "W = ppmi(C)\n",
    "\n",
    "# SVD\n",
    "U, S, V = np.linalg.svd(W)\n",
    "\n",
    "np.set_printoptions(precision=3)  # 유효 자릿수를 세 자리로 표시\n",
    "print(C[0])\n",
    "print(W[0])\n",
    "print(U[0])\n",
    "\n",
    "# 플롯\n",
    "for word, word_id in word_to_id.items():\n",
    "    plt.annotate(word, (U[word_id, 0], U[word_id, 1]))\n",
    "plt.scatter(U[:,0], U[:,1], alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    짧은 문장 ------------------> 동시발생 행렬 ------------------> 단어간 유사도 ------------------> PPMI 행렬 ------------------> 밀집벡터\n",
    "              preprocessing 함수                cosine_similarity                  ppmi 함수                     특이값 분해(SVD)\n",
    "              create_co_matrix 함수             most_similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. 지금까지는 작은 말뭉치를 사용했습니다. 그럼 본격적으로 큰 말뭉치인 PTB 데이터셋에 통계기법을 적용하는 실습을 구현해보세요\n",
    "    긴 문장 ------------------> 동시발생 행렬 ------------------> 단어간 유사도 ------------------> PPMI 행렬 ------------------> 밀집벡터\n",
    "              preprocessing 함수                cosine_similarity                  ppmi 함수                     특이값 분해(SVD)\n",
    "              create_co_matrix 함수             most_similar\n",
    "![fig2-12](dl2_images/fig2-12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. you와 i의 코사인 유사도를 알아보는 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T06:25:42.791685Z",
     "start_time": "2020-08-24T06:25:42.783690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0]\n",
      " [1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n",
      "0\n",
      "[0 1 0 0 0 0 0]\n",
      "4\n",
      "[0 1 0 1 0 0 0]\n",
      "0.7071067691154799\n"
     ]
    }
   ],
   "source": [
    "from common.util import preprocess, create_co_matrix, cos_similarity\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "print(C)\n",
    "\n",
    "c0 = C[word_to_id['you']]  # \"you\"의 단어 벡터\n",
    "print(word_to_id['you'])\n",
    "print(c0)\n",
    "\n",
    "c1 = C[word_to_id['i']]    # \"i\"의 단어 벡터\n",
    "print(word_to_id['i'])\n",
    "print(c1)\n",
    "\n",
    "print(cos_similarity(c0, c1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 동시 발생 행렬에서 you라는 단어와 가장 연관성이 높은 단어 5가지를 출력하는 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T06:27:33.766086Z",
     "start_time": "2020-08-24T06:27:33.758090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      " goodbye: 0.7071067691154799\n",
      " i: 0.7071067691154799\n",
      " hello: 0.7071067691154799\n",
      " say: 0.0\n",
      " and: 0.0\n"
     ]
    }
   ],
   "source": [
    "from common.util import preprocess, create_co_matrix, most_similar\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "\n",
    "most_similar('you', word_to_id, id_to_word, C, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 동시발생 행렬을 PPMI 행렬로 변환하는 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T06:28:38.981371Z",
     "start_time": "2020-08-24T06:28:38.972377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "동시발생 행렬\n",
      "[[0 1 0 0 0 0 0]\n",
      " [1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n",
      "--------------------------------------------------\n",
      "PPMI\n",
      "[[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      " [1.807 0.    0.807 0.    0.807 0.807 0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.    1.807 0.    1.807 0.    0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.807 0.    0.    0.    0.    2.807]\n",
      " [0.    0.    0.    0.    0.    2.807 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "from common.util import preprocess, create_co_matrix, cos_similarity, ppmi\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "W = ppmi(C)\n",
    "\n",
    "np.set_printoptions(precision=3)  # 유효 자릿수를 세 자리로 표시\n",
    "print('동시발생 행렬')\n",
    "print(C)\n",
    "print('-'*50)\n",
    "print('PPMI')\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 큰 말뭉치가 어떻게 생겼는지 확인하는 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T06:30:34.997111Z",
     "start_time": "2020-08-24T06:30:33.349189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ptb.train.txt ... \n",
      "Done\n",
      "말뭉치 크기: 929589\n",
      "corpus[:30]: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "\n",
      "id_to_word[0]: aer\n",
      "id_to_word[1]: banknote\n",
      "id_to_word[2]: berlitz\n",
      "\n",
      "word_to_id['car']: 3856\n",
      "word_to_id['happy']: 4428\n",
      "word_to_id['lexus']: 7426\n"
     ]
    }
   ],
   "source": [
    "from dataset import ptb\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "\n",
    "print('말뭉치 크기:', len(corpus))\n",
    "print('corpus[:30]:', corpus[:30])\n",
    "print()\n",
    "print('id_to_word[0]:', id_to_word[0])\n",
    "print('id_to_word[1]:', id_to_word[1])\n",
    "print('id_to_word[2]:', id_to_word[2])\n",
    "print()\n",
    "print(\"word_to_id['car']:\", word_to_id['car'])\n",
    "print(\"word_to_id['happy']:\", word_to_id['happy'])\n",
    "print(\"word_to_id['lexus']:\", word_to_id['lexus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 아래의 스크립트를 수행해보고 설명하세요\n",
    "    큰 말뭉치를 밀집벡터로 변환한 후 아래의 단어 (querys) 4개와 유사한 단어들을 각각 5개씩 출력하는 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T06:43:47.103993Z",
     "start_time": "2020-08-24T06:35:15.409132Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "동시발생 수 계산 ...\n",
      "PPMI 계산 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\rnn\\common\\util.py:141: RuntimeWarning: overflow encountered in long_scalars\n",
      "  pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n",
      "D:\\rnn\\common\\util.py:141: RuntimeWarning: invalid value encountered in log2\n",
      "  pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0% 완료\n",
      "2.0% 완료\n",
      "3.0% 완료\n",
      "4.0% 완료\n",
      "5.0% 완료\n",
      "6.0% 완료\n",
      "7.0% 완료\n",
      "8.0% 완료\n",
      "9.0% 완료\n",
      "10.0% 완료\n",
      "11.0% 완료\n",
      "12.0% 완료\n",
      "13.0% 완료\n",
      "14.0% 완료\n",
      "15.0% 완료\n",
      "16.0% 완료\n",
      "17.0% 완료\n",
      "18.0% 완료\n",
      "19.0% 완료\n",
      "20.0% 완료\n",
      "21.0% 완료\n",
      "22.0% 완료\n",
      "23.0% 완료\n",
      "24.0% 완료\n",
      "25.0% 완료\n",
      "26.0% 완료\n",
      "27.0% 완료\n",
      "28.0% 완료\n",
      "29.0% 완료\n",
      "30.0% 완료\n",
      "31.0% 완료\n",
      "32.0% 완료\n",
      "33.0% 완료\n",
      "34.0% 완료\n",
      "35.0% 완료\n",
      "36.0% 완료\n",
      "37.0% 완료\n",
      "38.0% 완료\n",
      "39.0% 완료\n",
      "40.0% 완료\n",
      "41.0% 완료\n",
      "42.0% 완료\n",
      "43.0% 완료\n",
      "44.0% 완료\n",
      "45.0% 완료\n",
      "46.0% 완료\n",
      "47.0% 완료\n",
      "48.0% 완료\n",
      "49.0% 완료\n",
      "50.0% 완료\n",
      "51.0% 완료\n",
      "52.0% 완료\n",
      "53.0% 완료\n",
      "54.0% 완료\n",
      "55.0% 완료\n",
      "56.0% 완료\n",
      "57.0% 완료\n",
      "58.0% 완료\n",
      "59.0% 완료\n",
      "60.0% 완료\n",
      "61.0% 완료\n",
      "62.0% 완료\n",
      "63.0% 완료\n",
      "64.0% 완료\n",
      "65.0% 완료\n",
      "66.0% 완료\n",
      "67.0% 완료\n",
      "68.0% 완료\n",
      "69.0% 완료\n",
      "70.0% 완료\n",
      "71.0% 완료\n",
      "72.0% 완료\n",
      "73.0% 완료\n",
      "74.0% 완료\n",
      "75.0% 완료\n",
      "76.0% 완료\n",
      "77.0% 완료\n",
      "78.0% 완료\n",
      "79.0% 완료\n",
      "80.0% 완료\n",
      "81.0% 완료\n",
      "82.0% 완료\n",
      "83.0% 완료\n",
      "84.0% 완료\n",
      "85.0% 완료\n",
      "86.0% 완료\n",
      "87.0% 완료\n",
      "88.0% 완료\n",
      "89.0% 완료\n",
      "90.0% 완료\n",
      "91.0% 완료\n",
      "92.0% 완료\n",
      "93.0% 완료\n",
      "94.0% 완료\n",
      "95.0% 완료\n",
      "96.0% 완료\n",
      "97.0% 완료\n",
      "98.0% 완료\n",
      "99.0% 완료\n",
      "100.0% 완료\n",
      "calculating SVD ...\n",
      "\n",
      "[query] you\n",
      " i: 0.7185859084129333\n",
      " we: 0.6385654807090759\n",
      " do: 0.5963179469108582\n",
      " 'll: 0.519759476184845\n",
      " anybody: 0.5134278535842896\n",
      "\n",
      "[query] year\n",
      " quarter: 0.6733551025390625\n",
      " month: 0.6254861354827881\n",
      " last: 0.6223835349082947\n",
      " earlier: 0.5952385067939758\n",
      " fiscal: 0.5802951455116272\n",
      "\n",
      "[query] car\n",
      " luxury: 0.6619530916213989\n",
      " auto: 0.6419267058372498\n",
      " cars: 0.5567983388900757\n",
      " corsica: 0.5467645525932312\n",
      " truck: 0.49986007809638977\n",
      "\n",
      "[query] toyota\n",
      " motor: 0.7274244427680969\n",
      " motors: 0.6677854061126709\n",
      " honda: 0.6473543047904968\n",
      " nissan: 0.6305743455886841\n",
      " chevrolet: 0.5758364200592041\n"
     ]
    }
   ],
   "source": [
    "from dataset import ptb\n",
    "from common.util import most_similar, create_co_matrix, ppmi\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "window_size = 2\n",
    "wordvec_size = 100\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "print('동시발생 수 계산 ...')\n",
    "C = create_co_matrix(corpus, vocab_size, window_size)\n",
    "\n",
    "print('PPMI 계산 ...')\n",
    "W = ppmi(C, verbose=True)\n",
    "\n",
    "print('calculating SVD ...')\n",
    "try:\n",
    "    # truncated SVD (빠르다!)\n",
    "    from sklearn.utils.extmath import randomized_svd\n",
    "    U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5,\n",
    "                             random_state=None)\n",
    "\n",
    "except ImportError:\n",
    "    # SVD (느리다)\n",
    "    U, S, V = np.linalg.svd(W)\n",
    "\n",
    "word_vecs = U[:, :wordvec_size]\n",
    "\n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
