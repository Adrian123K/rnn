{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>■ 복습</b>\n",
    "    컴퓨터에게 인간의 언어를 이해시키기 위한 작업\n",
    "    \n",
    "    2장. 단어의 분산표현 (밀집벡터) : 개선된 통계기법\n",
    "    3장. CBOW 신경망을 이용해서 문장을 학습 : 밀집벡터를 입력층에서 추출\n",
    "    4장. CBOW 신경망을 큰 말뭉치가 들어갈 수 있도록 개선 : 밀집벡터 추출\n",
    "        CBOW 신경망의 문제점\n",
    "            1. 말뭉치가 크면 메모리 사용량이 많다\n",
    "            2. 성능이 느림            \n",
    "    5장. RNN(순환 신경망) : 기억하는 신경망\n",
    "        실습: 작은 말뭉치      -> RNN 신경망에 입력하고 학습을 시킴\n",
    "        실습: 스티브잡스 연설문 -> RNN 신경망에 입력하고 학습을 시킴\n",
    "                         ↑\n",
    "                    문장을 one hot 표현으로 변경\n",
    "        실습: 시계열 데이터    -> RNN 신경망에 입력\n",
    "            가상으로 생성, 1~11월까지의 데이터를 RNN 신경망에 넣고 12월 데이터를 예상하게끔 상관계수로 12월 데이터를 신경망이 잘 맞췄는지 확인\n",
    "    6장. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 1. LSTM 의 큰그림 \n",
    "    이번 6장에서는 가 추가된 RNN 을 살펴봅니다.  RNN 의 큰 문제점 2가지는 기울기 소실 또는 기울기 폭발 이었는데 그것을 대신하는 계층으로써 게이트 가 추가된 RNN 이 큰 효과가 있습니다 \n",
    "    이 계층들에는 게이트 라는 구조가 사용되며, 게이트는 데이터와 기울기 흐름을 적절히 제어하는 메커니즘입니다.\n",
    "    또한, LSTM 계층을 사용한 언어모델을 생성합니다.  그리고 PTB 데이터셋을 학습하여 퍼플렉서티를 구합니다.\n",
    "    나아가 LSTM 다층화, 드롭아웃, 가중치 공유 등의 기법을 적용해 정확도를 큰 폭으로 향상 시킵니다.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 2.  RNN 계층에서 일어나는 일이 무엇인지 아래의 그림을 보고 설명하세요\n",
    "![fig6-2](http://cfile279.uf.daum.net/image/992EEB345F4F5CC42885F8)\n",
    "\n",
    "    RNN 계층은 시계열 데이터인 X1을 입력하면 h1을 출력\n",
    "    h1은 RNN 계층의 은닉상태라고 하며 과거 정보를 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 3. RNN 의 가장 큰 문제점은 무엇인가요 ? \n",
    "    장기기억에 취약하다는 문제점이 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 4. 언어모델의 역할이 무엇인가요 ?\n",
    "    주어진 단어들을 기초로 다음에 출현할 단얼르 예측하는 역할\n",
    "    과거의 주가 데이터를 기초로 다음에 출현할 주가를 예측하는 역할"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 5. 단순한 RNN 계층에서 일어나는 큰 문제점 2가지는 무엇입니까?\n",
    "    1. 기울기 소실\n",
    "    2. 기울기 폭발"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 6.  기울기 소실이 발생하는 이유가 무엇입니까?\n",
    "![fig6-6](http://cfile265.uf.daum.net/image/99A45F425F4F6AA3358D57)\n",
    "\n",
    "    RNN 계층의 활성화 함수가 tanh 함수이기 때문\n",
    "    그래서 이를 개선하기 위해서 ReLU 함수를 사용한다\n",
    "\n",
    "    x가 0으로부터  멀어질 수록 기울기가 작아집니다.\n",
    "        역전파에서는 기울기가 tanh 노드를 지날 때 마다 값은 계속 작아진다는 뜻입니다.  그래서 tanh 함수를 T번 통과하면 기울기도 T번 반복해서 작아집니다.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 7.  기울기 폭발은 왜 발생하나요?\n",
    "![fig6-7](http://cfile260.uf.daum.net/image/999B4E475F4F6C113733DC)\n",
    "\n",
    "    Matmul 때문\n",
    "        Matmul 노드에의 역전파는 dhWt라는 행렬곱으로 기울기를 계산\n",
    "        같은 계산을 시계열 데이터의 시간 크기만큼 반복\n",
    "        기울기는 Matmul 노드를 지날 때 마다 그림 6-7처럼 커지게 된다\n",
    "        \n",
    "    기울기 소실 -> tanh\n",
    "    기울기 폭발 -> Matmul 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 8.  기울기 폭발의 대책은 무엇입니까 ?\n",
    "    기울기 클리핑 기법 사용\n",
    "    기울기가 문턱 값을 초과하면 아래의 식처럼 기울기를 수정\n",
    "![fig](http://cfile277.uf.daum.net/image/9979B94B5F4F6CB829CB3A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 9.  그래서 위의 문제를 LSTM 은 어떻게 해결했습니까 ?\n",
    "![fig6-13](http://cfile252.uf.daum.net/image/995B23485F4F6D2E0676F2)\n",
    "\n",
    "    게이트가 추가된 RNN으로 해결\n",
    "    게이트는 데이터의 흐름을 제어\n",
    "    게이트를 얼마나 열고 닫을지도 데이터를 통해 자동으로 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 10.  게이트가 추가된 RNN 의 종류 2가지가 무엇입니까?\n",
    "    1. LSTM (Long short-term memory models)\n",
    "    2. GRU (Grated Recurrent Unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 11.  RNN 과 LSTM 의 차이가 무엇입니까 ?\n",
    "![fig](http://cfile244.uf.daum.net/image/99E6194E5F4F8F1D14692D)\n",
    "    c라는 경로의 유무\n",
    "        c : 기억셀\n",
    "        h : 은닉 상태"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 12.  은닉상태 ht 는 어떻게 계산되어져 출력되는 것입니까 ?\n",
    "![fig](http://cfile300.uf.daum.net/image/99C197355F4F9144067189)\n",
    "\n",
    "    과거의 기억셀 Ct-1는 어떤 계산에 의해 갱신된 Ct로 출력되고 이 출력된 Ct는 tanh로 계산되어져서 ht로 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 13.  아래의 그림에 output 게이트의 역할이 무엇입니까 ?\n",
    "![fig](http://cfile268.uf.daum.net/image/99DC37415F4F92ED0A287E)\n",
    "\n",
    "    tanh(ct)의 각 원소에 대해서 그것이 다음 시각의 은닉 상태에 얼마나 중요한가를 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 14.  forget 게이트의 역할은 무엇입니까 ?\n",
    "    이전 기억셀 Ct-1의 기억중에서 불필요한 기억을 잊게 해주는 역할\n",
    "![fig](http://cfile294.uf.daum.net/image/991D84445F4F959D0D7DCB)\n",
    "![e](http://cfile271.uf.daum.net/image/99C0644C5F4F956A0F0B3B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 15.  새로운 기억셀의 역할은 무엇입니까 ?\n",
    "    잊어야 할 기억은 잊어버리되 새로 기억해야할 정보는 기억해야하므로 새로 기억해야할 정보를 기억하게 하는 역할\n",
    "![fig](http://cfile268.uf.daum.net/image/99FE20455F4F978B15A4A2)\n",
    "![e](http://cfile256.uf.daum.net/image/99DF44485F4F97EF1F1DCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▩ 16.  input 게이트의 i 역할은 무엇입니까 ?\n",
    "    새로운 정보를 무비판적으로 수용하는게 아니라 적절히 선택하게 하는 역할\n",
    "![fig](http://cfile288.uf.daum.net/image/99572D435F4F98DC0B31BE)\n",
    "![e](http://cfile241.uf.daum.net/image/994BD4425F4F98BF10F2DE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history['val_'+metric], '')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([metric, 'val_'+metric])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True,\n",
    "                          as_supervised=True)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "\n",
    "encoder = info.features['text'].encoder\n",
    "\n",
    "print('Vocabulary size: {}'.format(encoder.vocab_size))\n",
    "\n",
    "sample_string = 'Hello TensorFlow.'\n",
    "\n",
    "encoded_string = encoder.encode(sample_string)\n",
    "print('Encoded string is {}'.format(encoded_string))\n",
    "\n",
    "original_string = encoder.decode(encoded_string)\n",
    "print('The original string: \"{}\"'.format(original_string))\n",
    "\n",
    "assert original_string == sample_string\n",
    "\n",
    "for index in encoded_string:\n",
    "    print('{} ----> {}'.format(index, encoder.decode([index])))\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.padded_batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = test_dataset.padded_batch(BATCH_SIZE)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(encoder.vocab_size, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_dataset, epochs=10,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=30)\n",
    "\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))\n",
    "\n",
    "\n",
    "def pad_to_size(vec, size):\n",
    "    zeros = [0] * (size - len(vec))\n",
    "    vec.extend(zeros)\n",
    "    return vec\n",
    "\n",
    "\n",
    "def sample_predict(sample_pred_text, pad):\n",
    "    encoded_sample_pred_text = encoder.encode(sample_pred_text)\n",
    "\n",
    "    if pad:\n",
    "        encoded_sample_pred_text = pad_to_size(encoded_sample_pred_text, 64)\n",
    "    encoded_sample_pred_text = tf.cast(encoded_sample_pred_text, tf.float32)\n",
    "    predictions = model.predict(tf.expand_dims(encoded_sample_pred_text, 0))\n",
    "\n",
    "    return (predictions)\n",
    "\n",
    "# predict on a sample text without padding.\n",
    "\n",
    "sample_pred_text = ('The movie was cool. The animation and the graphics '\n",
    "                    'were out of this world. I would recommend this movie.')\n",
    "predictions = sample_predict(sample_pred_text, pad=False)\n",
    "print(predictions)\n",
    "\n",
    "# predict on a sample text with padding\n",
    "\n",
    "sample_pred_text = ('The movie was cool. The animation and the graphics '\n",
    "                    'were out of this world. I would recommend this movie.')\n",
    "predictions = sample_predict(sample_pred_text, pad=True)\n",
    "print(predictions)\n",
    "\n",
    "\n",
    "plot_graphs(history, 'accuracy')\n",
    "\n",
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
